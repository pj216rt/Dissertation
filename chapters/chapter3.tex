% !TEX root = ../main.tex
In statistics, we often want to know how much a variable varies in response to changes in some other variables. 
Statistical models are techniques developed for describing observed data.  
These models consist of a some function, usually analytic consisting of combinations of known factors and some noise.  
If the analytic portion is a good approximation to a response \(y\), then this noise is typically lower.  
A "bad" model would then have a larger noise component.  
Commonly, this observed data is finitely-dimensional: a set of scalar observations modeled by more scalar predictors.  
Although this observed data may be very high dimensional (possibly, in the case of image data, involving millions of data points), its dimension is still finite.  
One possible example may be attempting to predict house price from various predictors\cite{de_cock_ames_2011}.

In recent years however, data has become larger and more complex.  
One subset of this large, complex data is functional data.  
With functional data, data can be thought of as functions supported on some continuous or infinitly dimensional domain, though in practice, this data is discrete realizations of the continuous functions.   
Many pieces of literature suggest that much of the classical regression/ANOVA methods learned in statistical textbooks can be easily extended to this functional domain.

The origins of FDA lay in longitudinal, or repeated measures data.  
However, now there are many more applications including image data, genomic data, and many more areas of research\cite{morris_functional_2015}.  
For our present work, the emphasis will be on functional regression.  

There are several different types of functional regression models, depending on whether the response, the predictors, or both are considered functions.  
We will be considering the case where we observe functional responses, a general example being the following: 

\begin{equation}
    y_i(t) = f(t) + \epsilon_i(t), \quad t \in \mathcal{T},
\end{equation}

\noindent
where $y_i(t)$ represents the $i$th subjectâ€™s noisy trajectory, $f(t)$ is the underlying smooth function of interest, and \(\epsilon_i(t)\) is a mean 0 random noise. 
In practice, we only observe a finite, often sparse, collection of noisy evaluations of this infinite-dimensional object \(t=\{t_1,t_2,\dots,t_D\}\).  
Let the observed value of the functional response for subject \(i\) at time \(t_j\) be denoted \(Y_i(t_j)\), for \(i = 1, \dots, N\) and \(j = 1, \dots, T_i\).  
Given a set of \(p\) scalar predictors \(X_{ia}\) where \( a = 1,2,\dots, p\), the mean structure of the functional model \(f(t)\) can be expressed as a linear combination of these predictors, analogous to an ordinary linear regression\footnote{Using the notation of Morris\cite{morris_functional_2015}.}:

\begin{equation}
    Y_i(t_j) = \sum_{a=1}^{p} X_{ia}B_a(t_j) + E_i(t_j)
\end{equation}

\noindent
where \(E_i(t)\) denotes residual errors.  
Typically, the behavior of functional coefficients \(B_a(t_j)\) is of interest.  
Because \(B_a(t_j)\) is typically assumed to be smooth and therefore infinite-dimensional, some form of dimension reduction is necessary before estimation and inference can proceed. 
The most common approaches involve representing \(B_a(t_j)\)  in terms of a finite basis expansion:

\begin{equation}
    B_a(t_j) \approx \sum_{\ell=1}^K \theta_\ell \, \psi_\ell(t),
\end{equation}

\noindent
where $\{\psi_\ell(t)\}_{\ell=1}^K$ are chosen basis functions (e.g., B-splines, Fourier series, or functional principal components), and $\boldsymbol\theta=(\theta_1,\ldots,\theta_K)^\top$ are the corresponding coefficients.  

This basis representation allows us to recast the infinite-dimensional regression problem into a finite-dimensional one, where estimation of the coefficient vector $\boldsymbol\theta$ becomes feasible with standard linear model tools, subject to additional constraints such as smoothness penalties. 
This penalty is often encoded through a matrix $P$ that penalizes roughness (e.g., between adjacent coefficients in the sequence of basis functions), thereby balancing fidelity to the data with smoothness of the estimated function.

With spline regression/modeling, there are often competing objectives.  
First, you want a model that fits the observed data as closely as possible.  
However, sometimes this resulting model is too "wiggly", making interpretability difficult.  
As a consequence, we want a model that is still a "good" fit, but not necessarily the "best" in terms of fit to the data.  
We achieve this by by minimizing a penalized least-squares criterion that looks like:

\begin{equation}
    \min_{\beta} \left\{ 
        \sum_{i=1}^{n} \left( y_i - \beta(t_i) \right)^2 
        + \lambda \int \left[\beta''(t)\right]^2 \, dt
        \right\},
\end{equation}

The first term in this equation is the standard residual sum of squares, while the second term is a quadratic penalty on the second derivatives of \(\beta\).  
\(\lambda\) is a smoothing parameter that serves as a balance between the two components of this penalized least squares equation; smaller values of \(\lambda\) penalize the fit less, while larger values penalizes more, forcing more smoothness.  
Often times, this penalty term is expressed as a matrix\footnote{See Appendix}.

The origins of what we now call functional data analysis (FDA) can be traced to the mid-20th century. Grenander (1950) laid the groundwork for statistical analysis of stochastic processes by proposing the decomposition of covariance structures using eigenvalues and eigenvectors \cite{ulf_grenander_stochastic_1950}.  
Rao was considering how to deal with functions instead of scalars\footnote{"In particular cases, if devices exist to record growth continuously with time, we have the problem of computing the average of entire curves and not merely at a number of time points"}.  
He considered using principal components to do so\cite{rao_statistical_1958}.  (Rao 1965 too)