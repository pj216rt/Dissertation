
@article{ibrahim_basic_2010,
	title = {Basic {Concepts} and {Methods} for {Joint} {Models} of {Longitudinal} and {Survival} {Data}},
	volume = {28},
	abstract = {Joint models for longitudinal and survival data are particularly relevant to many cancer clinical trials
and observational studies in which longitudinal biomarkers (eg, circulating tumor cells, immune
response to a vaccine, and quality-of-life measurements) may be highly associated with time to
event, such as relapse-free survival or overall survival. In this article, we give an introductory
overview on joint modeling and present a general discussion of a broad range of issues that arise
in the design and analysis of clinical trials using joint models. To demonstrate our points
throughout, we present an analysis from the Eastern Cooperative Oncology Group trial E1193, as
well as examine some operating characteristics of joint models through simulation studies.},
	number = {16},
	journal = {Journal of Clinical Oncology},
	author = {Ibrahim, Joseph and Chu, Haitao and Chen, Liddy},
	month = jun,
	year = {2010},
	pages = {2796--2801},
	file = {Ibrahim et al. - 2010 - Basic Concepts and Methods for Joint Models of Lon.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\44UGXU7A\\Ibrahim et al. - 2010 - Basic Concepts and Methods for Joint Models of Lon.pdf:application/pdf},
}

@article{long_joint_2018,
	title = {Joint modeling of multivariate longitudinal data and survival data in several observational studies of {Huntington}’s disease},
	volume = {18},
	abstract = {Background: Joint modeling is appropriate when one wants to predict the time to an event with covariates that
are measured longitudinally and are related to the event. An underlying random effects structure links the survival
and longitudinal submodels and allows for individual-specific predictions. Multiple time-varying and time-invariant
covariates can be included to potentially increase prediction accuracy. The goal of this study was to estimate a
multivariate joint model on several longitudinal observational studies of Huntington’s disease, examine external
validity performance, and compute individual-specific predictions for characterizing disease progression. Emphasis
was on the survival submodel for predicting the hazard of motor diagnosis.
Methods: Data from four observational studies was analyzed: Enroll-HD, PREDICT-HD, REGISTRY, and Track-HD. A
Bayesian approach to estimation was adopted, and external validation was performed using a time-varying AUC
measure. Individual-specific cumulative hazard predictions were computed based on a simulation approach. The
cumulative hazard was used for computing predicted age of motor onset and also for a deviance residual
indicating the discrepancy between observed diagnosis status and model-based status.
Results: The joint model trained in a single study had very good performance in discriminating among
diagnosed and pre-diagnosed participants in the remaining test studies, with the 5-year mean AUC = .83
(range .77–.90), and the 10-year mean AUC = .86 (range .82–.92). Graphical analysis of the predicted age of
motor diagnosis showed an expected strong relationship with the trinucleotide expansion that causes Huntington’s
disease. Graphical analysis of the deviance-type residual revealed there were individuals who converted to a diagnosis
despite having relatively low model-based risk, others who had not yet converted despite having relatively
high risk, and the majority falling between the two extremes.
Conclusions: Joint modeling is an improvement over traditional survival modeling because it considers all
the longitudinal observations of covariates that are predictive of an event. Predictions from joint models can
have greater accuracy because they are tailored to account for individual variability. These predictions can
provide relatively accurate characterizations of individual disease progression, which might be important in
the timing of interventions, determining the qualification for appropriate clinical trials, and general genotypic
analysis.},
	number = {138},
	journal = {BMC Medical Research Methodology},
	author = {Long, Jeffrey and Mills, James},
	month = nov,
	year = {2018},
	file = {Long and Mills - 2018 - Joint modeling of multivariate longitudinal data a.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\AQHASNFE\\Long and Mills - 2018 - Joint modeling of multivariate longitudinal data a.pdf:application/pdf},
}

@article{rothman_causation_2011,
	title = {Causation and {Casual} {Inference} in {Epidemeology}},
	volume = {95},
	abstract = {Concepts of cause and causal inference are largely self-taught from early learning experiences. A model of causation that describes causes in terms of sufficient causes and their component causes illuminates important principles such
as multicausality, the dependence of the strength of component causes on the
prevalence of complementary component causes, and interaction between component causes.
Philosophers agree that causal propositions cannot be proved, and find flaws or
practical limitations in all philosophies of causal inference. Hence, the role of logic,
belief, and observation in evaluating causal propositions is not settled. Causal
inference in epidemiology is better viewed as an exercise in measurement of an
effect rather than as a criterion-guided process for deciding whether an effect is present or not.},
	number = {S1},
	journal = {American Journal of Public Health},
	author = {Rothman, Kenneth and Greenland, Sander},
	month = oct,
	year = {2011},
	pages = {144--150},
	file = {Rothman and Greenland - 2011 - Causation and Casual Inference in Epidemeology.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\DB69JKVX\\Rothman and Greenland - 2011 - Causation and Casual Inference in Epidemeology.pdf:application/pdf},
}

@article{chipman_bart_2010,
	title = {Bart: {Bayesian} {Additive} {Regression} {Trees}},
	volume = {4},
	abstract = {We develop a Bayesian “sum-of-trees” model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm
that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random
basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a
likelihood. This approach enables full posterior inference including point and
interval estimates of the unknown regression function as well as the marginal
effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART’s
many features are illustrated with a bake-off against competing methods on
42 different data sets, with a simulation experiment and on a drug discovery
classification problem.},
	number = {1},
	journal = {The Annals of Applied Statistics},
	author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
	month = mar,
	year = {2010},
	pages = {266--298},
	file = {Chipman et al. - 2010 - Bart Bayesian Additive Regression Trees.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\655WWYSV\\Chipman et al. - 2010 - Bart Bayesian Additive Regression Trees.pdf:application/pdf},
}

@inproceedings{xiao_preventing_2021,
	address = {Wuhan},
	title = {Preventing {Pedestrian} {Injury} {Severity} at {Signalized} {Intersections}: {A} {Bayesian} {Multilevel} {Parametric} {Survival} {Model}},
	abstract = {This study intended to investigate the influencing factors of pedestrian injury severity at signalized intersections, considering the heterogeneity issue of unobserved factors at different signalized intersections and the spatial attributes with survival models. To achieve the objectives, a Bayesian multilevel parametric survival model was developed, in which the survival model addressed the pedestrian severity levels varying with time, while the panel data model accommodated the heterogeneity attributed to unobserved factors and spatial features within the Bayesian framework. The pedestrian-related crash data of Hong Kong metropolitan area from 2008 to 2012 were integrated, involving 376 signalized intersections with 2,090 pedestrian severity samples. By comparing the proportional hazard (PH) and accelerated failure time (AFT) models, the PH model with exponential distribution showed priority to the other models. Results revealed that pedestrian age, injury location, pedestrian special circumstance, pedestrian contributory, number of pedestrian stream, obstruction, road type, presence of tram/LRT stops and bus stops were potentially significant factors of increasing the pedestrian injury severity probability. The findings provide useful insights for practitioners and policy makers to improve pedestrian safety at signalized intersections.},
	author = {Xiao, Daiquan and Xuecai, Xu and Yuan, Quan and Ma, Changxi},
	year = {2021},
	pages = {169--176},
}

@article{song_decision_2015,
	title = {Decision tree methods: applications for classification and prediction},
	volume = {27},
	number = {2},
	journal = {Shanghai Archives of Psychiatry},
	author = {Song, Yan-yan and Lu, Ying},
	year = {2015},
	pages = {130--135},
}

@unpublished{minka_bayesian_1999,
	title = {Bayesian {Linear} {Regression}},
	abstract = {This note derives the posterior, evidence, and predictive density for linear multivariate
regression under zero-mean Gaussian noise. Many Bayesian texts, such as Box \& Tiao
(1973), cover linear regression. This note contributes to the discussion by paying careful
attention to invariance issues, demonstrating model selection based on the evidence, and
illustrating the shape of the predictive density. Piecewise regression and basis function
regression are also discussed.},
	language = {English},
	author = {Minka, Thomas P.},
	month = sep,
	year = {1999},
}

@article{kelter_bayesian_2020,
	title = {Bayesian {Survival} {Analysis} in {STAN} for {Improved} {Measuring} of {Uncertainty} in {Parameter} {Estimates}},
	volume = {18},
	abstract = {Survival analysis is an important analytic method in the social and medical
sciences. Also known under the name time-to-event analysis, this method
provides parameter estimation and model fitting commonly conducted via
maximum-likelihood. Bayesian survival analysis offers multiple advantages
over the frequentist approach for measurement practitioners, however,
computational difficulties have mitigated interest in Bayesian survival models. This paper shows that Bayesian survival models can be fitted in
a straightforward manner via the probabilistic programming language
Stan, which offers full Bayesian inference through Hamiltonian Monte
Carlo algorithms. Illustrations show the benefits for measurement practitioners in the social and medical sciences.},
	journal = {Measurement},
	author = {Kelter, Riko},
	year = {2020},
	pages = {101--109},
	file = {Kelter - 2020 - Bayesian Survival Analysis in STAN for Improved Me.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\P7RFCWZK\\Kelter - 2020 - Bayesian Survival Analysis in STAN for Improved Me.pdf:application/pdf},
}

@article{austin_introduction_2016,
	title = {Introduction to the {Analysis} of {Survival} {Data} in the {Presence} of {Competing} {Risks}},
	volume = {133},
	issn = {0009-7322},
	abstract = {Competing risks occur frequently in the analysis of survival data. A competing risk is an event whose occurrence precludes the occurrence of the primary event of interest. In a study examining time to death attributable to cardiovascular causes, death attributable to noncardiovascular causes is a competing risk. When estimating the crude incidence of outcomes, analysts should use the cumulative incidence function, rather than the complement of the Kaplan-Meier survival function. The use of the Kaplan-Meier survival function results in estimates of incidence that are biased upward, regardless of whether the competing events are independent of one another. When fitting regression models in the presence of competing risks, researchers can choose from 2 different families of models: modeling the effect of covariates on the cause-specific hazard of the outcome or modeling the effect of covariates on the cumulative incidence function. The former allows one to estimate the effect of the covariates on the rate of occurrence of the outcome in those subjects who are currently event free. The latter allows one to estimate the effect of covariates on the absolute risk of the outcome over time. The former family of models may be better suited for addressing etiologic questions, whereas the latter model may be better suited for estimating a patient’s clinical prognosis. We illustrate the application of these methods by examining cause-specific mortality in patients hospitalized with heart failure. Statistical software code in both R and SAS is provided.},
	number = {6},
	journal = {Circulation},
	author = {Austin, Peter C. and See, Douglas S. and Fine, Jason P.},
	year = {2016},
	pages = {601--609},
}

@unpublished{berger_overview_1994,
	address = {Madrid, Spain},
	type = {Reading},
	title = {An {Overview} of {Robust} {Bayesian} {Analysis}},
	abstract = {Robust Bayesian analysis is the study of the sensitivity of Bayesian answers to uncertain inputs. This paper seeks to provide an overview of the subject, one that is accessible to statisticians outside the field. Recent developments in the area are also reviewed, though with very uneven emphasis.},
	language = {English},
	author = {Berger, James O.},
	year = {1994},
}

@article{gelman_philosophy_2012,
	title = {Philosophy and the practice of {Bayesian} statistics},
	volume = {66},
	abstract = {A substantial school in the philosophy of science identifies Bayesian inference with
inductive inference and even rationality as such, and seems to be strengthened by the
rise and practical success of Bayesian statistics. We argue that the most successful
forms of Bayesian statistics do not actually support that particular philosophy but rather
accord much better with sophisticated forms of hypothetico-deductivism. We examine
the actual role played by prior distributions in Bayesian models, and the crucial aspects of
model checking and model revision, which fall outside the scope of Bayesian confirmation
theory. We draw on the literature on the consistency of Bayesian updating and also on
our experience of applied work in social science. Clarity about these matters should
benefit not just philosophy of science, but also statistical practice. At best, the inductivist
view has encouraged researchers to fit and compare models without checking them; at
worst, theorists have actively discouraged practitioners from performing model checking
because it does not fit into their framework.},
	journal = {British Journal of Mathematical and Statistical Psychology},
	author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
	year = {2012},
	pages = {8--38},
	file = {Gelman and Shalizi - 2012 - Philosophy and the practice of Bayesian statistics.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\RCYTV7K2\\Gelman and Shalizi - 2012 - Philosophy and the practice of Bayesian statistics.pdf:application/pdf},
}

@article{hernan_definition_nodate,
	title = {A definition of causal effect for epidemiological research},
	volume = {58},
	abstract = {Estimating the causal effect of some exposure on some
outcome is the goal of many epidemiological studies. This
article reviews a formal definition of causal effect for such
studies. For simplicity, the main description is restricted to
dichotomous variables and assumes that no random error
attributable to sampling variability exists. The appendix
provides a discussion of sampling variability and a
generalisation of this causal theory. The difference between
association and causation is described—the redundant
expression ‘‘causal effect’’ is used throughout the article to
avoid confusion with a common use of ‘‘effect’’ meaning
simply statistical association—and shows why, in theory,
randomisation allows the estimation of causal effects
without further assumptions. The article concludes with a
discussion on the limitations of randomised studies. These
limitations are the reason why methods for causal inference
from observational data are needed.},
	number = {4},
	journal = {Journal of Epidemiology \& Community Health},
	author = {Hernan, M. A.},
	pages = {265--271},
	file = {Hernan - A definition of causal effect for epidemiological .pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\GGXLHJ6R\\Hernan - A definition of causal effect for epidemiological .pdf:application/pdf},
}

@book{hernan_causal_2020,
	address = {Boca Raton, FL},
	title = {Causal {Inference}: {What} {If}},
	publisher = {Chapman \& Hall},
	author = {Hernán, Miguel A. and Roins, James M.},
	month = dec,
	year = {2020},
}

@book{pearl_casual_2016,
	address = {United Kingdom},
	title = {Casual {Inference} in {Statistics}: {A} {Primer}},
	isbn = {978-1-119-18684-7},
	language = {English},
	publisher = {John Wiley \& Sons Ltd},
	author = {Pearl, Judea and Glymour, Madelyn and Jewell, Nicholas P.},
	year = {2016},
}

@article{miller_robust_2018,
	title = {Robust {Bayesian} {Inference} via {Coarsening}},
	volume = {114},
	abstract = {The standard approach to Bayesian inference is based on the assumption that the distribution of the data belongs to the chosen model class. However, even a small violation of this assumption can have a large impact on the outcome of a Bayesian procedure. We introduce a novel approach to Bayesian inference that improves robustness to small departures from the model: rather than conditioning on the event that the observed data are generated by the model, one conditions on the event that the model generates data close to the observed data, in a distributional sense. When closeness is defined in terms of relative entropy, the resulting “coarsened” posterior can be approximated by simply tempering the likelihood—that is, by raising the likelihood to a fractional power—thus, inference can usually be implemented via standard algorithms, and one can even obtain analytical solutions when using conjugate priors. Some theoretical properties are derived, and we illustrate the approach with real and simulated data using mixture models and autoregressive models of unknown order. Supplementary materials for this article are available online.},
	language = {English},
	number = {527},
	journal = {Journal of the American Statistical Association},
	author = {Miller, Jeffrey W. and Dunson, David B.},
	month = aug,
	year = {2018},
	file = {Miller and Dunson - 2018 - Robust Bayesian Inference via Coarsening.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\JZYR8QZD\\Miller and Dunson - 2018 - Robust Bayesian Inference via Coarsening.pdf:application/pdf},
}

@book{casella_statistical_2002,
	address = {Pacific Grove, CA},
	edition = {2nd},
	title = {Statistical {Inference}},
	isbn = {0-534-24312-6},
	language = {English},
	publisher = {Duxbury},
	author = {Casella, George and Berger, Roger L.},
	year = {2002},
}

@book{fisher_statistical_1990,
	address = {New York},
	edition = {13th},
	title = {Statistical {Methods}, {Experimental} {Design}, and {Scientifi} {Inference}},
	isbn = {0-19-852229-0},
	abstract = {Re-issue},
	language = {English},
	publisher = {Oxford University Press},
	author = {Fisher, R.A.},
	year = {1990},
}

@article{aldrich_r_2008,
	title = {R. {A}. {Fisher} on {Bayes} and {Bayes}’ {Theorem}},
	volume = {3},
	abstract = {Ronald Fisher believed that “The theory of inverse probability is
founded upon an error, and must be wholly rejected.” This note describes how
Fisher divided responsibility for the error between Bayes and Laplace. Bayes he
admired for formulating the problem, producing a solution and then withholding
it; Laplace he blamed for promulgating the theory and for distorting the concept
of probability to accommodate the theory. At the end of his life Fisher added
a refinement: in the Essay Bayes had anticipated one of Fisher’s own fiducial
arguments},
	number = {1},
	journal = {Bayesian Analysis},
	author = {Aldrich, John},
	year = {2008},
	pages = {161--170},
	file = {Aldrich - 2008 - R. A. Fisher on Bayes and Bayes’ Theorem.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\LBP55XPY\\Aldrich - 2008 - R. A. Fisher on Bayes and Bayes’ Theorem.pdf:application/pdf},
}

@article{lewis_bayesian_2021,
	title = {Bayesian {Restricted} {Likelihood} {Methods}: {Conditioning} on {Insufficient} {Statistics} in {Bayesian} {Regression} (with {Discussion})},
	volume = {16},
	url = {https://doi.org/10.1214/21-BA1257},
	abstract = {Bayesian methods have proven themselves to be successful across a wide range of scientific problems and have many well-documented advantages over competing methods. However, these methods run into difficulties for two major and prevalent classes of problems: handling data sets with outliers and dealing with model misspecification. We outline the drawbacks of previous solutions to both of these problems and propose a new method as an alternative. When working with the new method, the data is summarized through a set of insufficient statistics, targeting inferential quantities of interest, and the prior distribution is updated with the summary statistics rather than the complete data. By careful choice of conditioning statistics, we retain the main benefits of Bayesian methods while reducing the sensitivity of the analysis to features of the data not captured by the conditioning statistics. For reducing sensitivity to outliers, classical robust estimators (e.g., M-estimators) are natural choices for conditioning statistics. A major contribution of this work is the development of a data augmented Markov chain Monte Carlo (MCMC) algorithm for the linear model and a large class of summary statistics. We demonstrate the method on simulated and real data sets containing outliers and subject to model misspecification. Success is manifested in better predictive performance for data points of interest as compared to competing methods.},
	language = {English},
	number = {4},
	journal = {Bayesian Analysis},
	author = {Lewis, John R. and MacEachern, Steven N. and Lee, Yoonkyung},
	month = dec,
	year = {2021},
	pages = {1393--1462},
	file = {Lewis et al. - 2021 - Bayesian Restricted Likelihood Methods Conditioni.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\XUFNXG2G\\Lewis et al. - 2021 - Bayesian Restricted Likelihood Methods Conditioni.pdf:application/pdf},
}

@article{carrigan_they_2013,
	title = {They make a desert and call it peace},
	volume = {23},
	issn = {1033-2839},
	abstract = {When the Roman historian Tacitus surveyed the impact of the colonisation of Britain and its repressive effect on the native population he summed up his sense of despair with the searing phrase, 'They make a desert and call it peace.' Imperial overstretch had, according to Tacitus, reduced Rome to a broken society. Rome had achieved only a pyrrhic victory. Britain was prostrate before the power of Rome - but at what cost to the ideal of a civilisation that idolised the arts and culture but in practice created client states at the point of a sword?},
	language = {English},
	journal = {Legal Education Review},
	author = {Carrigan, Frank},
	month = jan,
	year = {2013},
	pages = {313--343},
	file = {Carrigan - 2013 - They make a desert and call it peace.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\FH73RNUU\\Carrigan - 2013 - They make a desert and call it peace.pdf:application/pdf},
}

@unpublished{shalizi_lecture_2015,
	type = {Lecture {Notes}},
	title = {Lecture 6: {The} {Method} of {Maximum} {Likelihood} for {Simple} {Linear} {Regression}},
	url = {https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf},
	urldate = {2023-10-10},
	author = {Shalizi, Cosma},
	month = sep,
	year = {2015},
	file = {Shalizi - 2015 - Lecture 6 The Method of Maximum Likelihood for Si.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\6E7X73QU\\Shalizi - 2015 - Lecture 6 The Method of Maximum Likelihood for Si.pdf:application/pdf},
}

@article{hill_bayesian_2011,
	title = {Bayesian nonparametric modeling for causal inference},
	volume = {20},
	abstract = {Researchers have long struggled to identify causal effects in nonexperimental settings. Many recently proposed strategies assume ignorability of the treatment assignment mechanism and require fitting two models—one for the assignment mechanism
and one for the response surface. This article proposes a strategy that instead focuses
on very flexibly modeling just the response surface using a Bayesian nonparametric
modeling procedure, Bayesian Additive Regression Trees (BART). BART has several
advantages: it is far simpler to use than many recent competitors, requires less guesswork in model fitting, handles a large number of predictors, yields coherent uncertainty intervals, and fluidly handles continuous treatment variables and missing data
for the outcome variable. BART also naturally identifies heterogeneous treatment effects. BART produces more accurate estimates of average treatment effects compared
to propensity score matching, propensity-weighted estimators, and regression adjustment in the nonlinear simulation situations examined. Further, it is highly competitive
in linear settings with the “correct” model, linear regression. Supplemental materials
including code and data to replicate simulations and examples from the article as well
as methods for population inference are available online.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Hill, Jennifer L.},
	year = {2011},
	pages = {1--24},
	file = {Hill - 2011 - Bayesian nonparametric modeling for causal inferen.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\6A5ETEL5\\Hill - 2011 - Bayesian nonparametric modeling for causal inferen.pdf:application/pdf},
}

@article{wagenmakers_bayesian_2010,
	title = {Bayesian hypothesis testing for psychologists: {A} tutorial on the {Savage}–{Dickey} method},
	volume = {60},
	abstract = {In the field of cognitive psychology, the p-value hypothesis test has
established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental
effect. An alternative and arguably more appropriate measure of
evidence is conveyed by a Bayesian hypothesis test, which prefers
the model with the highest average likelihood. One of the main
problems with this Bayesian hypothesis test, however, is that it
often requires relatively sophisticated numerical methods for its
computation. Here we draw attention to the Savage–Dickey density
ratio method, a method that can be used to compute the result of a
Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method’s validity, generality, and flexibility.},
	number = {3},
	journal = {Cognitive Psychology},
	author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
	year = {2010},
	pages = {158--189},
	file = {Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\CZEFSM3A\\Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:application/pdf},
}

@article{lindley_philosophy_2000,
	series = {D},
	title = {The {Philosophy} of {Statistics}},
	volume = {49},
	abstract = {This paper puts forward an overall view of statistics. It is argued that statistics is the study of uncertainty. The many demonstrations that uncertainties can only combine according to the rules of the probability calculus are summarized. The conclusion is that statistical inference is firmly based on probability alone. Progress is therefore dependent on the construction of a probability model; methods for doing this are considered. It is argued that the probabilities are personal. The roles of likelihood and exchangeability are explained. Inference is only of value if it can be used, so the extension to decision analysis, incorporating utility, is related to risk and to the use of statistics in science and law. The paper has been written in the hope that it will be intelligible to all who are interested in statistics.},
	number = {3},
	journal = {Journal of the Royal Statistical Society},
	author = {Lindley, Dennia V.},
	year = {2000},
	pages = {293--337},
}

@article{mcgillis_attribution_1974,
	title = {Attribution and the {Law}},
	volume = {2},
	abstract = {Complex theories of culpability have evolved in the law, which specify the circumstances in which an action
is to be viewed as voluntary or involuntary, and justifiable or not justifiable. Legal theories also distinguish
among varying degrees of responsibility for criminal acts depending upon the mental state of the defendant.
These theories have been developed, for the most part, on the basis of logical analysis. Recently psychologists have begun to empirically study the judgmental processes used in assigning responsibility for actions. This article reviews both the legal and psychological approaches to the area and notes the potential
contributions psychological research can make to our understanding of judgmental biases in the justice
system. The empirical research can help indicate conditions in which legal principles are ignored and
replaced with common sense interpretations of the law and legal principles.},
	number = {4},
	journal = {:aw and Human Behavior},
	author = {McGillis, Daniel},
	year = {1974},
	pages = {289--300},
	file = {McGillis - 1974 - Attribution and the Law.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\KQE45Z65\\McGillis - 1974 - Attribution and the Law.pdf:application/pdf},
}

@article{austin_introduction_2011,
	title = {An {Introduction} to {Propensity} {Score} {Methods} for {Reducing} the {Effects} of {Confounding} in {Observational} {Studies}},
	volume = {46},
	abstract = {The propensity score is the probability of treatment assignment conditional on
observed baseline characteristics. The propensity score allows one to design and
analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity
score is a balancing score: conditional on the propensity score, the distribution
of observed baseline covariates will be similar between treated and untreated
subjects. I describe 4 different propensity score methods: matching on the propensity score, stratification on the propensity score, inverse probability of treatment
weighting using the propensity score, and covariate adjustment using the propensity
score. I describe balance diagnostics for examining whether the propensity score
model has been adequately specified. Furthermore, I discuss differences between
regression-based methods and propensity score-based methods for the analysis of
observational data. I describe different causal average treatment effects and their
relationship with propensity score analyses.},
	number = {3},
	journal = {Multivariate Behavioral Research},
	author = {Austin, Peter C.},
	month = may,
	year = {2011},
	pages = {399--424},
	file = {Austin - 2011 - An Introduction to Propensity Score Methods for Re.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\6DN439KH\\Austin - 2011 - An Introduction to Propensity Score Methods for Re.pdf:application/pdf},
}

@article{vanderweele_causal_2011,
	title = {Causal mediation analysis with survival data},
	volume = {22},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3109321/},
	abstract = {Causal mediation analysis is considered for time-to-event outcomes and survival analysis models.
Different possible effect decompositions are discussed for the survival function, hazard, mean
survival time and median survival scales. Approaches to mediation analysis in the social sciences
are related to counterfactual approaches using additive hazard, proportional hazard and accelerated
failure time models. The product-coefficient method from the social sciences gives mediated
effects on the hazard difference scale for additive hazard models, on the log mean survival time
difference scale for accelerated failure time models, and on the log hazard scale for the
proportional hazards model but only if the outcome is rare. With the proportional hazards model
and a common outcome, the product-coefficient method can provide a valid test for the presence
of a mediator effect but does not provide a measure. When additive hazard, accelerated failure
time, or the rare-outcome proportional hazards models are employed and combined with the
counterfactual approach, exposure-mediator interactions can be accommodated in a relatively
straightforward manner.},
	number = {4},
	journal = {Epidemeology},
	author = {VanderWeele, Tyler J.},
	month = jul,
	year = {2011},
	pages = {582--585},
	file = {VanderWeele - 2011 - Causal mediation analysis with survival data.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\RXPM9AI4\\VanderWeele - 2011 - Causal mediation analysis with survival data.pdf:application/pdf},
}

@article{cole_adjusted_2004,
	title = {Adjusted survival curves with inverse probability weights},
	volume = {75},
	abstract = {Kaplan—Meier survival curves and the associated nonparametric log rank
test statistic are methods of choice for unadjusted survival analyses, while the semiparametric Cox proportional hazards regression model is used ubiquitously as a method
for covariate adjustment. The Cox model extends naturally to include covariates, but
there is no generally accepted method to graphically depict adjusted survival curves.
The authors describe a method and provide a simple worked example using inverse
probability weights (IPW) to create adjusted survival curves. When the weights are
non-parametrically estimated, this method is equivalent to direct standardization of
the survival curves to the combined study population.},
	number = {1},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Cole, Stephen R. and Hernan, Miguel A.},
	year = {2004},
	pages = {45--49},
	file = {Cole and Hernan - 2004 - Adjusted survival curves with inverse probability .pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\A6WKJ968\\Cole and Hernan - 2004 - Adjusted survival curves with inverse probability .pdf:application/pdf},
}

@article{pereira_somatic_2016,
	title = {The somatic mutation profiles of 2,433 breast cancers refine their genomic and transcriptomic landscapes},
	volume = {7},
	abstract = {The genomic landscape of breast cancer is complex, and inter- and intra-tumour
heterogeneity are important challenges in treating the disease. In this study, we sequence 173
genes in 2,433 primary breast tumours that have copy number aberration (CNA),
gene expression and long-term clinical follow-up data. We identify 40 mutation-driver
(Mut-driver) genes, and determine associations between mutations, driver CNA profiles,
clinical-pathological parameters and survival. We assess the clonal states of Mut-driver
mutations, and estimate levels of intra-tumour heterogeneity using mutant-allele fractions.
Associations between PIK3CA mutations and reduced survival are identified in three
subgroups of ER-positive cancer (defined by amplification of 17q23, 11q13–14 or 8q24).
High levels of intra-tumour heterogeneity are in general associated with a worse outcome,
but highly aggressive tumours with 11q13–14 amplification have low levels of intra-tumour
heterogeneity. These results emphasize the importance of genome-based stratification of
breast cancer, and have important implications for designing therapeutic strategies.},
	journal = {Nature Communications},
	author = {Pereira, Bernard and Chin, Suet-Feung and Rueda, Oscar M.},
	year = {2016},
	file = {Pereira et al. - 2016 - The somatic mutation profiles of 2,433 breast canc.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\ZGSKREH4\\Pereira et al. - 2016 - The somatic mutation profiles of 2,433 breast canc.pdf:application/pdf},
}

@inproceedings{chapfuwa_enabling_2021,
	address = {Virtual Event},
	title = {Enabling {Counterfactual} {Survival} {Analysis} with {Balanced} {Representations}},
	url = {https://dl.acm.org/doi/10.1145/3450439.3451875},
	abstract = {Balanced representation learning methods have been applied successfully to counterfactual inference from observational data. However, approaches that account for survival outcomes are relatively
limited. Survival data are frequently encountered across diverse
medical applications, i.e., drug development, risk profiling, and
clinical trials, and such data are also relevant in fields like manufacturing (e.g., for equipment monitoring). When the outcome of interest is a time-to-event, special precautions for handling censored
events need to be taken, as ignoring censored outcomes may lead
to biased estimates. We propose a theoretically grounded unified
framework for counterfactual inference applicable to survival outcomes. Further, we formulate a nonparametric hazard ratio metric
for evaluating average and individualized treatment effects. Experimental results on real-world and semi-synthetic datasets, the
latter of which we introduce, demonstrate that the proposed approach significantly outperforms competitive alternatives in both
survival-outcome prediction and treatment-effect estimation.},
	booktitle = {Association for {Computing} {Machinery}},
	author = {Chapfuwa, Paidamoyo and Assaad, Serge and Zeng, Shuxi},
	month = apr,
	year = {2021},
	pages = {133--145},
	file = {Chapfuwa et al. - 2021 - Enabling Counterfactual Survival Analysis with Bal.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\QZ3WAYIR\\Chapfuwa et al. - 2021 - Enabling Counterfactual Survival Analysis with Bal.pdf:application/pdf},
}

@book{ibrahim_bayesian_2001,
	title = {Bayesian {Survival} {Analysis}},
	publisher = {Springer},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Sinha, Debajyoti},
	year = {2001},
}

@article{austin_use_2014,
	title = {The use of propensity score methods with survival or time-to-event outcomes: reporting measures of effect similar to those used in randomized experiments},
	volume = {33},
	abstract = {Propensity score methods are increasingly being used to estimate causal treatment effects in observational studies. In medical and epidemiological studies, outcomes are frequently time-to-event in nature. Propensity-score
methods are often applied incorrectly when estimating the effect of treatment on time-to-event outcomes. This
article describes how two different propensity score methods (matching and inverse probability of treatment
weighting) can be used to estimate the measures of effect that are frequently reported in randomized controlled
trials: (i) marginal survival curves, which describe survival in the population if all subjects were treated or if all
subjects were untreated; and (ii) marginal hazard ratios. The use of these propensity score methods allows one to
replicate the measures of effect that are commonly reported in randomized controlled trials with time-to-event
outcomes: both absolute and relative reductions in the probability of an event occurring can be determined. We
also provide guidance on variable selection for the propensity score model, highlight methods for assessing the
balance of baseline covariates between treated and untreated subjects, and describe the implementation of a
sensitivity analysis to assess the effect of unmeasured confounding variables on the estimated treatment effect
when outcomes are time-to-event in nature. The methods in the paper are illustrated by estimating the effect
of discharge statin prescribing on the risk of death in a sample of patients hospitalized with acute myocardial
infarction. In this tutorial article, we describe and illustrate all the steps necessary to conduct a comprehensive analysis of the effect of treatment on time-to-event outcomes},
	journal = {Statistics in Medicine},
	author = {Austin, Peter C.},
	year = {2014},
	pages = {1242--1258},
	file = {Austin - 2014 - The use of propensity score methods with survival .pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\APUMPAJ9\\Austin - 2014 - The use of propensity score methods with survival .pdf:application/pdf},
}

@misc{de_stavola_causal_2021,
	address = {LSHTM Centre for Statistical Methodology},
	type = {Presentation},
	title = {Causal {Inference} for {Survival} {Outcomes}: {An} {Introduction}},
	author = {De Stavola, Bianca L},
	month = nov,
	year = {2021},
	file = {De Stavola - 2021 - Causal Inference for Survival Outcomes An Introdu.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\DLW6K8YA\\De Stavola - 2021 - Causal Inference for Survival Outcomes An Introdu.pdf:application/pdf},
}

@article{weinberg_how_1996,
	title = {How {Cancer} {Arises}},
	volume = {275},
	number = {3},
	journal = {Scientific American},
	author = {Weinberg, Robert A.},
	month = sep,
	year = {1996},
	pages = {62--70},
}

@article{nygren_what_2001,
	title = {What is cancer chemotherapy?},
	volume = {40},
	abstract = {The importance of chemotherapy for cure of cancer is increasing, especially with its use as adjuvans to local therapy. Furthermore, in advanced disease, when the tumour has disseminated from its place of origin, chemotherapy has an expanding role in efforts to relieve cancer-related symptoms and to prolong life. Despite its shortcomings, chemotherapy, therefore, is an important treatment modality in oncology and will probably remain so for considerable time. This presentation, within the frame of The Swedish Council on Technology Assessment in Health Care (SBU) project to review cancer chemotherapy, aims to provide a brief overview of the field cancer chemotherapy. It includes a historical perspective of cancer chemotherapy, some practical aspects and theoretical considerations with respect to the action of, resistance to and metabolism of these drugs. Furthermore, some outlooks into the nearest future with respect to ways to improve and develop cancer chemotherapy are provided as well as some aspects of chemotherapy from an employee-protection perspective.},
	number = {2-3},
	journal = {Acta Oncologica},
	author = {Nygren, Peter},
	year = {2001},
	pages = {166--174},
	file = {Nygren - 2001 - What is cancer chemotherapy.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\R7YGW2T7\\Nygren - 2001 - What is cancer chemotherapy.pdf:application/pdf},
}

@article{liu_k-cdfs_2022,
	title = {K-{CDFs}: {A} {Nonparametric} {Clustering} {Algorithm} via {Cumulative} {Distribution} {Function}},
	volume = {32},
	abstract = {We propose a novel partitioning clustering procedure based on the cumulative distribution function (CDF), called K-CDFs. For univariate data, the K-CDFs represent the cluster centers by empirical CDFs and assign each observation to the closest center measured by the Crame´
r-von Mises distance. The procedure is nonparametric and does not require assumptions on cluster distributions imposed by mixture models. A projection technique is used to generalize the K-CDFs for univariate data to an arbitrary dimension. The proposed procedure has several appealing properties. It is robust to heavy-tailed data, is not sensitive to the data dimensions, does not require moment conditions on data and can effectively detect linearly nonseparable clusters. To implement the K-CDFs, we propose two kinds of algorithms: a greedy algorithm as the classical Lloyd’s algorithm and a spectral relaxation algorithm. We illustrate the finite sample performance of the proposed algorithms through simulation experiments and empirical analyses of several real datasets. Supplementary files for this article are available online.},
	number = {1},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Liu, Jicai and Li, Jinhong and Zhang, Riquan},
	month = jul,
	year = {2022},
	pages = {304--318},
}

@article{ghosh_two-sample_2021,
	title = {Two-sample high dimensional mean test based on prepivots},
	volume = {163},
	abstract = {Testing equality of mean vectors is a very commonly used criterion when comparing two multivariate random variables. Traditional tests such as Hotelling's 
 become either unusable or output small power when the number of variables is greater than the combined sample size. A novel method is proposed using both prepivoting and Edgeworth expansion for testing the equality of two population mean vectors in a “large p, small n” setting. The asymptotic null distribution of the test statistic is derived and it is shown that the power of suggested test converges to one under certain alternatives when both n and p increase to infinity. Finite sample performance of the proposed test statistic is compared with other recently developed tests designed to also handle the “large p, small n” situation through simulations. The proposed test achieves competitive rates for both type I error rate and power. The usefulness of suggested test is illustrated by applications to two microarray gene expression data sets.},
	journal = {Computational Statistics \& Data Analysis},
	author = {Ghosh, Santu and Ayyala, Deepak Nag and Hellebuyck, Rafael},
	year = {2021},
}

@unpublished{hariton_randomised_2018,
	type = {Author manuscript},
	title = {Randomised controlled trials—the gold standard for effectiveness research},
	author = {Hariton, Eduardo and Locascio, Joseph J.},
	month = dec,
	year = {2018},
	file = {Hariton and Locascio - 2018 - Randomised controlled trials—the gold standard for.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\MTQV45WQ\\Hariton and Locascio - 2018 - Randomised controlled trials—the gold standard for.pdf:application/pdf},
}

@article{payne_rugby_2008,
	title = {Rugby (the religion of {Wales}) and its influence on the {Catholic} church: should {Pope} {Benedict} {XVI} be worried?},
	volume = {337},
	abstract = {Objective To explore the perceived wisdom that papal
mortality is related to the success of the Welsh rugby
union team.
Design Retrospective observational study of historical
Vatican and sporting data.
Main outcome measure Papal deaths between 1883 and
the present day.
Results There is no evidence of a link between papal
deaths and any home nation grand slams (when one
nation succeeds in beating all other competing teams in
every match). There was, however, weak statistical
evidence to support an association between Welsh
performance and the number of papal deaths.
Conclusion Given the dominant Welsh performances of
2008, the Vatican medical team should take special care
of the pontiff this Christmas.},
	journal = {The BMJ},
	author = {Payne, Gareth C. and Payne, Rebecca E. and Farewell, Daniel M.},
	month = dec,
	year = {2008},
	file = {Payne et al. - Rugby (the religion of Wales) and its influence on.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\UMCE8BPT\\Payne et al. - Rugby (the religion of Wales) and its influence on.pdf:application/pdf},
}

@article{messerli_chocolate_2012,
	title = {Chocolate {Consumption}, {Cognitive} {Function}, and {Nobel} {Laureates}},
	volume = {367},
	journal = {The New England Journal of Medicine},
	author = {Messerli, Franz H},
	month = oct,
	year = {2012},
	pages = {1562--1564},
	file = {Messerli - 2012 - Chocolate Consumption, Cognitive Function, and Nob.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\69DIL7UA\\Messerli - 2012 - Chocolate Consumption, Cognitive Function, and Nob.pdf:application/pdf},
}

@book{klein_survival_2003,
	edition = {2nd},
	title = {Survival {Analysis}: {Techniques} for {Censored} and {Truncated} {Data}},
	isbn = {0-387-95399-X},
	language = {English},
	publisher = {Springer},
	author = {Klein, John P. and Moeschberger, Melvin L.},
	year = {2003},
}

@article{zhang_survival_2017,
	title = {Survival analysis in the presence of competing risks},
	volume = {5},
	abstract = {Survival analysis in the presence of competing risks imposes additional challenges for clinical
investigators in that hazard function (the rate) has no one-to-one link to the cumulative incidence function
(CIF, the risk). CIF is of particular interest and can be estimated non-parametrically with the use cuminc()
function. This function also allows for group comparison and visualization of estimated CIF. The effect of
covariates on cause-specific hazard can be explored using conventional Cox proportional hazard model by
treating competing events as censoring. However, the effect on hazard cannot be directly linked to the effect
on CIF because there is no one-to-one correspondence between hazard and cumulative incidence. FineGray model directly models the covariate effect on CIF and it reports subdistribution hazard ratio (SHR).
However, SHR only provide information on the ordering of CIF curves at different levels of covariates, it has
no practical interpretation as HR in the absence of competing risks. Fine-Gray model can be fit with crr()
function shipped with the cmprsk package. Time-varying covariates are allowed in the crr() function, which
is specified by cov2 and tf arguments. Predictions and visualization of CIF for subjects with given covariate
values are allowed for crr object. Alternatively, competing risk models can be fit with riskRegression package by
employing different link functions between covariates and outcomes. The assumption of proportionality can
be checked by testing statistical significance of interaction terms involving failure time. Schoenfeld residuals
provide another way to check model assumption.},
	number = {3},
	journal = {Annals of Translational Medicine.},
	author = {Zhang, Zhongheng},
	year = {2017},
	pages = {9},
	file = {Zhang - 2017 - Survival analysis in the presence of competing ris.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\D4RJBL9P\\Zhang - 2017 - Survival analysis in the presence of competing ris.pdf:application/pdf},
}

@techreport{bayne-jones_smoking_1964,
	type = {{REPORT} {OF} {THE} {ADVISORY} {COMMITTEE} {TO} {THE} {SURGEON} {GENERAL} {OF} {THE} {PUBLIC} {HEALTH} {SERVICE}},
	title = {Smoking and {Health}},
	number = {1103},
	author = {Bayne-Jones, Stanhope and Burdette, Walter J and Cochran, William G and Farber, Emmanuel},
	year = {1964},
	file = {Bayne-Jones et al. - Smoking and Health.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\U2AFMQJD\\Bayne-Jones et al. - Smoking and Health.pdf:application/pdf},
}

@article{lu_biomechanics_2012,
	title = {Biomechanics of human movement and its clinical applications},
	volume = {28},
	abstract = {All life forms on earth, including humans, are constantly subjected to the universal
force of gravitation, and thus to forces from within and surrounding the body. Through the
study of the interaction of these forces and their effects, the form, function and motion of
our bodies can be examined and the resulting knowledge applied to promote quality of life.
Under gravity and other loads, and controlled by the nervous system, human movement is
achieved through a complex and highly coordinated mechanical interaction between bones,
muscles, ligaments and joints within the musculoskeletal system. Any injury to, or lesion in,
any of the individual elements of the musculoskeletal system will change the mechanical interaction and cause degradation, instability or disability of movement. On the other hand, proper
modification, manipulation and control of the mechanical environment can help prevent
injury, correct abnormality, and speed healing and rehabilitation. Therefore, understanding
the biomechanics and loading of each element during movement using motion analysis is helpful for studying disease etiology, making decisions about treatment, and evaluating treatment
effects. In this article, the history and methodology of human movement biomechanics, and
the theoretical and experimental methods developed for the study of human movement,
are reviewed. Examples of motion analysis of various patient groups, prostheses and orthoses,
and sports and exercises, are used to demonstrate the use of biomechanical and
stereophotogrammetry-based human motion analysis studies to address clinical issues. It is
suggested that further study of the biomechanics of human movement and its clinical applications will benefit from the integration of existing engineering techniques and the continuing
development of new technology.},
	journal = {Kaohsiung Journal of Medical Sciences},
	author = {Lu, Tung-Wu and Chang, Chu-Fen},
	year = {2012},
	pages = {S13--S25},
}

@article{scirvani_temporomandibular_2008,
	title = {Temporomandibular {Disorders}},
	volume = {308},
	journal = {The New England Journal of Medicine},
	author = {Scirvani, Steven J. and Keith, David A. and Kaban, Leonard B.},
	year = {2008},
	pages = {2693--2705},
}

@article{rubin_inference_1976,
	title = {Inference and missing data},
	volume = {63},
	abstract = {When making sampling distribution inferences about the parameter of the data, 0, it is
 appropriate to ignore the process that causes missing data if the missing data are 'missing
 at random' and the observed data are 'observed at random', but these inferences are
 generally conditional on the observed pattern of missing data. When making direct-
 likelihood or Bayesian inferences about 0, it is appropriate to ignore the process that causes
 missing data if the missing data are missing at random and the parameter of the missing data
 process is 'distinct' from 0. These conditions are the weakest general conditions under which
 ignoring the process that causes missing data always leads to correct inferences.},
	number = {3},
	journal = {Biometrika},
	author = {Rubin, Donald B.},
	year = {1976},
	pages = {581--592},
	file = {Rubin - 1976 - Inference and missing data.pdf:C\:\\Users\\Jack Swanson\\Zotero\\storage\\E82RYDAU\\Rubin - 1976 - Inference and missing data.pdf:application/pdf},
}

@article{popper_propensity_1959,
	title = {The {Propensity} {Interpreation} of {Probability}},
	volume = {10},
	number = {37},
	journal = {The British Journal for the Philosophy of Science},
	author = {Popper, Karl R.},
	month = may,
	year = {1959},
	pages = {25--42},
}

@article{ranstam_why_2012,
	title = {Why the {P}-value culture is bad and confidence intervals a better alternative},
	volume = {20},
	abstract = {In spite of frequent discussions of misuse and misunderstanding of probability values (P-values) they still
appear in most scientific publications, and the disadvantages of erroneous and simplistic P-value
interpretations grow with the number of scientific publications. Osteoarthritis and Cartilage prefer
confidence intervals. This is a brief discussion of problems surrounding P-values and confidence intervals},
	journal = {Osteoarthritis and Cartilage},
	author = {Ranstam, J.},
	year = {2012},
	pages = {805--808},
}

@article{stanton_galton_2001,
	title = {Galton, {Pearson}, and the {Peas}: {A} {Brief} {History} of {Linear} {Regression} for {Statistics} {Instructors}},
	volume = {9},
	abstract = {An examination of publications of Sir Francis Galton and Karl Pearson revealed that Galton's work on
inherited characteristics of sweet peas led to the initial conceptualization of linear regression. Subsequent
efforts by Galton and Pearson brought about the more general techniques of multiple regression and the
product-moment correlation coefficient. Modern textbooks typically present and explain correlation prior to
introducing prediction problems and the application of linear regression. This paper presents a brief history
of how Galton originally derived and applied linear regression to problems of heredity. This history
illustrates additional approaches instructors can use to introduce simple linear regression to students.},
	number = {3},
	journal = {Journal of Statistics Education},
	author = {Stanton, Jeffrey M.},
	year = {2001},
	pages = {13},
}

@book{morris_famine_2023,
	series = {Elements in {Ancient} {Egypt} in {Context}},
	title = {Famine and {Feast} in {Ancient} {Egypt}},
	isbn = {978-1-00-907458-2},
	abstract = {This Element is about the creation and curation of social
memory in pharaonic and Greco-Roman Egypt. Ancient, Classical,
Medieval, and Ottoman sources attest to the horror that characterized
catastrophic famines. Occurring infrequently and rarely reaching the
canonical seven-years’ length, famines appeared and disappeared like
nightmares. Communities that remain aware of potentially recurring
tragedies are often advantaged in their efforts to avert or ameliorate
worst-case scenarios. For this and other reasons, pharaonic and
Greco-Roman Egyptians preserved intergenerational memories of
hunger and suffering. This Element begins with a consideration of the
trajectories typical of severe Nilotic famines and the concept of social
memory. It then argues that personal reflection and literature,
prophecy, and an annual festival of remembrance functioned – at
different times, and with varying degrees of success – to convince the
well-fed that famines had the power to unseat established order and to render a comfortably familiar world unrecognizable.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Morris, Ellen},
	month = jun,
	year = {2023},
}

@book{jaynes_probability_2003,
	title = {Probability {Theory}},
	isbn = {978-0-511-06589-7},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Jaynes, E.T.},
	year = {2003},
}

@article{fienberg_when_2006,
	title = {When {Did} {Bayesian} {Inference} {Become} "{Bayesian}"},
	volume = {1},
	number = {1},
	journal = {Bayesian Analysis},
	author = {Fienberg, Stephen E.},
	year = {2006},
	pages = {1--40},
}

@article{fendler_history_2008,
	title = {The {History} of the {Bell} {Curve}: {Sorting} and the {Idea} of {Normal}},
	volume = {58},
	abstract = {Bell-curve thinking, as a model of distribution of success and failure in society, enjoys a perennial (ahistorical, objective, and law-like) status in education. As such it provides a rationale for sorting (tracking or streaming) practices in education, which has led many educators to criticize both bell-curve thinking and associated sorting practices. In this essay, Lynn Fendler and Irfan Muzaffar argue that the existing critiques of bell-curve thinking ring true for people who believe that the purpose of schooling is to promote a more equitable redistribution of resources in society; however, these arguments do not criticize the law-like character assumed for a bell curve as a representation of social reality. To extend these critiques, Fendler and Muzaffar focus on the history of the bell curve, from a representation of binomial probability, to a bearer of real things in nature, and finally to a set of expectations about how people should behave. They ultimately argue that the acceptance of bell-curve thinking in education is part of a recursive project of governance and normalization.},
	number = {1},
	journal = {Educational Theory},
	author = {Fendler, Lynn and Muzaffar, Irfan},
	year = {2008},
	pages = {63--82},
}

@article{pederson_fiducial_1978,
	title = {Fiducial {Inference}},
	volume = {46},
	number = {2},
	journal = {International Statistical Institute},
	author = {Pederson, J.G.},
	month = aug,
	year = {1978},
	pages = {147--170},
}

@article{zabell_fisher_2022,
	title = {Fisher, {Bayes}, and {Predictive} {Inference}},
	volume = {10},
	abstract = {We review historically the position of Sir R.A. Fisher towards Bayesian inference and, particulary, the classical Bayes-Laplace paradigm.  We focus on his Fiducial Argument.},
	number = {10},
	journal = {Mathematics},
	author = {Zabell, Sandy},
	year = {2022},
}

@article{ebrahimi_maximum_2000,
	title = {The {Maximum} {Entropy} {Method} for {Lifetime} {Distributions}},
	volume = {62},
	abstract = {An approach to produce a model for the data generating distribution is the
 well-known maximum entropy method. In this approach, the partial knowledge about the data
 generating distribution is formulated in terms of a set of information constraints, usually moment
 constraints, and the inference is based on the model that maximizes Shannon's entropy under
 these constraints. In this paper we investigate several problems of hazard rate function estimation
 based on the maximum entropy principle. The potential applications include developing several
 classes of the maximum entropy distributions which can be used to model different data-generating
 distributions that satisfy certain information constraints on the hazard rate function.},
	number = {2},
	journal = {Indian Statistical Institute},
	author = {Ebrahimi, Nader},
	month = jun,
	year = {2000},
	pages = {236--243},
}

@unpublished{efron_r_1998,
	type = {Lecture},
	title = {R.{A}. {Fisher} in the 21st {Century}},
	abstract = {t. Fisher is the single most important figure in 20th century
 statistics. This talk examines his influence on modern statistical think-
 ing, trying to predict how Fisherian we can expect the 21st century to
 be. Fisher's philosophy is characterized as a series of shrewd compro-
 mises between the Bayesian and frequentist viewpoints, augmented by
 some unique characteristics that are particularly useful in applied
 problems. Several current research topics are examined with an eye
 toward Fisherian influence, or the lack of it, and what this portends for
 future statistical developments. Based on the 1996 Fisher lecture, the
 article closely follows the text of that talk.},
	language = {English},
	author = {Efron, Bradley},
	month = may,
	year = {1998},
}

@article{kendall_studies_1960,
	title = {Studies in the {History} of {Probability} and {Statistics}: {Where} {Shall} the {History} of {Statistics} {Begin}?},
	volume = {47},
	journal = {Biometrika},
	author = {Kendall, M.G.},
	year = {1960},
	pages = {447--449},
}

@article{fienberg_review_1992,
	title = {Review: {A} {Brief} {History} of {Statistics} in {Three} and {One}-{Half} {Chapters}: {A} {Review} {Essay}},
	volume = {7},
	number = {2},
	journal = {Statistical Science},
	author = {Fienberg, Stephen E.},
	month = may,
	year = {1992},
	pages = {208--225},
}

@article{broemeling_account_2011,
	title = {An {Account} of {Early} {Statistical} {Inference} in {Arab} {Cryptology}},
	volume = {65},
	abstract = {Recently discovered manuscripts reveal an early use of statistical inference in Arab cryptology in the eighth through twelfth centuries A.D. The manuscripts report that al-Kindi (801-873) used relative frequency analysis some 1100 years ago to decode messages, some 800 years before the correspondence between Pascal and Fermat. Among his many accomplishments, al-Kindi is the author of the oldest known book on cryptology. This article describes an early use of statistical methods for the cryptanalysis of encrypted messages.},
	language = {English},
	number = {4},
	journal = {Ther American Statistician},
	author = {Broemeling, Lyle D.},
	month = nov,
	year = {2011},
	pages = {255--257},
}

@article{glass_john_1964,
	title = {John {Graunt} and {His} {Natural} and {Political} {Observations}},
	volume = {19},
	language = {English},
	number = {1},
	journal = {Notes and Records of the Royal Society of London},
	author = {Glass, D.V.},
	month = jun,
	year = {1964},
	pages = {63--100},
}

@book{pask_magnificent_2013,
	address = {United State of America},
	title = {Magnificent {Principia}: {Exploring} {Isaac} {Newton}'s {Masterpiece}},
	publisher = {Prometheus Books},
	author = {Pask, Collin},
	year = {2013},
}

@book{stigler_history_1986,
	title = {The {History} of {Statistics}: {The} {Measurement} {ofUncertainty} before 1900},
	publisher = {The Belknap Press of Harvard University Press},
	author = {Stigler, Stephen M.},
	year = {1986},
}

@article{barcella_variable_2016,
	title = {Variable {Selection} in {Covariate} {Dependent} {Random} {Partition} {Models}: an {Application} to {Urinary} {Tract} {Infection}},
	volume = {35},
	abstract = {Lower urinary tract symptoms (LUTS) can indicate the presence of urinary
tract infection (UTI), a condition that if it becomes chronic requires expensive
and time consuming care as well as leading to reduced quality of life. Detecting
the presence and gravity of an infection from the earliest symptoms is then highly
valuable. Typically, white blood cell count (WBC) measured in a sample of urine
is used to assess UTI. We consider clinical data from 1341 patients at their first
visit in which UTI (i.e. WBC≥ 1) is diagnosed. In addition, for each patient, a
clinical profile of 34 symptoms was recorded. In this paper we propose a Bayesian
nonparametric regression model based on the Dirichlet Process (DP) prior aimed at
providing the clinicians with a meaningful clustering of the patients based on both
the WBC (response variable) and possible patterns within the symptoms profiles
(covariates). This is achieved by assuming a probability model for the symptoms as
well as for the response variable. To identify the symptoms most associated to UTI,
we specify a spike and slab base measure for the regression coefficients: this induces
dependence of symptoms selection on cluster assignment. Posterior inference is
performed through Markov Chain Monte Carlo methods},
	number = {8},
	journal = {Statistics in Medicine},
	author = {Barcella, William and De Iorio, Maria and Baio, Gianluca and Malone-Lee, James},
	month = apr,
	year = {2016},
	pages = {1371--1389},
}

@article{cao_bayesian_2020,
	title = {Bayesian variable selection in logistic regression with application to whole brain functional connectivity analysis for {Parkinson}’s disease},
	volume = {30},
	abstract = {Parkinson’s disease is a progressive, chronic, and neurodegenerative disorder that is primarily diagnosed by clinical
examinations and magnetic resonance imaging (MRI). In this paper, we propose a Bayesian model to predict Parkinson’s
disease employing a functional MRI (fMRI) based radiomics approach. We consider a spike and slab prior for variable
selection in high-dimensional logistic regression models, and present an approximate Gibbs sampler by replacing a
logistic distribution with a t-distribution. Under mild conditions, we establish model selection consistency of the induced
posterior and illustrate the performance of the proposed method outperforms existing state-of-the-art methods
through simulation studies. In fMRI analysis, 6216 whole-brain functional connectivity features are extracted for
50 healthy controls along with 70 Parkinson’s disease patients. We apply our method to the resulting dataset and
further show its benefits with a higher average prediction accuracy of 0.83 compared to other contenders based on
10 random splits. The model fitting procedure also reveals the most discriminative brain regions for Parkinson’s disease.
These findings demonstrate that the proposed Bayesian variable selection method has the potential to support
radiological diagnosis for patients with Parkinson’s disease.},
	number = {3},
	journal = {Statistical Methods in Medical Research},
	author = {Cao, Xuan and Lee, Kyoungjae and Huang, Qingling},
	year = {2020},
	pages = {826--842},
}

@article{lu_bayesian_2022,
	title = {Bayesian approaches to variable selection: a comparative study from practical perspectives},
	volume = {18},
	abstract = {In many clinical studies, researchers are interested in parsimonious models that simultaneously
achieve consistent variable selection and optimal prediction. The resulting parsimonious models will facilitate
meaningful biologicalinterpretation and scientific findings.Variable selection via Bayesianinference has been
receiving significant advancement in recent years. Despite its increasing popularity, there is limited practical
guidance for implementing these Bayesian approaches and evaluating their comparative performance in
clinical datasets. In this paper, we review several commonly used Bayesian approaches to variable selection,
with emphasis on application and implementation through R software. These approaches can be roughly
categorized into four classes: namely the Bayesian model selection, spike-and-slab priors, shrinkage priors,
and the hybrid of both. To evaluate their variable selection performance under various scenarios, we compare
these four classes of approaches using real and simulated datasets. These results provide practical guidance
to researchers who are interested in applying Bayesian approaches for the purpose of variable selection.},
	number = {1},
	journal = {The International Journal of Biostatistics},
	author = {Lu, Zihang and Lou, Wendy},
	year = {2022},
	pages = {83--108},
}

@article{de_fraine_analysis_2005,
	title = {An {Analysis} of {Well}-{Being} in {Secondary} {School} with {Multilevel} {Growth} {Curve} models and {Multilevel} {Multivariate} {Models}},
	volume = {39},
	abstract = {This paper reports on an exploration of student well-being in secondary school. A
well-being questionnaire was administered four times to the same students. Multilevel models
were applied in which measurements are grouped within students within schools. Differences
between students are large, but there are only minor differences between schools regarding
the well-being. Two methods of analysis of longitudinal data are compared: a multilevel multivariate approach and a multilevel growth curve analysis. It is shown that the estimation of
individual growth curves is an elegant and parsimonious way of modelling. The multivariate approach on the other hand is a more modest model. The assumptions, advantages and
disadvantages of both perspectives are listed},
	journal = {Quality and Quantity},
	author = {De Fraine, Bieke and Van Landeghem, Georges and Van Damme, Jan and Onghena, Patrick},
	year = {2005},
	pages = {297--316},
}

@article{wallace_multilevel_2017,
	title = {Multilevel analysis exploring the links between stress, depression, and sleep problems among two-year college  students},
	volume = {65},
	abstract = {Objective—This study explored the association of stress and depression with a multidimensional 
sleep problems construct in a sample of 2-year college students.
Participants—The sample consisted of 440 students enrolled in 2-year study from Fall 2011 to 
Fall 2013.
Methods—Participants in an obesity prevention study completed surveys assessing sleep, stress, 
and depression at baseline, 4, 12, and 24 months. Multilevel models predicting sleep problems 
were conducted to distinguish episodic from chronic reports of stress and depression.
Results—Participants were primarily women (68\%), white (73\%), young adults (M age = 22.8), 
with an average of 8.4 hours of sleep per night. Neither stress nor depression was predictive of 
sleep quantity; however, they were predictive of sleep quality.
Conclusions—Results show that sleep quality rather than sleep quantity may be the greater 
health concern for young adults, suggesting that intervention programs targeting depression, stress 
management, and healthy sleep patterns are warranted.},
	number = {3},
	journal = {Journal of American College Health},
	author = {Wallace, Deshhira D. and Boynton, Marcella H. and Lytle, Leslie A.},
	year = {2017},
	pages = {187--196},
}

@article{underhill_depression_2003,
	title = {Depression and life satisfaction in patients with traumatic brain injury: a longitudinal study},
	volume = {17},
	abstract = {Primary objective: To assess the relationship between depression and life satisfaction among survivors oftraumatic brain injury (TBI) over a 3-year period after injury. It was hypothesized that survivors of TBIwith depression would have decreased life satisfaction.Research design: Two groups (depression vs no depression) longitudinal design.Methods and procedures: Interviewed survivors of TBI (n ¼ 324) by telephone at 24, 48 and 60 monthsafter hospitalization. At the 24-month interview, 90 (27.8\%) respondents reported a post-injurydiagnosis of depression and 234 (72.2\%) reported no diagnosis. Respondents then completed theLife Satisfaction Index I-A, which was repeated at the 48- and 60-month interviews.Main outcomes and results: The depression group had significantly lower life satisfaction than the nodepression group at 24-, 48- and 60-month interviews.Conclusions: Depression and diminished life satisfaction among survivors of TBI are persistent problemsthat require the close attention of medical and rehabilitation professionals.},
	number = {11},
	journal = {Brain Injury},
	author = {Underhill, Andrea and Lobello, Steven G. and Stroud, Thomas P. and Terry, Katherine S. and Devivo, Michael J. and Fine, Philip R.},
	year = {2003},
	pages = {973--982},
}

@article{underhill_reliability_2004,
	title = {Reliability and validity of the {Family} {Satisfaction} {Scale} with survivors of traumatic brain injury},
	volume = {41},
	abstract = {—For this study, we investigated the reliability and
validity of the FSS (Family Satisfaction Scale) in survivors of
traumatic brain injury (TBI). The FSS was administered during
the 12- and 60-month follow-up interviews. Data analyses
included Cronbach’s Alpha to determine internal consistency
and analysis of variance to determine the relationship of FSS
total score to Life Satisfaction Index-A (LSI-A) total scores,
marital status, living arrangement, and number of family contacts outside the home. Cronbach’s Alphas were 0.94
(12 months, N = 541) and 0.95 (60 months, N = 340). FSS total
score and marital status were significantly related at both
12 months (F3, 534 = 6.04, p {\textless} 0.001) and 60 months postdischarge (F3, 335 = 4.52, p {\textless} 0.005). FSS total scores are correlated with the number of family contacts (r342 = 0.12, p {\textless} 0.03)
and with LSI-A total scores (r337 = 0.43, p {\textless} 0.001). The FSS
has excellent internal consistency with survivors of TBI. We
also demonstrated the evidence of convergent validity},
	number = {4},
	journal = {Journal of Rehabilitation Research and Development},
	author = {Underhill, Andrea T. and LoBello, Steven G. and Fine, Philip R.},
	month = aug,
	year = {2004},
	pages = {603--610},
}

@article{kwok_analyzing_2008,
	title = {Analyzing {Longitudinal} {Data} with {Multilevel} {Models}: {An} {Example} with {Individuals} {Living} with {Lower} {Extremity} {Intra}-articular {Fractures}},
	volume = {53},
	abstract = {The use and quality of longitudinal research designs has increased over the past two decades, and
new approaches for analyzing longitudinal data, including multi-level modeling (MLM) and latent
growth modeling (LGM), have been developed. The purpose of this paper is to demonstrate the use
of MLM and its advantages in analyzing longitudinal data. Data from a sample of individuals with
intra-articular fractures of the lower extremity from the University of Alabama at Birmingham’s
Injury Control Research Center is analyzed using both SAS PROC MIXED and SPSS MIXED. We
start our presentation with a discussion of data preparation for MLM analyses. We then provide
example analyses of different growth models, including a simple linear growth model and a model
with a time-invariant covariate, with interpretation for all the parameters in the models. More
complicated growth models with different between- and within-individual covariance structures and
nonlinear models are discussed. Finally, information related to MLM analysis such as online
resources is provided at the end of the paper.},
	number = {3},
	journal = {Rehabilitation Psychology},
	author = {Kwok, Oi-Man and Underhill, Andrea T. and Berry, Jack W. and Luo, Wen and Elliott, Timothy R. and Yoon, Myeongsun},
	month = aug,
	year = {2008},
	pages = {370--386},
}

@mastersthesis{ra_variable_2011,
	address = {Athens, Georgia},
	title = {Variable {Selection} in {Longitudinal} data with {Application} to {Education}},
	abstract = {Variable selection with a large number of predictors is a very challenging and important
problem in multiple linear regression. However, relatively little attention has been paid to
issues of variable selection in longitudinal data with application to education. This study
examines data in which reading achievement of TOEIC measured for each quarter in a year
is a response variables and other predictors such as gender, socioeconomic status (SES), and
majors are used as predictors. Using this longitudinal educational data, we compare multiple
regression, backward elimination, group least selection absolute shrinkage and selection oper ator (LASSO), and linear mixed models in terms of their performance in variable selection.
In our case study, the results show that four different statistical methods contain different
sets of predictors in their models. The linear mixed model (LMM) provides the smallest
number of predictors (4 predictors among a total of 19 predictors). In addition, LMM is the
only appropriate method for the repeated measurement and is the best method with respect
to the principal of parsimony. We also provide interpretation of the selected model by LMM
in the conclusion.},
	language = {English},
	school = {The University of Georgia},
	author = {Ra, Jongmin},
	year = {2011},
}

@article{song_simulation_2013,
	title = {Simulation of longitudinal exposure data with variance-covariance structures based on mixed model},
	volume = {33},
	abstract = {Longitudinal data are important in exposure and risk assessments, especially for pollutants with long
half-lives in the human body and where chronic exposures to current levels in the environment raise
concerns for human health effects. It is usually difficult and expensive to obtain large longitudinal
data sets for human exposure studies. This paper reports a new simulation method to generate
longitudinal data with flexible numbers of subjects and days. Mixed models are used to describe the
variance-covariance structures of input longitudinal data. Based on estimated model parameters,
simulation data are generated with similar statistical characteristics compared to the input data. Three
criteria are used to determine similarity: the overall mean and standard deviation, the variance
components percentages, and the average autocorrelation coefficients. Upon the discussion of mixed
models, a simulation procedure is produced and numerical results are shown through one human
exposure study. Simulations of three sets of exposure data successfully meet above criteria. In
particular, simulations can always retain correct weights of inter- and intra- subject variances as in
the input data. Autocorrelations are also well followed. Compared with other simulation algorithms,
this new method stores more information about the input overall distribution so as to satisfy the above
multiple criteria for statistical targets. In addition, it generates values from numerous data sources
and simulates continuous observed variables better than current data methods. This new method also
provides flexible options in both modeling and simulation procedures according to various user
requirements.},
	number = {3},
	journal = {Risk Analysis},
	author = {Song, Peng and Xue, Jianping},
	month = mar,
	year = {2013},
	pages = {469--479},
}

@article{van_der_borght_multi-model_2014,
	title = {Multi-model inference using mixed effects from a inear regression based genetic algorithm},
	volume = {15},
	abstract = {Background: Different high-dimensional regression methodologies exist for the selection of variables to predict a
continuous variable. To improve the variable selection in case clustered observations are present in the training
data, an extension towards mixed-effects modeling (MM) is requested, but may not always be straightforward to
implement.
In this article, we developed such a MM extension (GA-MM-MMI) for the automated variable selection by a linear
regression based genetic algorithm (GA) using multi-model inference (MMI). We exemplify our approach by training
a linear regression model for prediction of resistance to the integrase inhibitor Raltegravir (RAL) on a genotype-phenotype
database, with many integrase mutations as candidate covariates. The genotype-phenotype pairs in this database were
derived from a limited number of subjects, with presence of multiple data points from the same subject, and with an
intra-class correlation of 0.92.
Results: In generation of the RAL model, we took computational efficiency into account by optimizing the GA
parameters one by one, and by using tournament selection. To derive the main GA parameters we used 3 times 5-fold
cross-validation. The number of integrase mutations to be used as covariates in the mixed effects models was 25
(chrom.size). A GA solution was found when R2
MM {\textgreater} 0.95 (goal.fitness). We tested three different MMI approaches to
combine the results of 100 GA solutions into one GA-MM-MMI model. When evaluating the GA-MM-MMI performance
on two unseen data sets, a more parsimonious and interpretable model was found (GA-MM-MMI TOP18: mixed-effects
model containing the 18 most prevalent mutations in the GA solutions, refitted on the training data) with better
predictive accuracy (R2
) in comparison to GA-ordinary least squares (GA-OLS) and Least Absolute Shrinkage and
Selection Operator (LASSO).
Conclusions: We have demonstrated improved performance when using GA-MM-MMI for selection of mutations on a
genotype-phenotype data set. As we largely automated setting the GA parameters, the method should be applicable
on similar datasets with clustered observations.},
	number = {88},
	journal = {BMC Bioinformatics},
	author = {Van der Borght, Koen and Verbeke, Geert and van Vlijmen, Herman},
	year = {2014},
}

@article{van_der_borght_quantitative_2013,
	title = {Quantitative prediction of integrase inhibitor resistance from genotype through consensus linear regression modeling},
	volume = {10},
	issn = {1743-422X},
	url = {https://doi.org/10.1186/1743-422X-10-8},
	doi = {10.1186/1743-422X-10-8},
	abstract = {Integrase inhibitors (INI) form a new drug class in the treatment of HIV-1 patients. We developed a linear regression modeling approach to make a quantitative raltegravir (RAL) resistance phenotype prediction, as Fold Change in IC50 against a wild type virus, from mutations in the integrase genotype.},
	number = {1},
	journal = {Virology Journal},
	author = {Van der Borght, Koen and Verheyen, Ann and Feyaerts, Maxim and Van Wesenbeeck, Liesbeth and Verlinden, Yvan and Van Craenenbroeck, Elke and van Vlijmen, Herman},
	month = jan,
	year = {2013},
	pages = {8},
}

@article{schreuder_participation_2023,
	title = {Participation and compliance in a 6-month daily diary study among individuals at risk for mental health problems.},
	volume = {35},
	issn = {1939-134X(Electronic),1040-3590(Print)},
	doi = {10.1037/pas0001197},
	abstract = {Intensive longitudinal (IL) measurement, which involves prolonged self-monitoring, may have important clinical applications but is also burdening. This raises the question who takes part in and successfully completes IL measurements. This preregistered study investigated which demographic, personality, economic, social, psychological, or physical participant characteristics are associated with participation and compliance in an IL study conducted in young adults at enhanced risk for psychopathology. Dutch young adults enrolled in the clinical cohort of the TRacking Adolescents’ Individual Lives Survey (TRAILS) were invited to a 6-month daily diary study. Participant characteristics came from five earlier TRAILS assessment waves collected from Age 11 onwards. To evaluate participation, we compared diary study participants (N = 134) to nonparticipants (N = 309) and a sex-matched subsample (N = 1926) of individuals from the general population cohort of TRAILS. To evaluate compliance, we analyzed which characteristics were related to the proportion of completed diary entries. We found that participants (23.6 ± 0.7 years old; 57\% male) were largely similar to nonparticipants. In addition, compared to the general population, participants reported more negative scores on nearly all characteristics. Internalizing problems predicted higher compliance. Externalizing problems, antisocial behavior, and daily smoking predicted lower compliance. Thus, in at-risk young adults, who scored lower on nearly every positive characteristic and higher on every negative characteristic relative to the general population, participation in a diary study is unbiased. Small biases in compliance occur, of which researchers should be aware. IL measurement is thus suitable in at-risk populations, which is a requirement for its usefulness in clinical practice. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Assessment},
	author = {Schreuder, Marieke J. and Groen, Robin N. and Wigman, Johanna T. W. and Wichers, Marieke and Hartman, Catharina A.},
	year = {2023},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {*At Risk Populations, *Compliance, *Mental Disorders, *Participation, Psychopathology},
	pages = {115--126},
}

@article{ormel_functional_2017,
	title = {Functional outcomes of child and adolescent mental disorders. {Current} disorder most important but psychiatric history matters as well},
	volume = {47},
	doi = {10.1017/S0033291716003445},
	number = {7},
	journal = {Psychological Medicine},
	author = {Ormel, J. and Oerlemans, A. M. and Raven, D. and Laceulle, O. M. and Hartman, C. A. and Veenstra, R. and Verhulst, F. C. and Vollebergh, W. and Rosmalen, J. G. M. and Reijneveld, S. A. and al, et},
	year = {2017},
	pages = {1271--1282},
}

@article{scollon_experience_2003,
	title = {Experience {Sampling}: {Promises} and {Pitfalls}, {Strengths} and {Weaknesses}},
	volume = {4},
	issn = {1573-7780},
	url = {https://doi.org/10.1023/A:1023605205115},
	doi = {10.1023/A:1023605205115},
	number = {1},
	journal = {Journal of Happiness Studies},
	author = {Scollon, Christie N. and Kim-Prieto, Chu and Diener, Ed},
	month = mar,
	year = {2003},
	pages = {5--34},
}

@article{tilling_multilevel_2001,
	title = {Multilevel growth curve models with covariate effects: application to recovery after stroke.},
	volume = {20},
	copyright = {Copyright 2001 John Wiley \& Sons, Ltd.},
	issn = {0277-6715},
	doi = {10.1002/sim.697},
	abstract = {In measuring the progression of, or recovery from, a disease an individual's outcome may be assessed on a number of occasions. A model of the relationship  between outcome and time since disease occurred which accounts for patient  characteristics could be used to describe patterns of recovery, to predict  outcome for a patient, or to evaluate health interventions. We use multilevel  models to analyse such data, focusing on the choice of powers of time both for  mean outcome and covariate effects. We give equations for predicted outcome and  corresponding standard errors (i) based only on baseline characteristics, and  (ii) by conditioning on previous outcomes for an individual. In a study of 331  stroke patients, outcome was measured approximately 0, 2,4,6 and 12 months after  stroke. Patient characteristics included age, sex, and pre-stroke handicap,  together with stroke-severity indicators (presence of limb deficit, dysphasia,  dysarthria or incontinence). Of these, only the effects of age, dysphasia and  presence of deficit varied with time. Conditioning on previous observations  improved the accuracy of predictions. The outcome variable clearly had a skewed  distribution, and the model residuals showed evidence of non-Normality. We  discuss alternative models for non-Normal data, and show that, here, the standard  (Normal errors) multilevel model gives equivalent parameter estimates and  predictions to those obtained from alternative models.},
	language = {eng},
	number = {5},
	journal = {Statistics in medicine},
	author = {Tilling, K. and Sterne, J. A. and Wolfe, C. D.},
	month = mar,
	year = {2001},
	pmid = {11241571},
	note = {Place: England},
	keywords = {*Models, Biological, *Recovery of Function, *Stroke Rehabilitation, Aged, Aged, 80 and over, Female, Humans, Male, Multivariate Analysis, Predictive Value of Tests, Randomized Controlled Trials as Topic, Reproducibility of Results, Treatment Outcome},
	pages = {685--704},
}

@article{rudd_randomised_1997,
	title = {Randomised controlled trial to evaluate early discharge scheme for patients with stroke.},
	volume = {315},
	issn = {0959-8138 1468-5833},
	doi = {10.1136/bmj.315.7115.1039},
	abstract = {OBJECTIVE: To assess the clinical effectiveness of an early discharge policy for patients with stroke by using a community based rehabilitation team. DESIGN:  Randomised controlled trial to compare conventional care with an early discharge  policy. SETTING: Two teaching hospitals in inner London. SUBJECTS: 331 medically  stable patients with stroke (mean age 71) who lived alone and were able to  transfer independently or who lived with a resident carer and were able to  transfer with help. INTERVENTIONS: 167 patients received specialist community  rehabilitation for up to 3 months after randomisation. 164 patients continued  with conventional hospital and community care. MAIN OUTCOME MEASURES: Barthel  score at 12 months. Secondary outcomes measured impairment with motoricity index,  minimental state examination, and Frenchay aphasia screening test; disability  with the Rivermead activity of daily living scales, hospital anxiety and  depression scale, and 5 m walk; handicap with the Nottingham health profile;  carer stress with caregiver strain index and patient and carer satisfaction. The  main process measure was length of stay after randomisation. RESULTS: One year  after randomisation no significant differences in clinical outcomes were found  apart from increased satisfaction with hospital care in the community therapy  group. Length of stay after randomisation in the community therapy group was  significantly reduced (12 v 18 days; P {\textless} 0.0001). Patients with impairments were  more likely to receive treatment in the community therapy group. CONCLUSIONS:  Early discharge with specialist community rehabilitation after stroke is  feasible, as clinically effective as conventional care, and acceptable to  patients. Considerable reductions in use of hospital beds are achievable.},
	language = {eng},
	number = {7115},
	journal = {BMJ (Clinical research ed.)},
	author = {Rudd, A. G. and Wolfe, C. D. and Tilling, K. and Beech, R.},
	month = oct,
	year = {1997},
	pmid = {9366727},
	pmcid = {PMC2127677},
	note = {Place: England},
	keywords = {Aged, Aged, 80 and over, Female, Humans, Male, *Patient Discharge, Activities of Daily Living, Adult, Anxiety/rehabilitation, Cerebrovascular Disorders/*rehabilitation, Cognition Disorders/rehabilitation, Community Health Services/*statistics \& numerical data, Depression/rehabilitation, Disabled Persons, Length of Stay, London, Middle Aged, Movement Disorders/rehabilitation, Outcome Assessment, Health Care/*statistics \& numerical data, Patient Care Team, Prognosis, State Medicine, Time Factors},
	pages = {1039--1044},
}

@article{vermunt_multilevel_2016,
	title = {Multilevel {Latent} {Variable} {Modeling}: {An} {Application} in {Education} {Testing}},
	volume = {37},
	url = {https://www.ajs.or.at/index.php/ajs/article/view/vol37%2C%20no3%264%20-%206},
	doi = {10.17713/ajs.v37i3&4.309},
	abstract = {A framework for multilevel latent variable modeling is presented that includes many existing models as special cases. It is shown that parameters can be estimated by maximum likelihood using a special variant of the EM algorithm. An application is presented from the field of school effectiveness research. This application uses a novel multilevel mixture item response model which clusters schools based on the students’ latent abilities and the item difficulties.},
	number = {3\&4},
	urldate = {2024-06-21},
	journal = {Austrian Journal of Statistics},
	author = {Vermunt, Jeroen K.},
	month = apr,
	year = {2016},
	note = {Section: Articles},
	pages = {285--299},
}

@article{fox_bayesian_2001,
	title = {Bayesian estimation of a multilevel {IRT} model using gibbs sampling},
	volume = {66},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02294839},
	doi = {10.1007/BF02294839},
	abstract = {In this article, a two-level regression model is imposed on the ability parameters in an item response theory (IRT) model. The advantage of using latent rather than observed scores as dependent variables of a multilevel model is that it offers the possibility of separating the influence of item difficulty and ability level and modeling response variation and measurement error. Another advantage is that, contrary to observed scores, latent scores are test-independent, which offers the possibility of using results from different tests in one analysis where the parameters of the IRT model and the multilevel model can be concurrently estimated. The two-parameter normal ogive model is used for the IRT measurement model. It will be shown that the parameters of the two-parameter normal ogive model and the multilevel model can be estimated in a Bayesian framework using Gibbs sampling. Examples using simulated and real data are given.},
	number = {2},
	journal = {Psychometrika},
	author = {Fox, Jean-Paul and Glas, Cees A. W.},
	month = jun,
	year = {2001},
	pages = {271--288},
}

@inproceedings{wang_doubly_2010,
	title = {Doubly {Regularized} {REML} for {Estimation} and {Selection} of {Fixed} and {Random} {Effects} in {Linear} {Mixed}-{Effects} {Models}},
	url = {https://api.semanticscholar.org/CorpusID:55934017},
	author = {Wang, Sijian and Song, Peter X.-K. and Zhu, Ji},
	year = {2010},
}

@article{rotheram-borus_six-year_2004,
	title = {Six-{Year} {Intervention} {Outcomes} for {Adolescent} {Children} of {Parents} {With} the {Human} {Immunodeficiency} {Virus}},
	volume = {158},
	issn = {1072-4710},
	url = {https://doi.org/10.1001/archpedi.158.8.742},
	doi = {10.1001/archpedi.158.8.742},
	abstract = {Having a parent with the human immunodeficiency virus has a significant negative impact on an adolescent child's adjustment.To assess the adjustment of adolescent children to having a parent with the human immunodeficiency virus over 6 years, following the delivery of a coping skills intervention.A randomized controlled trial with repeated evaluations that was analyzed with an intention-to-treat analysis. A skill-based intervention was delivered in 3 modules over 24 sessions, with the third module being delivered only if parents died.A representative sample of parents with the human immunodeficiency virus (n = 307) and their adolescent children (n = 423) was recruited from the Division of AIDS Services in New York City; 51.5\% (n = 158) of the parents died.Employment and school enrollment, receiving public welfare support, early parenthood, mental health symptoms, and the quality of romantic relationships.Over 6 years, significantly more adolescents in the intervention condition than the control condition were employed or in school (82.58\% vs 68.94\%), were less likely to receive public welfare payments (25.66\% vs 36.65\%), were less likely to have psychosomatic symptoms (mean, 0.24 vs 0.31), were more likely to report better problem-solving and conflict resolution skills in their romantic relationships (mean score, 4.38 vs 4.20), expected to have a partner with a good job (mean, 4.57 vs 4.19), and expected to be married when parenting (mean, 3.05 vs 2.40). With marginal significance, the percentage of parents in the intervention condition (34.6\%) was less than in the control condition (44.1\%).Physicians must consider the psychosocial consequences of illness-related challenges on children and provide interventions.Arch Pediatr Adolesc Med. 2004;158:742-748--{\textgreater}},
	number = {8},
	urldate = {2024-06-21},
	journal = {Archives of Pediatrics \& Adolescent Medicine},
	author = {Rotheram-Borus, Mary Jane and Lee, Martha and Lin, Ying-Ying and Lester, Patricia},
	month = aug,
	year = {2004},
	pages = {742--748},
}

@article{tsuno_workplace_2018,
	title = {Workplace {Bullying} and {Psychological} {Distress}: {A} {Longitudinal} {Multilevel} {Analysis} {Among} {Japanese} {Employees}.},
	volume = {60},
	issn = {1536-5948 1076-2752},
	doi = {10.1097/JOM.0000000000001433},
	abstract = {OBJECTIVE: We sought to investigate the contextual effect of workplace bullying on subsequent individual psychological distress and intention to leave. METHODS:  A longitudinal study was conducted among 3142 Japanese employees in the public  sector. Both the baseline and follow-up questionnaires inquired about demographic  and occupational characteristics, workplace bullying, psychological distress, and  intention to leave. RESULTS: The results of three-level  (individual-division-department) multilevel analyses revealed that division-level  workplace bullying was associated with increased individual-level psychological  distress after adjustment for individual experience of workplace bullying, while  the association between individual experience of bullying and psychological  distress was not statistically significant in the same model. CONCLUSION: The  results of the current study suggest that the presence of bullying in the  workplace can have a detrimental effect on employees' mental health even if they  are not personally victimized.},
	language = {eng},
	number = {12},
	journal = {Journal of occupational and environmental medicine},
	author = {Tsuno, Kanami and Kawachi, Ichiro and Kawakami, Norito and Miyashita, Kazuhisa},
	month = dec,
	year = {2018},
	pmid = {30124499},
	note = {Place: United States},
	keywords = {Aged, Female, Humans, Male, Adult, Middle Aged, *Public Sector/organization \& administration, Adolescent, Bullying/*psychology, Exposure to Violence/psychology, Intention, Japan, Longitudinal Studies, Personnel Turnover, Prospective Studies, Stress, Psychological/*etiology, Surveys and Questionnaires, Workplace Violence/*psychology, Young Adult},
	pages = {1067--1072},
}

@article{bondell_joint_2010,
	title = {Joint variable selection for fixed and random effects in linear mixed-effects models.},
	volume = {66},
	copyright = {© 2010, The International Biometric Society.},
	issn = {1541-0420 0006-341X},
	doi = {10.1111/j.1541-0420.2010.01391.x},
	abstract = {It is of great practical interest to simultaneously identify the important predictors that correspond to both the fixed and random effects components in a  linear mixed-effects (LME) model. Typical approaches perform selection separately  on each of the fixed and random effect components. However, changing the  structure of one set of effects can lead to different choices of variables for  the other set of effects. We propose simultaneous selection of the fixed and  random factors in an LME model using a modified Cholesky decomposition. Our  method is based on a penalized joint log likelihood with an adaptive penalty for  the selection and estimation of both the fixed and random effects. It performs  model selection by allowing fixed effects or standard deviations of random  effects to be exactly zero. A constrained expectation-maximization algorithm is  then used to obtain the final estimates. It is further shown that the proposed  penalized estimator enjoys the Oracle property, in that, asymptotically it  performs as well as if the true model was known beforehand. We demonstrate the  performance of our method based on a simulation study and a real data example.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Bondell, Howard D. and Krishna, Arun and Ghosh, Sujit K.},
	month = dec,
	year = {2010},
	pmid = {20163404},
	pmcid = {PMC2895687},
	note = {Place: England},
	keywords = {Humans, *Linear Models, Algorithms, Biometry/*methods, Computer Simulation, Likelihood Functions, Models, Statistical},
	pages = {1069--1077},
}

@article{ghosh_spatio-temporal_2010,
	title = {Spatio-{Temporal} {Analysis} of {Total} {Nitrate} {Concentrations} {Using} {Dynamic} {Statistical} {Models}},
	volume = {105},
	issn = {01621459},
	url = {http://www.jstor.org/stable/29747062},
	abstract = {[Atmospheric concentrations of total nitrate (TNO₃), defined here as gas-phase nitric acid plus particle-phase nitrate, are difficult to simulate in numerical air quality models due to the presence of a variety of formation pathways and loss mechanisms, some of which are highly uncertain. The goal of this study is to estimate the relative importance of these different pathways across the Eastern United States by identifying empirical relationships that exist between TNO₃ concentrations and a set of covariates (ammonium, sulfate, ozone, wind speed, relative humidity, and precipitation) measured from January 1997 to July 2004. We develop two dynamic statistical models to quantify these relationships. A major advantage of these models over typical linear regression models is that their regression coefficients can vary temporally. Results show that TNO₃ is sensitive to ozone throughout the year, indicating an importance of daytime photochemical production of TNO₃, especially in the Southeast. Sensitivity of TNO₃ to residual ammonium \$(\{{\textbackslash}rm NH\}\_\{4\}{\textasciicircum}\{+\}-2\{{\textbackslash}rm SO\}\_\{4\}{\textasciicircum}\{2-\})\$ is most pronounced during winter, indicating a seasonal importance of gas/particle partitioning that is accentuated in the Midwest. Using a number of physical and chemical explanations, confidence is established in the spatial and temporal patterns of several such empirical relationships. In the future, these relationships may be used quantitatively to improve our mechanistic understanding of TNO₃ formation pathways and loss mechanisms in the atmosphere.]},
	number = {490},
	urldate = {2024-06-21},
	journal = {Journal of the American Statistical Association},
	author = {Ghosh, Sujit K. and Bhave, Prakash V. and Davis, Jerry M. and Lee, Hyeyoung},
	year = {2010},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {538--551},
}

@article{fan_new_2004,
	title = {New {Estimation} and {Model} {Selection} {Procedures} for {Semiparametric} {Modeling} in {Longitudinal} {Data} {Analysis}},
	volume = {99},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214504000001060},
	doi = {10.1198/016214504000001060},
	number = {467},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Li, Runze},
	month = sep,
	year = {2004},
	note = {Publisher: Taylor \& Francis},
	pages = {710--723},
	annote = {doi: 10.1198/016214504000001060},
}

@article{greven_restricted_2008,
	title = {Restricted {Likelihood} {Ratio} {Testing} for {Zero} {Variance} {Components} in {Linear} {Mixed} {Models}},
	volume = {17},
	issn = {10618600},
	url = {http://www.jstor.org/stable/25651233},
	abstract = {[The goal of our article is to provide a transparent, robust, and computationally feasible statistical platform for restricted likelihood ratio testing (RLRT) for zero variance components in linear mixed models. This problem is nonstandard because under the null hypothesis the parameter is on the boundary of the parameter space. Our proposed approach is different from the asymptotic results of Stram and Lee who assumed that the outcome vector can be partitioned into many independent subvectors. Thus, our methodology applies to a wider class of mixed models, which includes models with a moderate number of clusters or nonparametric smoothing components. We propose two approximations to the finite sample null distribution of the RLRT statistic. Both approximations converge weakly to the asymptotic distribution obtained by Stram and Lee when their assumptions hold. When their assumptions do not hold, we show in extensive simulation studies that both approximations outperform the Stram and Lee approximation and the parametric bootstrap. We also identify and address numerical problems associated with standard mixed model software. Our methods are motivated by and applied to a large longitudinal study on air pollution health effects in a highly susceptible cohort. Relevant software is posted as an online supplement.]},
	number = {4},
	urldate = {2024-06-22},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Greven, Sonja and Crainiceanu, Ciprian M. and Küchenhoff, Helmut and Peters, Annette},
	year = {2008},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
	pages = {870--891},
}

@article{marino_covariate_2017,
	title = {Covariate {Selection} for {Multilevel} {Models} with {Missing} {Data}.},
	volume = {6},
	issn = {2049-1573},
	doi = {10.1002/sta4.133},
	abstract = {Missing covariate data hampers variable selection in multilevel regression settings. Current variable selection techniques for multiply-imputed data  commonly address missingness in the predictors through list-wise deletion and  stepwise-selection methods which are problematic. Moreover, most variable  selection methods are developed for independent linear regression models and do  not accommodate multilevel mixed effects regression models with incomplete  covariate data. We develop a novel methodology that is able to perform covariate  selection across multiply-imputed data for multilevel random effects models when  missing data is present. Specifically, we propose to stack the multiply-imputed  data sets from a multiple imputation procedure and to apply a group variable  selection procedure through group lasso regularization to assess the overall  impact of each predictor on the outcome across the imputed data sets. Simulations  confirm the advantageous performance of the proposed method compared with the  competing methods. We applied the method to reanalyze the Healthy  Directions-Small Business cancer prevention study, which evaluated a behavioral  intervention program targeting multiple risk-related behaviors in a  working-class, multi-ethnic population.},
	language = {eng},
	number = {1},
	journal = {Stat (International Statistical Institute)},
	author = {Marino, Miguel and Buxton, Orfeu M. and Li, Yi},
	year = {2017},
	pmid = {28239457},
	pmcid = {PMC5323238},
	note = {Place: United States},
	keywords = {BIC, cancer prevention, group lasso, intervention studies, multilevel, multiple imputation, regularization, Rubin’s rules},
	pages = {31--46},
}

@article{yi_variable_2015,
	title = {Variable {Selection} and {Inference} {Procedures} for {Marginal} {Analysis} of {Longitudinal} {Data} with {Missing} {Observations} and {Covariate} {Measurement} {Error}.},
	volume = {43},
	issn = {0319-5724},
	doi = {10.1002/cjs.11268},
	abstract = {In contrast to extensive attention on model selection for univariate data, research on model selection for longitudinal data remains largely unexplored.  This is particularly the case when data are subject to missingness and  measurement error. To address this important problem, we propose marginal methods  that simultaneously carry out model selection and estimation for longitudinal  data with missing responses and error-prone covariates. Our method have several  appealing features: the applicability is broad because the methods are developed  for a unified framework with marginal generalized linear models; model  assumptions are minimal in that no full distribution is required for the response  process and the distribution of the mismeasured covariates is left unspecified;  and the implementation is straightforward. To justify the proposed methods, we  provide both theoretical properties and numerical assessments.},
	language = {eng},
	number = {4},
	journal = {The Canadian journal of statistics = Revue canadienne de statistique},
	author = {Yi, Grace Y. and Tan, Xianming and Li, Runze},
	month = dec,
	year = {2015},
	pmid = {26877582},
	pmcid = {PMC4751048},
	note = {Place: Canada},
	keywords = {Longitudinal data, Marginal analysis, Measurement error, Missing data, Model selection, Simulation-extrapolation},
	pages = {498--518},
}

@article{ibrahim_fixed_2011,
	title = {Fixed and random effects selection in mixed effects models.},
	volume = {67},
	copyright = {© 2010, The International Biometric Society.},
	issn = {1541-0420 0006-341X},
	doi = {10.1111/j.1541-0420.2010.01463.x},
	abstract = {We consider selecting both fixed and random effects in a general class of mixed effects models using maximum penalized likelihood (MPL) estimation along with the  smoothly clipped absolute deviation (SCAD) and adaptive least absolute shrinkage  and selection operator (ALASSO) penalty functions. The MPL estimates are shown to  possess consistency and sparsity properties and asymptotic normality. A model  selection criterion, called the IC(Q) statistic, is proposed for selecting the  penalty parameters (Ibrahim, Zhu, and Tang, 2008, Journal of the American  Statistical Association 103, 1648-1658). The variable selection procedure based  on IC(Q) is shown to consistently select important fixed and random effects. The  methodology is very general and can be applied to numerous situations involving  random effects, including generalized linear mixed models. Simulation studies and  a real data set from a Yale infant growth study are used to illustrate the  proposed methodology.},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Ibrahim, Joseph G. and Zhu, Hongtu and Garcia, Ramon I. and Guo, Ruixin},
	month = jun,
	year = {2011},
	pmid = {20662831},
	pmcid = {PMC3041932},
	note = {Place: England},
	keywords = {Humans, Biometry/*methods, Computer Simulation, Models, Statistical, *Likelihood Functions, Growth, Infant},
	pages = {495--503},
}

@article{wasserman_maltreatment_1993,
	title = {Maltreatment of children born to cocaine-dependent mothers},
	volume = {147},
	issn = {0002-922X},
	doi = {10.1001/archpedi.1993.02160360066021},
	abstract = {To investigate the relationship between maternal cocaine dependency and child maltreatment in a cohort of young children.
Historical cohort study at an urban, tertiary care medical center.
47 infants, born between January and September 1989, whose mothers were regular users of cocaine during pregnancy, based on history and the results of newborn's urine toxicology screens. These cocaine-exposed infants were matched to a comparison group of 47 infants whose mothers did not use cocaine during pregnancy. Matching was on the basis of birth date, race, method of payment for the hospitalization, and marital status of the mother.
Occurrence of maltreatment (physical abuse, sexual abuse, or neglect), and placement either in foster care or with a substitute caretaker.
By 24 months of life, maltreatment had occurred in 23\% of the cocaine group vs 4\% of the comparison group (risk ratio, 5.5; 95\% confidence interval, 1.3 to 23.5). Physical abuse had occurred in 11\% of the cocaine group vs 2\% of the comparison group, while neglect had occurred in 11\% vs 0\% (P {\textless} .05). Changes in placement had occurred in 20\% of the cocaine group vs 2\% of the comparison group (risk ratio, 10.0; 95\% confidence interval, 1.3 to 75.1). Of the 10 placements, only three were directly linked to an episode of maltreatment.
Children identified during the neonatal period as regularly "exposed" to cocaine in utero are at a substantially increased risk both of maltreatment and of changes in the primary caretaker during the first 24 months of life.},
	number = {12},
	journal = {American journal of diseases of children (1960)},
	author = {WASSERMAN, D. R and LEVENTHAL, J. M},
	year = {1993},
	note = {Place: Chicago, IL
Publisher: American Medical Association},
	keywords = {Female, Male, Prognosis, Cocaine, Human beings, Medical sciences, Mothers, Retrospective Studies, Victims of crimes},
	pages = {1324--1328},
	annote = {ObjectType-Article-2},
}

@article{muthen_longitudinal_1998,
	title = {Longitudinal studies of achievement growth using latent variable modeling},
	volume = {10},
	issn = {1041-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S1041608099801356},
	doi = {10.1016/S1041-6080(99)80135-6},
	abstract = {This article gives a pedagogical description of growth modeling of longitudinal data using latent variable methods. The growth modeling is described using an example of mathematics achievement developing over grades 7 to 10 in two cohorts of students. The article describes the basic idea behind growth modeling of individual differences in growth over time and applies it to mathematics achievement development as a function of background variables such as gender, mother's education, and home resources. The modeling ideas are described in words, diagrams, and formulas. The discussion covers modeling that assesses the form of the growth, the influence of background variables on the growth, multiple-cohort analysis, analysis with missing data, and multiple-group analysis of males and females. A corresponding set of analyses are performed on the mathematics data to illustrate the modeling ideas.},
	number = {2},
	journal = {Learning and Individual Differences},
	author = {Muthen, Bengt O. and Khoo, Siek-Toon},
	month = jan,
	year = {1998},
	pages = {73--101},
}

@article{oravecz_fitting_2018,
	title = {Fitting growth curve models in the {Bayesian} framework},
	volume = {25},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-017-1281-0},
	doi = {10.3758/s13423-017-1281-0},
	abstract = {Growth curve modeling is a popular methodological tool due to its flexibility in simultaneously analyzing both within-person effects (e.g., assessing change over time for one person) and between-person effects (e.g., comparing differences in the change trajectories across people). This paper is a practical exposure to fitting growth curve models in the hierarchical Bayesian framework. First the mathematical formulation of growth curve models is provided. Then we give step-by-step guidelines on how to fit these models in the hierarchical Bayesian framework with corresponding computer scripts (JAGS and R). To illustrate the Bayesian GCM approach, we analyze a data set from a longitudinal study of marital relationship quality. We provide our computer code and example data set so that the reader can have hands-on experience fitting the growth curve model.},
	number = {1},
	journal = {Psychonomic Bulletin \& Review},
	author = {Oravecz, Zita and Muth, Chelsea},
	month = feb,
	year = {2018},
	pages = {235--255},
}

@article{belsky_temperament_1987,
	title = {Temperament and {Attachment} {Security} in the {Strange} {Situation}: {An} {Empirical} {Rapprochement}},
	volume = {58},
	issn = {00093920, 14678624},
	url = {http://www.jstor.org/stable/1130215},
	doi = {10.2307/1130215},
	abstract = {[In response to Frodi and Thompson's recent demonstration that infants classified A1-B2 in the Strange Situation differ significantly in emotional expression from infants classified B3-C2, several longitudinal data sets were examined to determine whether these group differences might be a function of infant temperament. Data from 3 separate samples revealed significant concordance between infant-mother and infant-father Strange Situation classifications when scored in terms of A1-B2 versus B3-C2, but not when scored in terms of the traditional A-B-C system. In addition, in 2 samples on which newborn behavioral data were available. A1-B2 infants displayed more autonomic stability than B3-C2 infants, and in one of the samples the former infants were more alert and positively responsive as newborns (with means in the same direction in Sample 2). Moreover, mothers of A1-B2 infants described their babies as less difficult to care for at 3 months of age. Considered together, these findings suggest that infant temperament affects the manner in which security or insecurity is expressed rather than whether or not the infant develops a secure or insecure attachment. These findings are discussed in terms of their implications for the study of the interactional antecedents and the developmental consequences of attachment security.]},
	number = {3},
	urldate = {2024-06-22},
	journal = {Child Development},
	author = {Belsky, Jay and Rovine, Michael},
	year = {1987},
	note = {Publisher: [Wiley, Society for Research in Child Development]},
	pages = {787--795},
}

@article{belsky_patterns_1990,
	title = {Patterns of {Marital} {Change} across the {Transition} to {Parenthood}: {Pregnancy} to {Three} {Years} {Postpartum}},
	volume = {52},
	issn = {00222445, 17413737},
	url = {http://www.jstor.org/stable/352833},
	doi = {10.2307/352833},
	abstract = {[The purpose of this investigation was to advance the study of marital change across the transition to parenthood by moving beyond the study of central tendencies to examine variation in the manner and extent to which spouses' experiences of their mates and their marital relationships changed from the last trimester of pregnancy through three years postpartum. Analyses of marital data collected at four points in time on 128 middle- and working-class families rearing a firstborn child resulted in the identification of four distinct patterns of marital change, which were labeled accelerating decline, linear decline, no change, and modest positive increase. In a series of developmentally ordered discriminant function analyses, efforts were made to distinguish decliners from increasers by using demographic, personality, and marital information collected prenatally; data on infant temperament and change in infant temperament obtained at three and nine months postpartum, respectively; and data on negative life events and income change collected at three years postpartum. Analyses revealed that patterns of marital change are determined by multiple factors and are largely identifiable prior to the infant's birth. Postnatal information on infant temperament often improved the ability to discriminate marriages that declined and improved in quality across the transition to parenthood.]},
	number = {1},
	urldate = {2024-06-22},
	journal = {Journal of Marriage and Family},
	author = {Belsky, Jay and Rovine, Michael},
	year = {1990},
	note = {Publisher: [Wiley, National Council on Family Relations]},
	pages = {5--19},
}

@article{steele_multilevel_2008,
	title = {Multilevel {Models} for {Longitudinal} {Data}},
	volume = {171},
	issn = {09641998, 1467985X},
	url = {http://www.jstor.org/stable/30130728},
	abstract = {[Repeated measures and repeated events data have a hierarchical structure which can be analysed by using multilevel models. A growth curve model is an example of a multilevel random-coefficients model, whereas a discrete time event history model for recurrent events can be fitted as a multilevel logistic regression model. The paper describes extensions to the basic growth curve model to handle auto-correlated residuals, multiple-indicator latent variables and correlated growth processes, and event history models for correlated event processes. The multilevel approach to the analysis of repeated measures data is contrasted with structural equation modelling. The methods are illustrated in analyses of children's growth, changes in social and political attitudes, and the interrelationship between partnership transitions and childbearing.]},
	number = {1},
	urldate = {2024-06-22},
	journal = {Journal of the Royal Statistical Society. Series A (Statistics in Society)},
	author = {Steele, Fiona},
	year = {2008},
	note = {Publisher: [Wiley, Royal Statistical Society]},
	pages = {5--19},
}

@article{goldstein_multilevel_1994,
	title = {Multilevel time series models with applications to repeated measures data.},
	volume = {13},
	issn = {0277-6715},
	doi = {10.1002/sim.4780131605},
	abstract = {The analysis of repeated measures data can be conducted efficiently using a two-level random coefficients model. A standard assumption is that the  within-individual (level 1) residuals are uncorrelated. In some cases, especially  where measurements are made close together in time, this may not be reasonable  and this additional correlation structure should also be modelled. A time series  model for such data is proposed which consists of a standard multilevel model for  repeated measures data augmented by an autocorrelation model for the level 1  residuals. First- and second-order autoregressive models are considered in  detail, together with a seasonal component. Both discrete and continuous time are  considered and it is shown how the autocorrelation parameters can themselves be  structured in terms of further explanatory variables. The models are fitted to a  data set consisting of repeated height measurements on children.},
	language = {eng},
	number = {16},
	journal = {Statistics in medicine},
	author = {Goldstein, H. and Healy, M. J. and Rasbash, J.},
	month = aug,
	year = {1994},
	pmid = {7973240},
	note = {Place: England},
	keywords = {Humans, Male, Adolescent, Growth, *Models, Statistical, Body Height, Child, Linear Models, Time},
	pages = {1643--1655},
}

@article{shortreed_outcome-adaptive_2017,
	title = {Outcome-adaptive lasso: {Variable} selection for causal inference.},
	volume = {73},
	copyright = {© 2017, The International Biometric Society.},
	issn = {1541-0420 0006-341X},
	doi = {10.1111/biom.12679},
	abstract = {Methodological advancements, including propensity score methods, have resulted in improved unbiased estimation of treatment effects from observational data.  Traditionally, a "throw in the kitchen sink" approach has been used to select  covariates for inclusion into the propensity score, but recent work shows  including unnecessary covariates can impact both the bias and statistical  efficiency of propensity score estimators. In particular, the inclusion of  covariates that impact exposure but not the outcome, can inflate standard errors  without improving bias, while the inclusion of covariates associated with the  outcome but unrelated to exposure can improve precision. We propose the  outcome-adaptive lasso for selecting appropriate covariates for inclusion in  propensity score models to account for confounding bias and maintaining  statistical efficiency. This proposed approach can perform variable selection in  the presence of a large number of spurious covariates, that is, covariates  unrelated to outcome or exposure. We present theoretical and simulation results  indicating that the outcome-adaptive lasso selects the propensity score model  that includes all true confounders and predictors of outcome, while excluding  other covariates. We illustrate covariate selection using the outcome-adaptive  lasso, including comparison to alternative approaches, using simulated data and  in a survey of patients using opioid therapy to manage chronic pain.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Shortreed, Susan M. and Ertefaie, Ashkan},
	month = dec,
	year = {2017},
	pmid = {28273693},
	pmcid = {PMC5591052},
	note = {Place: England},
	keywords = {Humans, Computer Simulation, Model selection, *Models, Statistical, *Outcome Assessment, Health Care, *Propensity Score, Analgesics, Opioid/therapeutic use, Bias, Biometry, Chronic Pain/drug therapy, Comparative effectiveness, Observational studies, Propensity score},
	pages = {1111--1122},
}

@article{pourciau_newton_2001,
	title = {Newton and the {Notion} of {Limit}},
	volume = {28},
	issn = {0315-0860},
	url = {https://www.sciencedirect.com/science/article/pii/S0315086000923012},
	doi = {10.1006/hmat.2000.2301},
	abstract = {We investigate Newton's understanding of the limit concept through a study of certain proofs appearing in the Principia. We find that Newton, not Cauchy, was the first to present an epsilon argument, and that, in general, Newton's understanding of limits was clearer than commonly thought. We observe Newton's distinction between two properties easily confused, namely f/g→1 and f - g→0, we resolve a problem created by a spurious translation appearing in Cajori's revision of Motte's original translation, and we come to a deeper understanding of the well-known but less well understood Lemma XI of Section I, Book I. Copyright 2001 Academic Press. Nous examinons la notion newtonienne du concept de limite en étudiant certaines preuves qui apparaissent dans les Principia. Nous découvrons que Newton, et non pas Cauchy, a été le premier à présenter un argument d'epsilon et que, en général, la compréhension newtonienne de la limite était bien plus lucide qu'on ne le pense communément. Nous observons la distinction que fait Newton entre deux propriétes qui se confondent facilement, à savoir f/g→1 et f - g→0, et nous résolvons un problème né d'une traduction incorrecte parue dans la révision par Cajori de la traduction de Motte. Nous parvenons ainsi à comprendre plus complètement le Lemma XI, Section I, Livre I, bien connu mais moins bien compris.},
	number = {1},
	journal = {Historia Mathematica},
	author = {Pourciau, Bruce},
	month = feb,
	year = {2001},
	pages = {18--30},
}

@article{turner_association_2016,
	title = {Association of levels of opioid use with pain and activity interference among patients initiating chronic opioid therapy: a longitudinal study.},
	volume = {157},
	issn = {1872-6623 0304-3959},
	doi = {10.1097/j.pain.0000000000000452},
	abstract = {Little is known about long-term pain and function outcomes among patients with chronic noncancer pain initiating chronic opioid therapy (COT). In the  Middle-Aged/Seniors Chronic Opioid Therapy study of patients identified through  electronic pharmacy records as initiating COT for chronic noncancer pain, we  examined the relationships between level of opioid use (over the 120 days before  outcome assessment) and pain and activity interference outcomes at 4- and  12-month follow-ups. Patients aged 45+ years (N = 1477) completed a baseline  interview; 1311 and 1157 of these comprised the 4- and 12-month analysis samples,  respectively. Opioid use was classified based on self-report and electronic  pharmacy records for the 120 days before the 4- and 12-month outcome assessments.  Controlling for patient characteristics that predict sustained COT and pain  outcomes, patients who had used opioids minimally or not at all, compared with  those with intermittent/lower-dose and regular/higher-dose opioid use, had better  pain intensity and activity interference outcomes. Adjusted mean (95\% confidence  interval) pain intensity (0-10 scale) at 12 months was 4.91 (4.68-5.13) for the  minimal/no use group and 5.71 (5.50-5.92) and 5.72 (5.51-5.93) for the  intermittent/lower-dose and regular/higher-dose groups, respectively. A similar  pattern was observed for pain intensity at 4 months and for activity interference  at both time points. Better outcomes in the minimal/no use group could reflect  pain improvement leading to opioid discontinuation. The similarity in outcomes of  regular/higher-dose and intermittent/lower-dose opioid users suggests that  intermittent and/or lower-dose use vs higher-dose use may confer risk reduction  without reducing benefits.},
	language = {eng},
	number = {4},
	journal = {Pain},
	author = {Turner, Judith A. and Shortreed, Susan M. and Saunders, Kathleen W. and LeResche, Linda and Von Korff, Michael},
	month = apr,
	year = {2016},
	pmid = {26785321},
	pmcid = {PMC4939796},
	note = {Place: United States},
	keywords = {Aged, Female, Humans, Male, Adult, Middle Aged, Longitudinal Studies, *Pain Measurement, Analgesics, Opioid/*therapeutic use, Chronic Pain/*drug therapy, Opioid-Related Disorders/*drug therapy, Self Report},
	pages = {849--857},
}

@book{velentgas_developing_2013,
	address = {Rockville (MD)},
	title = {Developing a {Protocol} for {Observational} {Comparative} {Effectiveness} {Research}: {A} {User}'s {Guide}},
	copyright = {Copyright © 2013, Agency for Healthcare Research and Quality.},
	abstract = {The Observational CER User's Guide serves as a resource for investigators and stakeholders when designing observational comparative effectiveness research  (CER) studies, particularly those with findings that are intended to translate  into decisions or actions. The User's Guide provides principles for designing  research that will inform health care decisions of patients and other  stakeholders. Furthermore, it serves as a reference for increasing the  transparency of the methods used in a study and standardizing the review of  protocols through checklists provided in every chapter.},
	language = {eng},
	editor = {Velentgas, Priscilla and Dreyer, Nancy A. and Nourjah, Parivash and Smith, Scott R. and Torchia, Marion M.},
	month = jan,
	year = {2013},
	pmid = {23469377},
	note = {Book Title: Developing a Protocol for Observational Comparative Effectiveness Research: A User's Guide},
}

@article{aguilar_intuitive_2023,
	title = {Intuitive joint priors for {Bayesian} linear multilevel models: {The} {R2D2M2} prior},
	volume = {17},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Aguilar, Javier Enrique and Bürkner, Paul-Christian},
	year = {2023},
	note = {ISBN: 1935-7524
Publisher: The Institute of Mathematical Statistics and the Bernoulli Society},
	pages = {1711--1767},
}

@article{alvarez_bayesian_2014,
	title = {{BAYESIAN} {INFERENCE} {FOR} {A} {COVARIANCE} {MATRIX}},
	doi = {10.4148/2475-7772.1004},
	journal = {Conference on Applied Statistics in Agriculture},
	author = {Alvarez, Ignacio and Niemi, Jarad and Simpson, Matt},
	month = apr,
	year = {2014},
}

@article{bartonicek_value_2021,
	title = {The value of {Bayesian} predictive projection for variable selection: an example of selecting lifestyle predictors of young adult well-being},
	volume = {21},
	issn = {1471-2458},
	url = {https://doi.org/10.1186/s12889-021-10690-3},
	doi = {10.1186/s12889-021-10690-3},
	abstract = {Variable selection is an important issue in many fields such as public health and psychology. Researchers often gather data on many variables of interest and then are faced with two challenging goals: building an accurate model with few predictors, and making probabilistic statements (inference) about this model. Unfortunately, it is currently difficult to attain these goals with the two most popular methods for variable selection methods: stepwise selection and LASSO. The aim of the present study was to demonstrate the use predictive projection feature selection – a novel Bayesian variable selection method that delivers both predictive power and inference. We apply predictive projection to a sample of New Zealand young adults, use it to build a compact model for predicting well-being, and compare it to other variable selection methods.},
	number = {1},
	journal = {BMC Public Health},
	author = {Bartonicek, A. and Wickham, S. R. and Pat, N. and Conner, T. S.},
	month = apr,
	year = {2021},
	pages = {695},
}

@article{carvalho_horseshoe_2010,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/asq017},
	doi = {10.1093/biomet/asq017},
	abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator’s advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator’s tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
	number = {2},
	urldate = {2024-06-25},
	journal = {Biometrika},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	month = jun,
	year = {2010},
	pages = {465--480},
}

@book{chan-lau_lasso_2017,
	series = {{IMF} {Working} {Papers}},
	title = {Lasso {Regressions} and {Forecasting} {Models} in {Applied} {Stress} {Testing}},
	isbn = {978-1-4755-9930-5},
	url = {https://books.google.com/books?id=Wv8jDwAAQBAJ},
	publisher = {INTERNATIONAL MONETARY FUND},
	author = {Chan-Lau, J.A.},
	year = {2017},
}

@article{gu_bayesian_2022,
	title = {Bayesian {One}-{Sided} {Variable} {Selection}},
	volume = {57},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2020.1813067},
	doi = {10.1080/00273171.2020.1813067},
	number = {2-3},
	journal = {Multivariate Behavioral Research},
	author = {Gu, Xin and Hoijtink, Herbert and Mulder, Joris},
	month = jun,
	year = {2022},
	note = {Publisher: Routledge},
	pages = {264--278},
	annote = {doi: 10.1080/00273171.2020.1813067},
}

@article{heinze_variable_2018,
	title = {Variable selection – {A} review and recommendations for the practicing statistician},
	volume = {60},
	issn = {0323-3847},
	url = {https://doi.org/10.1002/bimj.201700067},
	doi = {10.1002/bimj.201700067},
	abstract = {Abstract Statistical models support medical research by facilitating individualized outcome prognostication conditional on independent variables or by estimating effects of risk factors adjusted for covariates. Theory of statistical models is well-established if the set of independent variables to consider is fixed and small. Hence, we can assume that effect estimates are unbiased and the usual methods for confidence interval estimation are valid. In routine work, however, it is not known a priori which covariates should be included in a model, and often we are confronted with the number of candidate variables in the range 10?30. This number is often too large to be considered in a statistical model. We provide an overview of various available variable selection methods that are based on significance or information criteria, penalized likelihood, the change-in-estimate criterion, background knowledge, or combinations thereof. These methods were usually developed in the context of a linear regression model and then transferred to more generalized linear models or models for censored survival data. Variable selection, in particular if used in explanatory modeling where effect estimates are of central interest, can compromise stability of a final model, unbiasedness of regression coefficients, and validity of p-values or confidence intervals. Therefore, we give pragmatic recommendations for the practicing statistician on application of variable selection methods in general (low-dimensional) modeling problems and on performing stability investigations and inference. We also propose some quantities based on resampling the entire variable selection process to be routinely reported by software packages offering automated variable selection algorithms.},
	number = {3},
	urldate = {2024-06-24},
	journal = {Biometrical Journal},
	author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
	month = may,
	year = {2018},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {change-in-estimate criterion, penalized likelihood, resampling, statistical model, stepwise selection},
	pages = {431--449},
}

@article{johnson_fitting_1996,
	title = {Fitting {Percentage} of {Body} {Fat} to {Simple} {Body} {Measurements}},
	volume = {4},
	issn = {null},
	url = {https://doi.org/10.1080/10691898.1996.11910505},
	doi = {10.1080/10691898.1996.11910505},
	number = {1},
	journal = {Journal of Statistics Education},
	author = {Johnson, Roger W.},
	month = mar,
	year = {1996},
	note = {Publisher: Taylor \& Francis},
	pages = {null--null},
	annote = {doi: 10.1080/10691898.1996.11910505},
}

@article{kakikawa_bayesian_2023,
	title = {Bayesian fused lasso modeling via horseshoe prior},
	volume = {6},
	issn = {2520-8764},
	url = {https://doi.org/10.1007/s42081-023-00213-2},
	doi = {10.1007/s42081-023-00213-2},
	abstract = {Bayesian fused lasso is one of the sparse Bayesian methods, which shrinks both regression coefficients and their successive differences simultaneously. In this paper, we propose a Bayesian fused lasso modeling via horseshoe prior. By assuming a horseshoe prior on the difference of successive regression coefficients, the proposed method enables us to prevent over-shrinkage of those differences. We also propose a Bayesian nearly hexagonal operator for regression with shrinkage and equality selection with horseshoe prior, which imposes priors on all combinations of differences of regression coefficients. Simulation studies and an application to real data show that the proposed method gives better performance than existing methods.},
	number = {2},
	journal = {Japanese Journal of Statistics and Data Science},
	author = {Kakikawa, Yuko and Shimamura, Kaito and Kawano, Shuichi},
	month = nov,
	year = {2023},
	pages = {705--727},
}

@article{bondell_simultaneous_2008,
	title = {Simultaneous regression shrinkage, variable selection, and supervised clustering of predictors with {OSCAR}.},
	volume = {64},
	issn = {0006-341X 1541-0420},
	doi = {10.1111/j.1541-0420.2007.00843.x},
	abstract = {Variable selection can be challenging, particularly in situations with a large number of predictors with possibly high correlations, such as gene expression  data. In this article, a new method called the OSCAR (octagonal shrinkage and  clustering algorithm for regression) is proposed to simultaneously select  variables while grouping them into predictive clusters. In addition to improving  prediction accuracy and interpretation, these resulting groups can then be  investigated further to discover what contributes to the group having a similar  behavior. The technique is based on penalized least squares with a geometrically  intuitive penalty function that shrinks some coefficients to exactly zero.  Additionally, this penalty yields exact equality of some coefficients,  encouraging correlated predictors that have a similar effect on the response to  form predictive clusters represented by a single coefficient. The proposed  procedure is shown to compare favorably to the existing shrinkage and variable  selection techniques in terms of both prediction error and model complexity,  while yielding the additional grouping information.},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Bondell, Howard D. and Reich, Brian J.},
	month = mar,
	year = {2008},
	pmid = {17608783},
	pmcid = {PMC2605279},
	note = {Place: England},
	keywords = {Biometry/*methods, Computer Simulation, Models, Statistical, *Algorithms, *Artificial Intelligence, *Cluster Analysis, *Data Interpretation, Statistical, *Epidemiologic Methods, *Regression Analysis, Models, Biological},
	pages = {115--123},
}

@article{li_variable_2017,
	title = {Variable selection using shrinkage priors},
	volume = {107},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947316302353},
	doi = {10.1016/j.csda.2016.10.008},
	abstract = {Variable selection has received widespread attention over the last decade as we routinely encounter high-throughput datasets in complex biological and environment research. Most Bayesian variable selection methods are restricted to mixture priors having separate components for characterizing the signal and the noise. However, such priors encounter computational issues in high dimensions. This has motivated continuous shrinkage priors, resembling the two-component priors facilitating computation and interpretability. While such priors are widely used for estimating high-dimensional sparse vectors, selecting a subset of variables remains a daunting task. A general approach for variable selection with shrinkage priors is proposed. The presence of very few tuning parameters makes our method attractive in comparison to ad hoc thresholding approaches. The applicability of the approach is not limited to continuous shrinkage priors, but can be used along with any shrinkage prior. Theoretical properties for near-collinear design matrices are investigated and the method is shown to have good performance in a wide range of synthetic data examples and in a real data example on selecting genes affecting survival due to lymphoma.},
	journal = {Computational Statistics \& Data Analysis},
	author = {Li, Hanning and Pati, Debdeep},
	month = mar,
	year = {2017},
	keywords = {Bayesian, Horseshoe, Markov chain Monte Carlo, Shrinkage priors, Variable selection},
	pages = {107--119},
}

@article{rosenwald_use_2002,
	title = {The use of molecular profiling to predict survival after chemotherapy for diffuse large-{B}-cell lymphoma.},
	volume = {346},
	issn = {1533-4406 0028-4793},
	doi = {10.1056/NEJMoa012914},
	abstract = {BACKGROUND: The survival of patients with diffuse large-B-cell lymphoma after chemotherapy is influenced by molecular features of the tumors. We used the  gene-expression profiles of these lymphomas to develop a molecular predictor of  survival. METHODS: Biopsy samples of diffuse large-B-cell lymphoma from 240  patients were examined for gene expression with the use of DNA microarrays and  analyzed for genomic abnormalities. Subgroups with distinctive gene-expression  profiles were defined on the basis of hierarchical clustering. A molecular  predictor of risk was constructed with the use of genes with expression patterns  that were associated with survival in a preliminary group of 160 patients and was  then tested in a validation group of 80 patients. The accuracy of this predictor  was compared with that of the international prognostic index. RESULTS: Three  gene-expression subgroups--germinal-center B-cell-like, activated B-cell-like,  and type 3 diffuse large-B-cell lymphoma--were identified. Two common oncogenic  events in diffuse large-B-cell lymphoma, bcl-2 translocation and c-rel  amplification, were detected only in the germinal-center B-cell-like subgroup.  Patients in this subgroup had the highest five-year survival rate. To identify  other molecular determinants of outcome, we searched for individual genes with  expression patterns that correlated with survival in the preliminary group of  patients. Most of these genes fell within four gene-expression signatures  characteristic of germinal-center B cells, proliferating cells, reactive stromal  and immune cells in the lymph node, or major-histocompatibility-complex class II  complex. We used 17 genes to construct a predictor of overall survival after  chemotherapy. This gene-based predictor and the international prognostic index  were independent prognostic indicators. CONCLUSIONS: DNA microarrays can be used  to formulate a molecular predictor of survival after chemotherapy for diffuse  large-B-cell lymphoma.},
	language = {eng},
	number = {25},
	journal = {The New England journal of medicine},
	author = {Rosenwald, Andreas and Wright, George and Chan, Wing C. and Connors, Joseph M. and Campo, Elias and Fisher, Richard I. and Gascoyne, Randy D. and Muller-Hermelink, H. Konrad and Smeland, Erlend B. and Giltnane, Jena M. and Hurt, Elaine M. and Zhao, Hong and Averett, Lauren and Yang, Liming and Wilson, Wyndham H. and Jaffe, Elaine S. and Simon, Richard and Klausner, Richard D. and Powell, John and Duffey, Patricia L. and Longo, Dan L. and Greiner, Timothy C. and Weisenburger, Dennis D. and Sanger, Warren G. and Dave, Bhavana J. and Lynch, James C. and Vose, Julie and Armitage, James O. and Montserrat, Emilio and López-Guillermo, Armando and Grogan, Thomas M. and Miller, Thomas P. and LeBlanc, Michel and Ott, German and Kvaloy, Stein and Delabie, Jan and Holte, Harald and Krajci, Peter and Stokke, Trond and Staudt, Louis M.},
	month = jun,
	year = {2002},
	pmid = {12075054},
	note = {Place: United States},
	keywords = {Female, Humans, Male, Middle Aged, Prognosis, Retrospective Studies, *Gene Expression Profiling, Antibiotics, Antineoplastic/administration \& dosage, Antineoplastic Combined Chemotherapy Protocols/therapeutic use, Biopsy, Lymphoma, B-Cell/drug therapy/*genetics/*mortality/pathology, Lymphoma, Large B-Cell, Diffuse/drug therapy/*genetics/*mortality/pathology, Oligonucleotide Array Sequence Analysis, Proportional Hazards Models, Survival Analysis},
	pages = {1937--1947},
}

@article{goldsmith_assessing_2016,
	title = {Assessing systematic effects of stroke on motorcontrol by using hierarchical function-on-scalar regression.},
	volume = {65},
	issn = {0035-9254 1467-9876},
	doi = {10.1111/rssc.12115},
	abstract = {This work is concerned with understanding common population-level effects of stroke on motor control while accounting for possible subject-level idiosyncratic  effects. Upper extremity motor control for each subject is assessed through  repeated planar reaching motions from a central point to eight pre-specified  targets arranged on a circle. We observe the kinematic data for hand position as  a bivariate function of time for each reach. Our goal is to estimate the  bivariate function-on-scalar regression with subject-level random functional  effects while accounting for potential correlation in residual curves; covariates  of interest are severity of motor impairment and target number. We express fixed  effects and random effects using penalized splines, and allow for residual  correlation using a Wishart prior distribution. Parameters are jointly estimated  in a Bayesian framework, and we implement a computationally efficient  approximation algorithm using variational Bayes. Simulations indicate that the  proposed method yields accurate estimation and inference, and application results  suggest that the effect of stroke on motor control has a systematic component  observed across subjects.},
	language = {eng},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series C, Applied statistics},
	author = {Goldsmith, Jeff and Kitago, Tomoko},
	month = feb,
	year = {2016},
	pmid = {27546913},
	pmcid = {PMC4988692},
	note = {Place: England},
	keywords = {Bayesian Regression, Bivariate Data, Gibbs Sampler, Penalized Splines, Variational Bayes},
	pages = {215--236},
}

@article{chen_variable_2016,
	title = {Variable {Selection} in {Function}-on-{Scalar} {Regression}.},
	volume = {5},
	issn = {2049-1573},
	doi = {10.1002/sta4.106},
	abstract = {For regression models with functional responses and scalar predictors, it is common for the number of predictors to be large. Despite this, few methods for  variable selection exist for function-on-scalar models, and none account for the  inherent correlation of residual curves in such models. By expanding the  coefficient functions using a B-spline basis, we pose the function-on-scalar  model as a multivariate regression problem. Spline coefficients are grouped  within coefficient function, and group-minimax concave penalty (MCP) is used for  variable selection. We adapt techniques from generalized least squares to account  for residual covariance by "pre-whitening" using an estimate of the covariance  matrix, and establish theoretical properties for the resulting estimator. We  further develop an iterative algorithm that alternately updates the spline  coefficients and covariance; simulation results indicate that this iterative  algorithm often performs as well as pre-whitening using the true covariance, and  substantially outperforms methods that neglect the covariance structure. We apply  our method to two-dimensional planar reaching motions in a study of the effects  of stroke severity on motor control, and find that our method provides lower  prediction errors than competing methods.},
	language = {eng},
	number = {1},
	journal = {Stat (International Statistical Institute)},
	author = {Chen, Yakuan and Goldsmith, Jeff and Ogden, Todd},
	year = {2016},
	pmid = {27429751},
	pmcid = {PMC4943585},
	note = {Place: United States},
	keywords = {Group MCP, Kinematic Data, Pre-whitening, Splines},
	pages = {88--101},
}

@article{wang_functional_2016,
	title = {Functional data analysis},
	volume = {3},
	number = {1},
	journal = {Annual Review of Statistics and its application},
	author = {Wang, Jane-Ling and Chiou, Jeng-Min and Müller, Hans-Georg},
	year = {2016},
	note = {ISBN: 2326-8298
Publisher: Annual Reviews},
	pages = {257--295},
}

@article{malsiner-walli_comparing_2016,
	title = {Comparing {Spike} and {Slab} {Priors} for {Bayesian} {Variable} {Selection}},
	volume = {40},
	url = {https://www.ajs.or.at/index.php/ajs/article/view/vol40%2C%20no4%20-%202},
	doi = {10.17713/ajs.v40i4.215},
	abstract = {An important task in building regression models is to decide which regressors should be included in the final model. In a Bayesian approach, variable selection can be performed using mixture priors with a spike and a slab component for the effects subject to selection. As the spike is concentrated at zero, variable selection is based on the probability of assigning the corresponding regression effect to the slab component. These posterior inclusion probabilities can be determined by MCMC sampling. In this paper we compare the MCMC implementations for several spike and slab priors with regard to posterior inclusion probabilities and their sampling efficiency for simulated data. Further, we investigate posterior inclusion probabilities analytically for different slabs in two simple settings. Application of variable selection with spike and slab priors is illustrated on a data set of psychiatric patients where the goal is to identify covariates affecting metabolism.},
	number = {4},
	urldate = {2024-07-07},
	journal = {Austrian Journal of Statistics},
	author = {Malsiner-Walli, Gertraud and Wagner, Helga},
	month = feb,
	year = {2016},
	note = {Section: Articles},
	pages = {241--264},
}

@article{mitchell_bayesian_1988,
	title = {Bayesian {Variable} {Selection} in {Linear} {Regression}},
	volume = {83},
	issn = {01621459, 1537274X},
	url = {http://www.jstor.org/stable/2290129},
	doi = {10.2307/2290129},
	abstract = {[This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose γ. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.]},
	number = {404},
	urldate = {2024-07-07},
	journal = {Journal of the American Statistical Association},
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	year = {1988},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1023--1032},
}

@article{r_b_ohara_review_2009,
	title = {A review of {Bayesian} variable selection methods: what, how and which},
	volume = {4},
	url = {https://doi.org/10.1214/09-BA403},
	doi = {10.1214/09-BA403},
	number = {1},
	journal = {Bayesian Analysis},
	author = {{R. B. O'Hara} and {M. J. Sillanpää}},
	month = mar,
	year = {2009},
	pages = {85--117},
}

@article{piironen_projection_2015,
	title = {Projection predictive variable selection using {Stan}+ {R}},
	journal = {arXiv preprint arXiv:1508.02502},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2015},
}

@article{juho_piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	url = {https://doi.org/10.1214/17-EJS1337SI},
	doi = {10.1214/17-EJS1337SI},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {{Juho Piironen} and {Aki Vehtari}},
	month = jan,
	year = {2017},
	pages = {5018--5051},
}

@article{rubio_bayesian_2022,
	title = {Bayesian variable selection and survival modeling: assessing the {Most} important comorbidities that impact lung and colorectal cancer survival in {Spain}},
	volume = {22},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-022-01582-0},
	doi = {10.1186/s12874-022-01582-0},
	abstract = {Cancer survival represents one of the main indicators of interest in cancer epidemiology. However, the survival of cancer patients can be affected by several factors, such as comorbidities, that may interact with the cancer biology. Moreover, it is interesting to understand how different cancer sites and tumour stages are affected by different comorbidities. Identifying the comorbidities that affect cancer survival is thus of interest as it can be used to identify factors driving the survival of cancer patients. This information can also be used to identify vulnerable groups of patients with comorbidities that may lead to worst prognosis of cancer. We address these questions and propose a principled selection and evaluation of the effect of comorbidities on the overall survival of cancer patients. In the first step, we apply a Bayesian variable selection method that can be used to identify the comorbidities that predict overall survival. In the second step, we build a general Bayesian survival model that accounts for time-varying effects. In the third step, we derive several posterior predictive measures to quantify the effect of individual comorbidities on the population overall survival. We present applications to data on lung and colorectal cancers from two Spanish population-based cancer registries. The proposed methodology is implemented with a combination of the R-packages mombf and rstan. We provide the code for reproducibility at https://github.com/migariane/BayesVarImpComorbiCancer.},
	number = {1},
	journal = {BMC Medical Research Methodology},
	author = {Rubio, Francisco Javier and Alvares, Danilo and Redondo-Sanchez, Daniel and Marcos-Gragera, Rafael and Sánchez, María-José and Luque-Fernandez, Miguel Angel},
	month = apr,
	year = {2022},
	pages = {95},
}

@article{van_erp_shrinkage_2019,
	title = {Shrinkage priors for {Bayesian} penalized regression},
	volume = {89},
	issn = {0022-2496},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249618300567},
	doi = {10.1016/j.jmp.2018.12.004},
	abstract = {In linear regression problems with many predictors, penalized regression techniques are often used to guard against overfitting and to select variables relevant for predicting an outcome variable. Recently, Bayesian penalization is becoming increasingly popular in which the prior distribution performs a function similar to that of the penalty term in classical penalization. Specifically, the so-called shrinkage priors in Bayesian penalization aim to shrink small effects to zero while maintaining true large effects. Compared to classical penalization techniques, Bayesian penalization techniques perform similarly or sometimes even better, and they offer additional advantages such as readily available uncertainty estimates, automatic estimation of the penalty parameter, and more flexibility in terms of penalties that can be considered. However, many different shrinkage priors exist and the available, often quite technical, literature primarily focuses on presenting one shrinkage prior and often provides comparisons with only one or two other shrinkage priors. This can make it difficult for researchers to navigate through the many prior options and choose a shrinkage prior for the problem at hand. Therefore, the aim of this paper is to provide a comprehensive overview of the literature on Bayesian penalization. We provide a theoretical and conceptual comparison of nine different shrinkage priors and parametrize the priors, if possible, in terms of scale mixture of normal distributions to facilitate comparisons. We illustrate different characteristics and behaviors of the shrinkage priors and compare their performance in terms of prediction and variable selection in a simulation study. Additionally, we provide two empirical examples to illustrate the application of Bayesian penalization. Finally, an R package bayesreg is available online (https://github.com/sara-vanerp/bayesreg) which allows researchers to perform Bayesian penalized regression with novel shrinkage priors in an easy manner.},
	journal = {Journal of Mathematical Psychology},
	author = {van Erp, Sara and Oberski, Daniel L. and Mulder, Joris},
	month = apr,
	year = {2019},
	keywords = {Bayesian, Shrinkage priors, Empirical Bayes, Penalization, Regression},
	pages = {31--50},
}

@book{cheney_numerical_2013,
	address = {Boston, MA},
	edition = {7th},
	title = {Numerical {Mathematics} and {Computing}},
	publisher = {Brooks/Cole},
	author = {Cheney, Ward and Kincaid, David},
	year = {2013},
}

@article{perperoglou_review_2019,
	title = {A review of spline function procedures in {R}},
	volume = {19},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/s12874-019-0666-3},
	doi = {10.1186/s12874-019-0666-3},
	abstract = {With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the STRengthening Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R.},
	number = {1},
	journal = {BMC Medical Research Methodology},
	author = {Perperoglou, Aris and Sauerbrei, Willi and Abrahamowicz, Michal and Schmid, Matthias},
	month = mar,
	year = {2019},
	pages = {46},
}

@book{gelman_bayesian_2013,
	series = {Chapman \& {Hall}/{CRC} {Texts} in {Statistical} {Science}},
	title = {Bayesian {Data} {Analysis}, {Third} {Edition}},
	isbn = {978-1-4398-4095-5},
	url = {https://books.google.com/books?id=ZXL6AQAAQBAJ},
	publisher = {Taylor \& Francis},
	author = {Gelman, A. and Carlin, J.B. and Stern, H.S. and Dunson, D.B. and Vehtari, A. and Rubin, D.B.},
	year = {2013},
	lccn = {2013039507},
}

@inproceedings{casciola_spline_1996,
	title = {Spline {Curves} in {Polar} and {Cartesian} {Coordinates}},
	url = {https://api.semanticscholar.org/CorpusID:18269611},
	author = {Casciola, Giulio and Morigi, Serena and E, A. Le M. Ehaut and Rabut, Christophe and Schumaker, Larry L.},
	year = {1996},
}

@article{brumback_smoothing_1998,
	title = {Smoothing {Spline} {Models} for the {Analysis} of {Nested} and {Crossed} {Samples} of {Curves}},
	volume = {93},
	issn = {01621459, 1537274X},
	url = {http://www.jstor.org/stable/2669837},
	doi = {10.2307/2669837},
	abstract = {[We introduce a class of models for an additive decomposition of groups of curves stratified by crossed and nested factors, generalizing smoothing splines to such samples by associating them with a corresponding mixed-effects model. The models are also useful for imputation of missing data and exploratory analysis of variance. We prove that the best linear unbiased predictors (BLUPs) from the extended mixed-effects model correspond to solutions of a generalized penalized regression where smoothing parameters are directly related to variance components, and we show that these solutions are natural cubic splines. The model parameters are estimated using a highly efficient implementation of the EM algorithm for restricted maximum likelihood (REML) estimation based on a preliminary eigenvector decomposition. Variability of computed estimates can be assessed with asymptotic techniques or with a novel hierarchical bootstrap resampling scheme for nested mixed-effects models. Our methods are applied to menstrual cycle data from studies of reproductive function that measure daily urinary progesterone; the sample of progesterone curves is stratified by cycles nested within subjects nested within conceptive and nonconceptive groups.]},
	number = {443},
	urldate = {2024-10-22},
	journal = {Journal of the American Statistical Association},
	author = {Brumback, Babette A. and Rice, John A.},
	year = {1998},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {961--976},
}

@article{paul_h_c_eilers_flexible_1996,
	title = {Flexible smoothing with {B}-splines and penalties},
	volume = {11},
	url = {https://doi.org/10.1214/ss/1038425655},
	doi = {10.1214/ss/1038425655},
	number = {2},
	journal = {Statistical Science},
	author = {{Paul H. C. Eilers} and {Brian D. Marx}},
	month = may,
	year = {1996},
	pages = {89--121},
}

@article{betancourt_conceptual_2017,
	title = {A conceptual introduction to {Hamiltonian} {Monte} {Carlo}},
	journal = {arXiv preprint arXiv:1701.02434},
	author = {Betancourt, Michael},
	year = {2017},
}

@article{rice_estimating_1991,
	title = {Estimating the {Mean} and {Covariance} {Structure} {Nonparametrically} {When} the {Data} are {Curves}},
	volume = {53},
	issn = {0035-9246},
	url = {https://doi.org/10.1111/j.2517-6161.1991.tb01821.x},
	doi = {10.1111/j.2517-6161.1991.tb01821.x},
	abstract = {We develop methods for the analysis of a collection of curves which are stochastically modelled as independent realizations of a random function with an unknown mean and covariance structure. We propose a method of estimating the mean function non-parametrically under the assumption that it is smooth. We suggest a variant on the usual form of cross-validation for choosing the degree of smoothing to be employed. This method of cross-validation, which consists of deleting entire sample curves, has the advantage that it does not require that the covariance structure be known or estimated. In the estimation of the covariance structure, we are primarily concerned with models in which the first few eigenfunctions are smooth and the eigenvalues decay rapidly, so that the variability is predominantly of large scale. We propose smooth nonparametric estimates of the eigenfunctions and a suitable method of cross-validation to determine the amount of smoothing. Our methods are applied to data on the gaits of a group of 5-year-old children.},
	number = {1},
	urldate = {2024-10-27},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Rice, John A. and Silverman, B. W.},
	month = sep,
	year = {1991},
	pages = {233--243},
}

@article{ramsay_when_1982,
	title = {When the data are functions},
	volume = {47},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02293704},
	doi = {10.1007/BF02293704},
	abstract = {A datum is often a continuous functionx(t) of a variable such as time observed over some interval. One or more such functions are observed for each subject or unit of observation. The extension of classical data analytic techniques designed forp-variate observations to such data is discussed. The essential step is the expression of the classical problem in the language of functional analysis, after which the extension to functions is a straightforward matter. A schematic device called the duality diagram is a very useful tool for describing an analysis and for suggesting new possibilities. Least squares approximation, descriptive statistics, principal components analysis, and canonical correlation analysis are discussed within this broader framework.},
	number = {4},
	journal = {Psychometrika},
	author = {Ramsay, J. O.},
	month = dec,
	year = {1982},
	pages = {379--396},
}

@article{ulf_grenander_stochastic_1950,
	title = {Stochastic processes and statistical inference},
	volume = {1},
	url = {https://doi.org/10.1007/BF02590638},
	doi = {10.1007/BF02590638},
	number = {3},
	journal = {Arkiv för Matematik},
	author = {{Ulf Grenander}},
	month = oct,
	year = {1950},
	pages = {195--277},
}

@article{goldsmith_corrected_2013,
	title = {Corrected confidence bands for functional data using principal components.},
	volume = {69},
	copyright = {Copyright © 2013, The International Biometric Society.},
	issn = {1541-0420 0006-341X},
	doi = {10.1111/j.1541-0420.2012.01808.x},
	abstract = {Functional principal components (FPC) analysis is widely used to decompose and express functional observations. Curve estimates implicitly condition on basis  functions and other quantities derived from FPC decompositions; however these  objects are unknown in practice. In this article, we propose a method for  obtaining correct curve estimates by accounting for uncertainty in FPC  decompositions. Additionally, pointwise and simultaneous confidence intervals  that account for both model- and decomposition-based variability are constructed.  Standard mixed model representations of functional expansions are used to  construct curve estimates and variances conditional on a specific decomposition.  Iterated expectation and variance formulas combine model-based conditional  estimates across the distribution of decompositions. A bootstrap procedure is  implemented to understand the uncertainty in principal component decomposition  quantities. Our method compares favorably to competing approaches in simulation  studies that include both densely and sparsely observed functions. We apply our  method to sparse observations of CD4 cell counts and to dense white-matter tract  profiles. Code for the analyses and simulations is publicly available, and our  method is implemented in the R package refund on CRAN.},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Goldsmith, J. and Greven, S. and Crainiceanu, C.},
	month = mar,
	year = {2013},
	pmid = {23003003},
	pmcid = {PMC3962763},
	note = {Place: England},
	keywords = {Humans, Computer Simulation, *Models, Statistical, *Confidence Intervals, Brain/pathology, CD4 Lymphocyte Count, HIV Infections/diagnosis, HIV/growth \& development, Magnetic Resonance Imaging, Multiple Sclerosis/pathology, Principal Component Analysis/*methods},
	pages = {41--51},
}

@article{goldsmith_generalized_2015,
	title = {Generalized multilevel function-on-scalar regression and principal component analysis.},
	volume = {71},
	copyright = {© 2015, The International Biometric Society.},
	issn = {1541-0420 0006-341X},
	doi = {10.1111/biom.12278},
	abstract = {This manuscript considers regression models for generalized, multilevel functional responses: functions are generalized in that they follow an  exponential family distribution and multilevel in that they are clustered within  groups or subjects. This data structure is increasingly common across scientific  domains and is exemplified by our motivating example, in which binary curves  indicating physical activity or inactivity are observed for nearly 600 subjects  over 5 days. We use a generalized linear model to incorporate scalar covariates  into the mean structure, and decompose subject-specific and subject-day-specific  deviations using multilevel functional principal components analysis. Thus,  functional fixed effects are estimated while accounting for within-function and  within-subject correlations, and major directions of variability within and  between subjects are identified. Fixed effect coefficient functions and principal  component basis functions are estimated using penalized splines; model parameters  are estimated in a Bayesian framework using Stan, a programming language that  implements a Hamiltonian Monte Carlo sampler. Simulations designed to mimic the  application have good estimation and inferential properties with reasonable  computation times for moderate datasets, in both cross-sectional and multilevel  scenarios; code is publicly available. In the application we identify effects of  age and BMI on the time-specific change in probability of being active over a  24-hour period; in addition, the principal components analysis identifies the  patterns of activity that distinguish subjects and days within subjects.},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Goldsmith, Jeff and Zipunnikov, Vadim and Schrack, Jennifer},
	month = jun,
	year = {2015},
	pmid = {25620473},
	pmcid = {PMC4479975},
	note = {Place: England},
	keywords = {Aged, Aged, 80 and over, Female, Humans, Male, Middle Aged, Computer Simulation, Models, Statistical, Linear Models, Biometry, *Regression Analysis, *Principal Component Analysis, Accelerometry, Accelerometry/statistics \& numerical data, Aging/physiology, Bayes Theorem, Bayesian inference, Generalized functional data, Hamiltonian Monte Carlo, Monte Carlo Method, Motor Activity, Penalized splines},
	pages = {344--353},
}

@article{guo_functional_2002,
	title = {Functional {Mixed} {Effects} {Models}},
	volume = {58},
	issn = {0006-341X},
	url = {https://doi.org/10.1111/j.0006-341X.2002.00121.x},
	doi = {10.1111/j.0006-341X.2002.00121.x},
	abstract = {Summary. In this article, a new class of functional models in which smoothing splines are used to model fixed effects as well as random effects is introduced. The linear mixed effects models are extended to non-parametric mixed effects models by introducing functional random effects, which are modeled as realizations of zero-mean stochastic processes. The fixed functional effects and the random functional effects are modeled in the same functional space, which guarantee the population-average and subject-specific curves have the same smoothness property. These models inherit the flexibility of the linear mixed effects models in handling complex designs and correlation structures, can include continuous covariates as well as dummy factors in both the fixed or random design matrices, and include the nested curves models as special cases. Two estimation procedures are proposed. The first estimation procedure exploits the connection between linear mixed effects models and smoothing splines and can be fitted using existing software. The second procedure is a sequential estimation procedure using Kalman filtering. This algorithm avoids inversion of large dimensional matrices and therefore can be applied to large data sets. A generalized maximum likelihood (GML) ratio test is proposed for inference and model selection. An application to comparison of cortisol profiles is used as an illustration.},
	number = {1},
	urldate = {2024-10-28},
	journal = {Biometrics},
	author = {Guo, Wensheng},
	month = mar,
	year = {2002},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {Functional models, Kalman filter, Mixed effects models, Sequential estimation, Smoothing spline, State space models},
	pages = {121--128},
}

@book{ramsay_functional_2005,
	series = {Springer {Series} in {Statistics}},
	title = {Functional {Data} {Analysis}},
	isbn = {978-0-387-40080-8},
	url = {https://books.google.com/books?id=mU3dop5wY_4C},
	publisher = {Springer},
	author = {Ramsay, J. and Silverman, B.W.},
	year = {2005},
	lccn = {2005923773},
}

@article{plackett_studies_1972,
	title = {Studies in the {History} of {Probability} and {Statistics}. {XXIX}: {The} {Discovery} of the {Method} of {Least} {Squares}},
	volume = {59},
	issn = {00063444, 14643510},
	url = {http://www.jstor.org/stable/2334569},
	doi = {10.2307/2334569},
	abstract = {[The circumstances in which the discovery of the method of least squares took place and the course of the ensuing controversy are examined in detail with the aid of correspondence. Some conclusions are drawn about the attitudes of the main participants and the nature of historical research in statistics.]},
	number = {2},
	urldate = {2024-10-29},
	journal = {Biometrika},
	author = {Plackett, R. L.},
	year = {1972},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {239--251},
}

@article{stephen_m_stigler_gauss_1981,
	title = {Gauss and the {Invention} of {Least} {Squares}},
	volume = {9},
	url = {https://doi.org/10.1214/aos/1176345451},
	doi = {10.1214/aos/1176345451},
	number = {3},
	journal = {The Annals of Statistics},
	author = {{Stephen M. Stigler}},
	month = may,
	year = {1981},
	pages = {465--474},
}

@article{zabell_commentary_2012,
	title = {Commentary on {Alan} {M}. {Turing}: {The} {Applications} of {Probability} to {Cryptography}},
	volume = {36},
	issn = {0161-1194},
	url = {https://doi.org/10.1080/01611194.2012.697811},
	doi = {10.1080/01611194.2012.697811},
	number = {3},
	journal = {Cryptologia},
	author = {Zabell, Sandy},
	month = jul,
	year = {2012},
	note = {Publisher: Taylor \& Francis},
	pages = {191--214},
	annote = {doi: 10.1080/01611194.2012.697811},
}

@inproceedings{taylor_alan_2015,
	title = {Alan {M}. {Turing}: {The} {Applications} of {Probability} to {Cryptography}},
	url = {https://api.semanticscholar.org/CorpusID:116894950},
	author = {Taylor, Ian},
	year = {2015},
}

@article{harrison_introduction_2010,
	title = {Introduction {To} {Monte} {Carlo} {Simulation}.},
	volume = {1204},
	issn = {0094-243X},
	doi = {10.1063/1.3295638},
	abstract = {This paper reviews the history and principles of Monte Carlo simulation, emphasizing techniques commonly used in the simulation of medical imaging.},
	language = {eng},
	journal = {AIP conference proceedings},
	author = {Harrison, Robert L.},
	month = jan,
	year = {2010},
	pmid = {20733932},
	pmcid = {PMC2924739},
	note = {Place: United States},
	pages = {17--21},
}

@article{kleffe_principal_1973,
	title = {Principal components of random variables with values in a seperable hilbert space},
	volume = {4},
	issn = {0047-6277},
	url = {https://doi.org/10.1080/02331887308801137},
	doi = {10.1080/02331887308801137},
	number = {5},
	journal = {Mathematische Operationsforschung und Statistik},
	author = {Kleffe, Jürgen},
	month = jan,
	year = {1973},
	note = {Publisher: Taylor \& Francis},
	pages = {391--406},
	annote = {doi: 10.1080/02331887308801137},
}

@book{wahba_spline_1990,
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Spline {Models} for {Observational} {Data}},
	isbn = {978-0-89871-244-5},
	url = {https://books.google.com/books?id=ScRQJEETs0EC},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Wahba, G.},
	year = {1990},
	lccn = {89028687},
}

@article{dung_direct_2017,
	title = {A direct method to solve optimal knots of {B}-spline curves: {An} application for non-uniform {B}-spline curves fitting.},
	volume = {12},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0173857},
	abstract = {B-spline functions are widely used in many industrial applications such as computer graphic representations, computer aided design, computer aided  manufacturing, computer numerical control, etc. Recently, there exist some  demands, e.g. in reverse engineering (RE) area, to employ B-spline curves for  non-trivial cases that include curves with discontinuous points, cusps or turning  points from the sampled data. The most challenging task in these cases is in the  identification of the number of knots and their respective locations in  non-uniform space in the most efficient computational cost. This paper presents a  new strategy for fitting any forms of curve by B-spline functions via local  algorithm. A new two-step method for fast knot calculation is proposed. In the  first step, the data is split using a bisecting method with predetermined  allowable error to obtain coarse knots. Secondly, the knots are optimized, for  both locations and continuity levels, by employing a non-linear least squares  technique. The B-spline function is, therefore, obtained by solving the ordinary  least squares problem. The performance of the proposed method is validated by  using various numerical experimental data, with and without simulated noise,  which were generated by a B-spline function and deterministic parametric  functions. This paper also discusses the benchmarking of the proposed method to  the existing methods in literature. The proposed method is shown to be able to  reconstruct B-spline functions from sampled data within acceptable tolerance. It  is also shown that, the proposed method can be applied for fitting any types of  curves ranging from smooth ones to discontinuous ones. In addition, the method  does not require excessive computational cost, which allows it to be used in  automatic reverse engineering applications.},
	language = {eng},
	number = {3},
	journal = {PloS one},
	author = {Dung, Van Than and Tjahjowidodo, Tegoeh},
	year = {2017},
	pmid = {28319131},
	pmcid = {PMC5358887},
	note = {Place: United States},
	keywords = {Computer Simulation, *Algorithms, Computer Graphics, Statistics as Topic/*methods},
	pages = {e0173857},
}

@article{grove_ct_2010,
	title = {From {CT} to {NURBS}: {Contour} {Fitting} with {B}-spline {Curves}},
	volume = {7},
	issn = {null},
	url = {https://doi.org/10.1080/16864360.2010.10738807},
	doi = {10.1080/16864360.2010.10738807},
	number = {sup1},
	journal = {Computer-Aided Design and Applications},
	author = {Grove, Olya and Rajab, Khairan and Piegl, Les A.},
	month = jan,
	year = {2010},
	note = {Publisher: Taylor \& Francis},
	pages = {1--19},
	annote = {doi: 10.1080/16864360.2010.10738807},
}

@article{ichida_curve_1977,
	title = {Curve {Fitting} by a {One}-{Pass} {Method} {With} a {Piecewise} {Cubic} {Polynomial}},
	volume = {3},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/355732.355737},
	doi = {10.1145/355732.355737},
	number = {2},
	journal = {ACM Trans. Math. Softw.},
	author = {Ichida, Kozo and Kiyono, Takeshi and Yoshimoto, Fujiichi},
	month = jun,
	year = {1977},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {164--174},
}

@article{xuming_he_data-adaptive_2001,
	title = {A data-adaptive knot selection scheme for fitting splines},
	volume = {8},
	issn = {1558-2361},
	doi = {10.1109/97.917695},
	number = {5},
	journal = {IEEE Signal Processing Letters},
	author = {{Xuming He} and {Lixin Shen} and {Zuowei Shen}},
	month = may,
	year = {2001},
	pages = {137--139},
}

@article{lyche_knot_1987,
	title = {Knot removal for parametric {B}-spline curves and surfaces},
	volume = {4},
	issn = {0167-8396},
	url = {https://www.sciencedirect.com/science/article/pii/0167839687900136},
	doi = {10.1016/0167-8396(87)90013-6},
	abstract = {This paper is the third in a sequence of papers in which a knot removal strategy for splines, based on certain discrete norms, is developed. In the first paper, approximation methods defined as best approximations in these norms were discussed, while in the second paper a knot removal strategy for spline functions was developed. In this paper the knot removal strategy is extended to parametric spline curves and tensor product surfaces. The method has been implemented and thoroughly tested on a computer. We illustrate with several examples and applications.},
	number = {3},
	journal = {Computer Aided Geometric Design},
	author = {Lyche, Tom and Mørken, Knut},
	month = nov,
	year = {1987},
	keywords = {Splines, B-splines, curves, discrete norms, knot removal, tensor product surfaces},
	pages = {217--230},
}

@article{locantore_robust_1999,
	title = {Robust principal component analysis for functional data},
	volume = {8},
	issn = {1863-8260},
	url = {https://doi.org/10.1007/BF02595862},
	doi = {10.1007/BF02595862},
	abstract = {A method for exploring the structure of populations of complex objects, such as images, is considered. The objects are summarized by feature vectors. The statistical backbone is Principal Component Analysis in the space of feature vectors. Visual insights come from representing the results in the original data space. In an ophthalmological example, endemic outliers motivate the development of a bounded influence approach to PCA.},
	number = {1},
	journal = {Test},
	author = {Locantore, N. and Marron, J. S. and Simpson, D. G. and Tripoli, N. and Zhang, J. T. and Cohen, K. L. and Boente, Graciela and Fraiman, Ricardo and Brumback, Babette and Croux, Christophe and Fan, Jianqing and Kneip, Alois and Marden, John I. and Peña, Daniel and Prieto, Javier and Ramsay, Jim O. and Valderrama, Mariano J. and Aguilera, Ana M. and Locantore, N. and Marron, J. S. and Simpson, D. G. and Tripoli, N. and Zhang, J. T. and Cohen, K. L.},
	month = jun,
	year = {1999},
	pages = {1--73},
}

@article{liggett_look_2003,
	title = {A {Look} at {Mass} {Spectral} {Measurement}},
	volume = {16},
	issn = {0933-2480},
	url = {https://doi.org/10.1080/09332480.2003.10554871},
	doi = {10.1080/09332480.2003.10554871},
	number = {4},
	journal = {CHANCE},
	author = {Liggett, Walter and Cazares, Lisa and Semmes, O. John},
	month = sep,
	year = {2003},
	note = {Publisher: ASA Website},
	pages = {24--28},
	annote = {doi: 10.1080/09332480.2003.10554871},
}

@article{kneip_inference_2001,
	title = {Inference for {Density} {Families} {Using} {Functional} {Principal} {Component} {Analysis}},
	volume = {96},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214501753168235},
	doi = {10.1198/016214501753168235},
	number = {454},
	journal = {Journal of the American Statistical Association},
	author = {Kneip, Alois and Utikal, Klaus J},
	month = jun,
	year = {2001},
	note = {Publisher: ASA Website},
	pages = {519--542},
	annote = {doi: 10.1198/016214501753168235},
}

@article{koner_second-generation_2023,
	title = {Second-generation functional data},
	volume = {10},
	issn = {2326-8298},
	number = {1},
	journal = {Annual review of statistics and its application},
	author = {Koner, Salil and Staicu, Ana-Maria},
	year = {2023},
	note = {Publisher: Annual Reviews},
	pages = {547--572},
}

@article{morris_functional_2015,
	title = {Functional regression},
	volume = {2},
	issn = {2326-8298},
	number = {1},
	journal = {Annual Review of Statistics and Its Application},
	author = {Morris, Jeffrey S},
	year = {2015},
	note = {Publisher: Annual Reviews},
	pages = {321--359},
}

@article{murray_bayes_0000,
	title = {Bayes' rule},
	url = {https://jaredsmurray.github.io/sta371h/files/05_bayes_rule%20(2).pdf},
	author = {Murray, J. S.},
	year = {0000},
}

@article{lawrence_d_stone_search_2014,
	title = {Search for the {Wreckage} of {Air} {France} {Flight} {AF} 447},
	volume = {29},
	url = {https://doi.org/10.1214/13-STS420},
	doi = {10.1214/13-STS420},
	number = {1},
	journal = {Statistical Science},
	author = {{Lawrence D. Stone} and {Colleen M. Keller} and {Thomas M. Kratzke} and {Johan P. Strumpfer}},
	month = feb,
	year = {2014},
	pages = {69--80},
}

@article{faraway_regression_1997,
	title = {Regression {Analysis} for a {Functional} {Response}},
	volume = {39},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1997.10485118},
	doi = {10.1080/00401706.1997.10485118},
	number = {3},
	journal = {Technometrics},
	author = {Faraway, Julian J.},
	month = aug,
	year = {1997},
	note = {Publisher: ASA Website},
	pages = {254--261},
	annote = {doi: 10.1080/00401706.1997.10485118},
}

@article{james_principal_2000,
	title = {Principal component models for sparse functional data},
	volume = {87},
	issn = {0006-3444},
	url = {https://doi.org/10.1093/biomet/87.3.587},
	doi = {10.1093/biomet/87.3.587},
	abstract = {The elements of a multivariate dataset are often curves rather than single points. Functional principal components can be used to describe the modes of variation of such curves. If one has complete measurements for each individual curve or, as is more common, one has measurements on a fine grid taken at the same time points for all curves, then many standard techniques may be applied. However, curves are often measured at an irregular and sparse set of time points which can differ widely across individuals. We present a technique for handling this more difficult case using a reduced rank mixed effects framework.},
	number = {3},
	urldate = {2024-11-19},
	journal = {Biometrika},
	author = {James, GM and Hastie, TJ and Sugar, CA},
	month = sep,
	year = {2000},
	pages = {587--602},
}

@article{reiss_fast_2010,
	title = {Fast function-on-scalar regression with penalized basis expansions.},
	volume = {6},
	issn = {1557-4679},
	doi = {10.2202/1557-4679.1246},
	abstract = {Regression models for functional responses and scalar predictors are often fitted by means of basis functions, with quadratic roughness penalties applied to avoid  overfitting. The fitting approach described by Ramsay and Silverman in the 1990 s  amounts to a penalized ordinary least squares (P-OLS) estimator of the  coefficient functions. We recast this estimator as a generalized ridge regression  estimator, and present a penalized generalized least squares (P-GLS) alternative.  We describe algorithms by which both estimators can be implemented, with  automatic selection of optimal smoothing parameters, in a more computationally  efficient manner than has heretofore been available. We discuss pointwise  confidence intervals for the coefficient functions, simultaneous inference by  permutation tests, and model selection, including a novel notion of pointwise  model selection. P-OLS and P-GLS are compared in a simulation study. Our methods  are illustrated with an analysis of age effects in a functional magnetic  resonance imaging data set, as well as a reanalysis of a now-classic Canadian  weather data set. An R package implementing the methods is publicly available.},
	language = {eng},
	number = {1},
	journal = {The international journal of biostatistics},
	author = {Reiss, Philip T. and Huang, Lei and Mennes, Maarten},
	year = {2010},
	pmid = {21969982},
	note = {Place: Germany},
	keywords = {Female, Humans, Male, Adult, Middle Aged, Models, Statistical, *Algorithms, *Regression Analysis, *Least-Squares Analysis, Age Factors, Canada, Data Interpretation, Statistical, Magnetic Resonance Imaging/statistics \& numerical data, Weather},
	pages = {Article 28},
}

@article{kirschenmann_concepts_1972,
	title = {Concepts of {Randomness}},
	volume = {1},
	issn = {00223611, 15730433},
	url = {http://www.jstor.org/stable/30226051},
	number = {3/4},
	urldate = {2024-11-20},
	journal = {Journal of Philosophical Logic},
	author = {Kirschenmann, Peter},
	year = {1972},
	note = {Publisher: Springer},
	pages = {395--414},
}

@article{kendall_theory_1941,
	title = {A {Theory} of {Randomness}},
	volume = {32},
	issn = {00063444, 14643510},
	url = {http://www.jstor.org/stable/2332245},
	doi = {10.2307/2332245},
	number = {1},
	urldate = {2024-11-20},
	journal = {Biometrika},
	author = {Kendall, M. G.},
	year = {1941},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {1--15},
}

@article{tsiligkaridis_covariance_2013,
	title = {Covariance {Estimation} in {High} {Dimensions} {Via} {Kronecker} {Product} {Expansions}},
	volume = {61},
	doi = {10.1109/TSP.2013.2279355},
	number = {21},
	journal = {IEEE Transactions on Signal Processing},
	author = {Tsiligkaridis, Theodoros and Hero, Alfred O.},
	year = {2013},
	keywords = {Brain modeling, Convergence, Covariance matrices, Estimation, high dimensional convergence rates, Kronecker product decompositions, Least squares approximations, mean-square error, multivariate prediction, penalized least squares, Standards, Structured covariance estimation, Symmetric matrices},
	pages = {5347--5360},
}

@article{dai_regularized_2023,
	title = {Regularized estimation of {Kronecker} structured covariance matrix using modified {Cholesky} decomposition},
	issn = {0094-9655},
	url = {https://doi.org/10.1080/00949655.2023.2291536},
	doi = {10.1080/00949655.2023.2291536},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Dai, Deliang and Hao, Chengcheng and Jin, Shaobo and Liang, Yuli},
	month = dec,
	year = {2023},
	note = {Publisher: Taylor \& Francis},
	pages = {1--26},
	annote = {doi: 10.1080/00949655.2023.2291536},
}

@article{zhang_note_2021,
	title = {A {Note} on {Wishart} and {Inverse} {Wishart} {Priors} for {Covariance} {Matrix}},
	volume = {1},
	doi = {10.35566/jbds/v1n2/p2},
	journal = {Journal of Behavioral Data Science},
	author = {Zhang, Zhiyong},
	month = jan,
	year = {2021},
}

@article{zhang_highdimensional_2021,
	title = {High‐dimensional multivariate geostatistics: {A} {Bayesian} matrix‐normal approach},
	volume = {32},
	number = {4},
	journal = {Environmetrics},
	author = {Zhang, Lu and Banerjee, Sudipto and Finley, Andrew O.},
	year = {2021},
	note = {ISBN: 1180-4009
Publisher: Wiley Online Library},
	pages = {e2675},
}

@book{oppenheim_signals_1996,
	address = {USA},
	title = {Signals \& systems (2nd ed.)},
	isbn = {0-13-814757-4},
	publisher = {Prentice-Hall, Inc.},
	author = {Oppenheim, Alan V. and Willsky, Alan S. and Nawab, S. Hamid},
	year = {1996},
}

@book{hald_history_2005,
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {A {History} of {Probability} and {Statistics} and {Their} {Applications} before 1750},
	isbn = {978-0-471-72517-6},
	url = {https://books.google.com/books?id=pOQy6-qnVx8C},
	publisher = {Wiley},
	author = {Hald, A.},
	year = {2005},
}

@misc{noauthor_ordeal_2016,
	title = {The {Ordeal} of {Herbert} {Hoover}},
	url = {https://www.archives.gov/publications/prologue/2004/summer/hoover},
	abstract = {Summer 2004, Vol. 36, No. 2 By Richard Norton Smith and Timothy Walch Herbert Hoover's inability to use the radio as an effective communication tool contributed to his election loss in 1932. (Herbert Hoover Library) Few Americans have known greater acclaim or more bitter criticism than Herbert Hoover. The son of a Quaker blacksmith, Hoover was orphaned at the age of nine and sent to live with relatives in Oregon.},
	language = {en},
	urldate = {2025-02-05},
	journal = {National Archives},
	month = aug,
	year = {2016},
	file = {Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\5Q8Z8VJ3\\hoover.html:text/html},
}

@misc{noauthor_if_nodate,
	title = {If {You} {Build} {It}, {He} {Will} {Come} {\textbar} {Quality} {Magazine}},
	url = {https://www.qualitymag.com/articles/97825-if-you-build-it-he-will-come},
	urldate = {2025-02-05},
}

@book{woodford_this_2001,
	series = {Great {Lakes} books},
	title = {This is {Detroit}, 1701-2001},
	isbn = {978-0-8143-2914-6},
	url = {https://books.google.com/books?id=cVP055AfqNEC},
	publisher = {Wayne State University Press},
	author = {Woodford, A.M.},
	year = {2001},
	lccn = {2001000689},
}

@misc{noauthor_trump_nodate,
	title = {Trump, {Harris} voters mostly say immigrants fill jobs {US} citizens don’t want {\textbar} {Pew} {Research} {Center}},
	url = {https://www.pewresearch.org/short-reads/2024/10/21/most-us-voters-say-immigrants-no-matter-their-legal-status-mostly-take-jobs-citizens-dont-want/},
	urldate = {2025-02-05},
}

@misc{noauthor_fact_nodate,
	title = {Fact {Check} {Team}: {Cities} that called to 'defund police' grappling with crime surge boost po},
	url = {https://abc3340.com/news/nation-world/fact-check-team-cities-that-called-to-defund-police-grappling-with-crime-surge-boost-police-funding-amid-staffing-shortfalls-washington-dc-baltimore-los-angeles-new-york-george-floyd-police-reform-public-safety-mental-health-social-services},
	urldate = {2025-02-05},
}

@misc{noauthor_yellowstone_nodate,
	title = {Yellowstone {National} {Park} - {National} {Parks} - {Research} {Guides} at {Ohio} {State} {University}},
	url = {https://guides.osu.edu/c.php?g=870252&p=6250943},
	urldate = {2025-02-05},
}

@misc{noauthor_trump_nodate-1,
	title = {Trump promises wall and massive deportation program - {POLITICO}},
	url = {https://www.politico.com/story/2016/08/donald-trump-immigration-address-arizona-227612},
	urldate = {2025-02-05},
}

@article{hill_how_2020,
	chapter = {U.S.},
	title = {How {George} {Floyd} {Was} {Killed} in {Police} {Custody}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2020/05/31/us/george-floyd-investigation.html},
	abstract = {The Times has reconstructed the death of George Floyd on May 25. Security footage, witness videos and official documents show how a series of actions by officers turned fatal.},
	language = {en-US},
	urldate = {2025-02-05},
	journal = {The New York Times},
	author = {Hill, Evan and Tiefenthäler, Ainara and Triebert, Christiaan and Jordan, Drew and Willis, Haley and Stein, Robin},
	month = jun,
	year = {2020},
	keywords = {Black Lives Matter Movement, Black People, Chauvin, Derek (1976- ), Floyd, George (d 2020), George Floyd Protests (2020), Minneapolis (Minn), Police Department (Minneapolis, Minn)},
	file = {Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\Y8WHTZSU\\george-floyd-investigation.html:text/html},
}

@article{gilovich_hot_1985,
	title = {The hot hand in basketball: {On} the misperception of random sequences},
	volume = {17},
	issn = {0010-0285},
	url = {https://www.sciencedirect.com/science/article/pii/0010028585900106},
	doi = {10.1016/0010-0285(85)90010-6},
	abstract = {We investigate the origin and the validity of common beliefs regarding “the hot hand” and “streak shooting” in the game of basketball. Basketball players and fans alike tend to believe that a player's chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell's varsity teams. The outcomes of previous shots influenced Cornell players' predictions but not their performance. The belief in the hot hand and the “detection” of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process.},
	number = {3},
	journal = {Cognitive Psychology},
	author = {Gilovich, Thomas and Vallone, Robert and Tversky, Amos},
	month = jul,
	year = {1985},
	pages = {295--314},
}

@article{miller_surprised_2018,
	title = {Surprised by the {Hot} {Hand} {Fallacy}? {A} {Truth} in the {Law} of {Small} {Numbers}},
	volume = {86},
	issn = {129682},
	url = {https://doi.org/10.3982/ECTA14943},
	doi = {10.3982/ECTA14943},
	abstract = {We prove that a subtle but substantial bias exists in a common measure of the conditional dependence of present outcomes on streaks of past outcomes in sequential data. The magnitude of this streak selection bias generally decreases as the sequence gets longer, but increases in streak length, and remains substantial for a range of sequence lengths often used in empirical work. We observe that the canonical study in the influential hot hand fallacy literature, along with replications, are vulnerable to the bias. Upon correcting for the bias, we find that the longstanding conclusions of the canonical study are reversed.},
	number = {6},
	journal = {Econometrica},
	author = {Miller, Joshua B. and Sanjurjo, Adam},
	month = nov,
	year = {2018},
	note = {Publisher: Econometric Society},
	pages = {2019--2047},
}

@article{tibshirani_lasso_1997,
	title = {{THE} {LASSO} {METHOD} {FOR} {VARIABLE} {SELECTION} {IN} {THE} {COX} {MODEL}},
	volume = {16},
	issn = {0277-6715},
	url = {https://doi.org/10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3},
	doi = {10.1002/(SICI)1097-0258(19970228)16:4<385::AID-SIM380>3.0.CO;2-3},
	abstract = {Abstract I propose a new method for variable selection and shrinkage in Cox's proportional hazards model. My proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. As a result it reduces the estimation variance while providing an interpretable final model. The method is a variation of the ?lasso? proposal of Tibshirani, designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting. ? 1997 by John Wiley \& Sons, Ltd.},
	number = {4},
	urldate = {2025-02-14},
	journal = {Statistics in Medicine},
	author = {TIBSHIRANI, ROBERT},
	month = feb,
	year = {1997},
	note = {Publisher: John Wiley \& Sons, Ltd},
	pages = {385--395},
}

@article{harden_simulating_2018,
	title = {Simulating {Duration} {Data} for the {Cox} {Model}},
	volume = {7},
	doi = {10.1017/psrm.2018.19},
	journal = {Political Science Research and Methods},
	author = {Harden, Jeffrey and Kropko, Jonathan},
	month = may,
	year = {2018},
	pages = {1--8},
}

@article{van_ravenzwaaij_simple_2018,
	title = {A simple introduction to {Markov} {Chain} {Monte}-{Carlo} sampling.},
	volume = {25},
	issn = {1531-5320 1069-9384},
	doi = {10.3758/s13423-016-1015-8},
	abstract = {Markov Chain Monte-Carlo (MCMC) is an increasingly popular method for obtaining information about distributions, especially for estimating posterior  distributions in Bayesian inference. This article provides a very basic  introduction to MCMC sampling. It describes what MCMC is, and what it can be used  for, with simple illustrative examples. Highlighted are some of the benefits and  limitations of MCMC sampling, as well as different approaches to circumventing  the limitations most likely to trouble cognitive scientists.},
	language = {eng},
	number = {1},
	journal = {Psychonomic bulletin \& review},
	author = {van Ravenzwaaij, Don and Cassey, Pete and Brown, Scott D.},
	month = feb,
	year = {2018},
	pmid = {26968853},
	pmcid = {PMC5862921},
	note = {Place: United States},
	keywords = {Humans, Algorithms, Bayesian inference, *Bayes Theorem, *Markov Chains, *Monte Carlo Method, Markov Chain Monte–Carlo, MCMC, Tutorial},
	pages = {143--154},
}

@book{wackerly_mathematical_2008,
	title = {Mathematical statistics with applications},
	volume = {7},
	publisher = {Thomson Brooks/Cole Belmont, CA},
	author = {Wackerly, Dennis D and Mendenhall, William and Scheaffer, Richard L},
	year = {2008},
}

@incollection{box_statistics_2005,
	title = {Statistics for experimenters},
	booktitle = {Wiley series in probability and statistics},
	publisher = {Wiley Hoboken, NJ},
	author = {Box, George EP and Hunter, J. Stuart and Hunter, William G.},
	year = {2005},
}

@article{klauenberg_tutorial_2015,
	title = {A tutorial on {Bayesian} {Normal} linear regression},
	volume = {52},
	issn = {0026-1394},
	url = {https://dx.doi.org/10.1088/0026-1394/52/6/878},
	doi = {10.1088/0026-1394/52/6/878},
	abstract = {Regression is a common task in metrology and often applied to calibrate instruments, evaluate inter-laboratory comparisons or determine fundamental constants, for example. Yet, a regression model cannot be uniquely formulated as a measurement function, and consequently the Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly. Bayesian inference, however, is well suited to regression tasks, and has the advantage of accounting for additional a priori information, which typically robustifies analyses. Furthermore, it is anticipated that future revisions of the GUM shall also embrace the Bayesian view.

Guidance on Bayesian inference for regression tasks is largely lacking in metrology. For linear regression models with Gaussian measurement errors this tutorial gives explicit guidance. Divided into three steps, the tutorial first illustrates how a priori knowledge, which is available from previous experiments, can be translated into prior distributions from a specific class. These prior distributions have the advantage of yielding analytical, closed form results, thus avoiding the need to apply numerical methods such as Markov Chain Monte Carlo. Secondly, formulas for the posterior results are given, explained and illustrated, and software implementations are provided. In the third step, Bayesian tools are used to assess the assumptions behind the suggested approach.

These three steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) are critical to Bayesian inference. The general guidance given here for Normal linear regression tasks is accompanied by a simple, but real-world, metrological example. The calibration of a flow device serves as a running example and illustrates the three steps. It is shown that prior knowledge from previous calibrations of the same sonic nozzle enables robust predictions even for extrapolations.},
	number = {6},
	journal = {Metrologia},
	author = {Klauenberg, Katy and Wübbeler, Gerd and Mickan, Bodo and Harris, Peter and Elster, Clemens},
	month = nov,
	year = {2015},
	note = {Publisher: IOP Publishing},
	pages = {878},
}

@article{trecroci_mental_2020,
	title = {Mental fatigue impairs physical activity, technical and decision-making performance during small-sided games.},
	volume = {15},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0238461},
	abstract = {The aim of this study was to investigate the effects of mental fatigue on physical activity, technical and decision-making performance during small-sided  games. Nine sub-elite soccer players were enrolled in the study. The players  performed two small-sided games on two occasions within a crossover experimental  design. Before each game, they underwent a mental fatiguing task (Stroop task)  and a control task (documentary watching) in a randomized, counterbalanced order.  Players' physical activity, technical, and decision-making performance were  obtained during small-sided games by GPS and video scouting. Results showed that  distance in acceleration covered per min, negative passes, passing accuracy, and  shot accuracy were likely impaired than control task after a mental fatiguing  protocol. Decision-making performance of negative passes, passes accuracy, and  dribbling accuracy resulted also likely decreased compared with control task.  These findings demonstrated that mental fatigue impacted on technical,  GPS-derived, and soccer-specific decision-making performance during SSG. In  conclusion, avoiding cognitively demanding tasks before playing soccer-specific  activities may be advisable to preserve players' physical activity, technical,  and decision-making skills.},
	language = {eng},
	number = {9},
	journal = {PloS one},
	author = {Trecroci, Athos and Boccolini, Gabriele and Duca, Marco and Formenti, Damiano and Alberti, Giampietro},
	year = {2020},
	pmid = {32903263},
	pmcid = {PMC7480836},
	note = {Place: United States},
	keywords = {Humans, Male, Adolescent, Young Adult, Athletes, Athletic Performance, Decision Making/*physiology, Exercise/*psychology, Heart Rate, Mental Fatigue/metabolism/*physiopathology, Soccer, Stroop Test},
	pages = {e0238461},
}

@article{gorroochurn_thirteen_2014,
	title = {Thirteen {Correct} {Solutions} to the “{Problem} of {Points}” and {Their} {Histories}},
	volume = {36},
	issn = {1866-7414},
	url = {https://doi.org/10.1007/s00283-014-9461-5},
	doi = {10.1007/s00283-014-9461-5},
	number = {3},
	journal = {The Mathematical Intelligencer},
	author = {Gorroochurn, Prakash},
	month = sep,
	year = {2014},
	pages = {56--64},
}

@article{cox_regression_1972,
	title = {Regression {Models} and {Life}-{Tables}},
	volume = {34},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2985181},
	abstract = {[The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.]},
	number = {2},
	urldate = {2025-02-17},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Cox, D. R.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	pages = {187--220},
}

@article{duan_bayesian_2018,
	title = {Bayesian variable selection for parametric survival model with applications to cancer omics data},
	volume = {12},
	journal = {Human genomics},
	author = {Duan, Weiwei and Zhang, Ruyang and Zhao, Yang and Shen, Sipeng and Wei, Yongyue and Chen, Feng and Christiani, David C.},
	year = {2018},
	note = {Publisher: Springer},
	pages = {1--15},
}

@article{faraggi_bayesian_1998,
	title = {Bayesian variable selection method for censored survival data},
	journal = {Biometrics},
	author = {Faraggi, David and Simon, Richard},
	year = {1998},
	note = {ISBN: 0006-341X
Publisher: JSTOR},
	pages = {1475--1485},
}

@article{held_objective_2016,
	title = {Objective {Bayesian} model selection for {Cox} regression},
	volume = {35},
	number = {29},
	journal = {Statistics in medicine},
	author = {Held, Leonhard and Gravestock, Isaac and Sabanés Bové, Daniel},
	year = {2016},
	note = {ISBN: 0277-6715
Publisher: Wiley Online Library},
	pages = {5376--5390},
}

@article{sha_bayesian_2006,
	title = {Bayesian variable selection for the analysis of microarray data with censored outcomes},
	volume = {22},
	number = {18},
	journal = {Bioinformatics},
	author = {Sha, Naijun and Tadesse, Mahlet G. and Vannucci, Marina},
	year = {2006},
	note = {ISBN: 1367-4811
Publisher: Oxford University Press},
	pages = {2262--2268},
}

@article{ibrahim_bayesian_1999,
	title = {Bayesian variable selection for proportional hazards models},
	volume = {27},
	number = {4},
	journal = {Canadian Journal of Statistics},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and MacEachern, Steven N.},
	year = {1999},
	note = {ISBN: 0319-5724
Publisher: Wiley Online Library},
	pages = {701--717},
}

@article{jelena_bradic_structured_2015,
	title = {Structured estimation for the nonparametric {Cox} model},
	volume = {9},
	url = {https://doi.org/10.1214/15-EJS1004},
	doi = {10.1214/15-EJS1004},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {{Jelena Bradic} and {Rui Song}},
	month = jan,
	year = {2015},
	pages = {492--534},
}

@article{jelena_bradic_regularization_2011,
	title = {Regularization for {Cox}’s proportional hazards model with {NP}-dimensionality},
	volume = {39},
	url = {https://doi.org/10.1214/11-AOS911},
	doi = {10.1214/11-AOS911},
	number = {6},
	journal = {The Annals of Statistics},
	author = {{Jelena Bradic} and {Jianqing Fan} and {Jiancheng Jiang}},
	month = dec,
	year = {2011},
	pages = {3092--3120},
}

@inproceedings{griffin_alternative_2005,
	title = {Alternative prior distributions for variable selection with very many more variables than observations},
	url = {https://api.semanticscholar.org/CorpusID:16114694},
	author = {Griffin, Jim E. and Brown, Philip J.},
	year = {2005},
}

@article{hsiang_bayesian_1975,
	title = {A {Bayesian} {View} on {Ridge} {Regression}},
	volume = {24},
	issn = {2515-7884},
	url = {https://doi.org/10.2307/2987923},
	doi = {10.2307/2987923},
	number = {4},
	urldate = {2025-02-17},
	journal = {Journal of the Royal Statistical Society Series D: The Statistician},
	author = {Hsiang, T. C.},
	month = dec,
	year = {1975},
	pages = {267--268},
}

@article{qing_li_bayesian_2010,
	title = {The {Bayesian} elastic net},
	volume = {5},
	url = {https://doi.org/10.1214/10-BA506},
	doi = {10.1214/10-BA506},
	number = {1},
	journal = {Bayesian Analysis},
	author = {{Qing Li} and {Nan Lin}},
	month = mar,
	year = {2010},
	pages = {151--170},
}

@article{carvalho_horseshoe_2010-1,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	issn = {00063444, 14643510},
	url = {http://www.jstor.org/stable/25734098},
	abstract = {[This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator's advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator's tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.]},
	number = {2},
	urldate = {2025-02-17},
	journal = {Biometrika},
	author = {CARVALHO, CARLOS M. and POLSON, NICHOLAS G. and SCOTT, JAMES G.},
	year = {2010},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {465--480},
}

@article{park_bayesian_2008,
	title = {The {Bayesian} {Lasso}},
	volume = {103},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214508000000337},
	doi = {10.1198/016214508000000337},
	number = {482},
	journal = {Journal of the American Statistical Association},
	author = {Park, Trevor and Casella, George},
	month = jun,
	year = {2008},
	note = {Publisher: ASA Website},
	pages = {681--686},
	annote = {doi: 10.1198/016214508000000337},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} via the {Lasso}},
	volume = {58},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2346178},
	abstract = {[We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.]},
	number = {1},
	urldate = {2025-02-17},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Tibshirani, Robert},
	year = {1996},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	pages = {267--288},
}

@article{hoerl_ridge_1970,
	title = {Ridge {Regression}: {Biased} {Estimation} for {Nonorthogonal} {Problems}},
	volume = {12},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1267351},
	doi = {10.2307/1267351},
	abstract = {[In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.]},
	number = {1},
	urldate = {2025-02-17},
	journal = {Technometrics},
	author = {Hoerl, Arthur E. and Kennard, Robert W.},
	year = {1970},
	note = {Publisher: [Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
	pages = {55--67},
}

@article{jaynes_prior_1968,
	title = {Prior probabilities},
	volume = {4},
	number = {3},
	journal = {IEEE Transactions on systems science and cybernetics},
	author = {Jaynes, Edwin T.},
	year = {1968},
	note = {ISBN: 0536-1567
Publisher: IEEE},
	pages = {227--241},
}

@article{kass_selection_1996,
	title = {The selection of prior distributions by formal rules},
	volume = {91},
	number = {435},
	journal = {Journal of the American statistical Association},
	author = {Kass, Robert E. and Wasserman, Larry},
	year = {1996},
	note = {ISBN: 0162-1459
Publisher: Taylor \& Francis},
	pages = {1343--1370},
}

@inproceedings{jadon_comprehensive_2024,
	title = {A comprehensive survey of regression-based loss functions for time series forecasting},
	publisher = {Springer},
	author = {Jadon, Aryan and Patil, Avinash and Jadon, Shruti},
	year = {2024},
	pages = {117--147},
}

@article{rao_statistical_1958,
	title = {Some statistical methods for comparison of growth curves},
	volume = {14},
	number = {1},
	journal = {Biometrics},
	author = {Rao, C. Radhakrishna},
	year = {1958},
	note = {ISBN: 0006-341X
Publisher: JSTOR},
	pages = {1--17},
}

@article{nikolova_curve_2009,
	title = {Curve fitting of sensors’ characteristics},
	journal = {Annual Journal of Electronics, ISSN},
	author = {Nikolova, B. and Nikolov, G. and Todorov, M.},
	year = {2009},
	pages = {1313--1842},
}

@article{fama_behavior_1965,
	title = {The behavior of stock-market prices},
	volume = {38},
	number = {1},
	journal = {The journal of Business},
	author = {Fama, Eugene F.},
	year = {1965},
	note = {ISBN: 0021-9398
Publisher: JSTOR},
	pages = {34--105},
}

@article{buescher_temporomandibular_2007,
	title = {Temporomandibular joint disorders},
	volume = {76},
	number = {10},
	journal = {American family physician},
	author = {Buescher, Jennifer J.},
	year = {2007},
	pages = {1477--1482},
}

@article{gelfand_efficient_1995,
	title = {Efficient {Parametrisations} for {Normal} {Linear} {Mixed} {Models}},
	volume = {82},
	issn = {00063444, 14643510},
	url = {http://www.jstor.org/stable/2337527},
	doi = {10.2307/2337527},
	abstract = {[The generality and easy programmability of modern sampling-based methods for maximisation of likelihoods and summarisation of posterior distributions have led to a tremendous increase in the complexity and dimensionality of the statistical models used in practice. However, these methods can often be extremely slow to converge, due to high correlations between, or weak identifiability of, certain model parameters. We present simple hierarchical centring reparametrisations that often give improved convergence for a broad class of normal linear mixed models. In particular, we study the two-stage hierarchical normal linear models, the Laird-Ware model for longitudinal data, and a general structure for hierarchically nested linear models. Using analytical arguments, simulation studies, and an example involving clinical markers of acquired immune deficiency syndrome (AIDS), we indicate when reparametrisation is likely to provide substantial gains in efficiency.]},
	number = {3},
	urldate = {2025-02-20},
	journal = {Biometrika},
	author = {Gelfand, Alan E. and Sahu, Sujit K. and Carlin, Bradley P.},
	year = {1995},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {479--488},
}

@article{browne_illustration_2004,
	title = {An illustration of the use of reparameterisation methods for improving {MCMC} efficiency in crossed random effect models},
	volume = {16},
	number = {1},
	journal = {Multilevel modelling newsletter},
	author = {Browne, William J.},
	year = {2004},
	note = {Publisher: Citeseer},
	pages = {13--25},
}

@article{morris_using_2019,
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	number = {11},
	journal = {Statistics in medicine},
	author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
	year = {2019},
	note = {ISBN: 0277-6715
Publisher: Wiley Online Library},
	pages = {2074--2102},
}

@article{edwards_pascal_1982,
	title = {Pascal and the {Problem} of {Points}},
	volume = {50},
	issn = {03067734, 17515823},
	url = {http://www.jstor.org/stable/1402496},
	doi = {10.2307/1402496},
	abstract = {The Pascal-Fermat correspondence and Pascal's "Trait\&\#xe9; du triangle arithm\&\#xe9;tique" are re-examined with special reference to the Problem of Points. It is concluded that, contrary to the views of some modern commentators, Pascal was responsible for the modern solution to the Problem, and that, in demonstrating it, he made use not only of mathematical induction, but of the concepts of expectation and of the binomial distribution for equal chances. /// Une relecture de la correspondance entre Pascal et Fermat et du "Trait\&\#xe9; du triangle arithm\&\#xe9;tique" de Pascal a men\&\#xe9; l'auteur \&\#xe0; la conclusion-contraire \&\#xe0; celle de quelques commentateurs modernes-que l'on doit \&\#xe0; Pascal lui-m\&\#xea;me la solution moderne du' Probl\&\#xe8;me des Partis' et que, en la d\&\#xe9;montrant, il utilisa non seulement le principe de l'induction math\&\#xe9;matique, mais aussi le concept d'esp\&\#xe9;rance, ainsi que la loi binomiale, dans le cas des probabilit\&\#xe9;s \&\#xe9;gales.},
	number = {3},
	urldate = {2025-02-24},
	journal = {International Statistical Review / Revue Internationale de Statistique},
	author = {Edwards, A. W. F.},
	year = {1982},
	note = {Publisher: [Wiley, International Statistical Institute (ISI)]},
	pages = {259--266},
}

@misc{kuo_housing_2024,
	title = {Housing {Costs} {Continue} to {Drive} {Inflation}},
	url = {https://eyeonhousing.org/2024/11/housing-costs-continue-to-drive-inflation/},
	abstract = {Inflation picked up again in October, showing the last mile to the 2\% target will be the hardest. Shelter costs remained the main driver of inflation, accounting for over 65\% of the 12-month increase in the all items less food and energy index. However, the year-over-year change in the shelter index has been below 5\%},
	language = {en-US},
	urldate = {2025-02-27},
	author = {Kuo, Fan-Yu},
	month = nov,
	year = {2024},
	file = {Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\R7EM79KC\\housing-costs-continue-to-drive-inflation.html:text/html},
}

@misc{ludwig_voters_2025,
	title = {Voters {Were} {Right} {About} the {Economy}. {The} {Data} {Was} {Wrong}.},
	url = {https://www.politico.com/news/magazine/2025/02/11/democrats-tricked-strong-economy-00203464},
	abstract = {Here’s why unemployment is higher, wages are lower and growth less robust than government statistics suggest.},
	language = {en},
	urldate = {2025-02-27},
	journal = {POLITICO},
	author = {Ludwig, Eugene},
	month = feb,
	year = {2025},
	file = {Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\FW5WP4TI\\democrats-tricked-strong-economy-00203464.html:text/html},
}

@misc{noauthor_cuyahoga_nodate,
	title = {The {Cuyahoga} {River} {Caught} {Fire} at {Least} a {Dozen} {Times}, but {No} {One} {Cared} {Until} 1969 {\textbar} {Smithsonian}},
	url = {https://www.smithsonianmag.com/history/cuyahoga-river-caught-fire-least-dozen-times-no-one-cared-until-1969-180972444/},
	urldate = {2025-02-27},
	file = {The Cuyahoga River Caught Fire at Least a Dozen Times, but No One Cared Until 1969 | Smithsonian:C\:\\Users\\Jack Swanson\\Zotero\\storage\\3GVNAFPJ\\cuyahoga-river-caught-fire-least-dozen-times-no-one-cared-until-1969-180972444.html:text/html},
}

@misc{guida_democrats_2025,
	title = {Democrats {Avoided} {Criticizing} {Biden}. {Jason} {Furman}’s {Doing} {It} {Now}.},
	url = {https://www.politico.com/news/magazine/2025/02/20/was-bidenomics-a-mistake-00204951},
	abstract = {One of the Obama administration’s top economists is offering a scorching public critique of Biden’s economic legacy.},
	language = {en},
	urldate = {2025-02-27},
	journal = {POLITICO},
	author = {Guida, Victoria},
	month = feb,
	year = {2025},
	file = {Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\LZXCJMUG\\was-bidenomics-a-mistake-00204951.html:text/html},
}

@misc{noauthor_democrats_nodate,
	title = {Democrats and the {Working} {Class}: {Ted} {Kennedy}'s {Warning}},
	url = {https://foreignpolicy.com/2024/10/21/democrats-working-class-kennedy-warning/},
	urldate = {2025-02-27},
	file = {Democrats and the Working Class\: Ted Kennedy's Warning:C\:\\Users\\Jack Swanson\\Zotero\\storage\\NLI3T4LZ\\democrats-working-class-kennedy-warning.html:text/html},
}

@article{kaufman_how_2024,
	chapter = {Magazine},
	title = {How {NAFTA} {Broke} {American} {Politics}},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2024/09/03/magazine/nafta-tarriffs-economy-trump-kamala-harris.html},
	abstract = {Since its passage in 1993, the trade agreement has played an outsize role in presidential elections — which now often hinge on the three Rust Belt states it helped to hollow out.},
	language = {en-US},
	urldate = {2025-02-27},
	journal = {The New York Times},
	author = {Kaufman, Dan},
	month = sep,
	year = {2024},
	keywords = {Baldwin, Tammy Suzanne Green, Biden, Joseph R Jr, Clinton, Bill, Factories and Manufacturing, Harris, Kamala D, Michigan, Milwaukee (Wis), North American Free Trade Agreement, Obama, Barack, Ohio, Organized Labor, Pennsylvania, Presidential Election of 2024, Trump, Donald J, United States Politics and Government, Vance, J D, Wisconsin},
	file = {Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\NMR9HJSK\\nafta-tarriffs-economy-trump-kamala-harris.html:text/html},
}

@article{zhang_note_2021-1,
	title = {A {Note} on {Wishart} and {Inverse} {Wishart} {Priors} for {Covariance} {Matrix}},
	volume = {1},
	doi = {10.35566/jbds/v1n2/p2},
	journal = {Journal of Behavioral Data Science},
	author = {Zhang, Zhiyong},
	month = jan,
	year = {2021},
}

@article{koenker_goodness_1999,
	title = {Goodness of {Fit} and {Related} {Inference} {Processes} for {Quantile} {Regression}},
	volume = {94},
	issn = {01621459, 1537274X},
	url = {http://www.jstor.org/stable/2669943},
	doi = {10.2307/2669943},
	abstract = {[We introduce a goodness-of-fit process for quantile regression analogous to the conventional R$^{\textrm{2}}$ statistic of least squares regression. Several related inference processes designed to test composite hypotheses about the combined effect of several covariates over an entire range of conditional quantile functions are also formulated. The asymptotic behavior of the inference processes is shown to be closely related to earlier p-sample goodness-of-fit theory involving Bessel processes. The approach is illustrated with some hypothetical examples, an application to recent empirical models of international economic growth, and some Monte Carlo evidence.]},
	number = {448},
	urldate = {2025-04-29},
	journal = {Journal of the American Statistical Association},
	author = {Koenker, Roger and {José A. F. Machado}},
	year = {1999},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1296--1310},
}

@article{yu_bayesian_2001,
	title = {Bayesian quantile regression},
	volume = {54},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/S0167715201001249},
	doi = {10.1016/S0167-7152(01)00124-9},
	abstract = {The paper introduces the idea of Bayesian quantile regression employing a likelihood function that is based on the asymmetric Laplace distribution. It is shown that irrespective of the original distribution of the data, the use of the asymmetric Laplace distribution is a very natural and effective way for modelling Bayesian quantile regression. The paper also demonstrates that improper uniform priors for the unknown model parameters yield a proper joint posterior. The approach is illustrated via a simulated and two real data sets.},
	number = {4},
	journal = {Statistics \& Probability Letters},
	author = {Yu, Keming and Moyeed, Rana A.},
	month = oct,
	year = {2001},
	keywords = {Bayesian inference, Asymmetric Laplace distribution, Markov chain Monte Carlo methods, Quantile regression},
	pages = {437--447},
}

@article{benoit_bayesqr_2017,
	title = {{bayesQR}: {A} {Bayesian} {Approach} to {Quantile} {Regression}},
	volume = {76},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v076i07},
	doi = {10.18637/jss.v076.i07},
	abstract = {After its introduction by Koenker and Basset (1978), quantile regression has become an important and popular tool to investigate the conditional response distribution in regression. The R package bayesQR contains a number of routines to estimate quantile regression parameters using a Bayesian approach based on the asymmetric Laplace distribution. The package contains functions for the typical quantile regression with continuous dependent variable, but also supports quantile regression for binary dependent variables. For both types of dependent variables, an approach to variable selection using the adaptive lasso approach is provided. For the binary quantile regression model, the package also contains a routine that calculates the fitted probabilities for each vector of predictors. In addition, functions for summarizing the results, creating traceplots, posterior histograms and drawing quantile plots are included. This paper starts with a brief overview of the theoretical background of the models used in the bayesQR package. The main part of this paper discusses the computational problems that arise in the implementation of the procedure and illustrates the usefulness of the package through selected examples.},
	number = {7},
	urldate = {2025-04-29},
	journal = {Journal of Statistical Software},
	author = {Benoit, Dries F. and Van den Poel, Dirk},
	month = jan,
	year = {2017},
	note = {Section: Articles},
	pages = {1 -- 32},
}

@article{ruppert_selecting_2002,
	title = {Selecting the {Number} of {Knots} for {Penalized} {Splines}},
	volume = {11},
	issn = {1061-8600},
	url = {https://doi.org/10.1198/106186002853},
	doi = {10.1198/106186002853},
	number = {4},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Ruppert, David},
	month = dec,
	year = {2002},
	note = {Publisher: ASA Website},
	pages = {735--757},
	annote = {doi: 10.1198/106186002853},
}

@article{denison_automatic_1998,
	title = {Automatic {Bayesian} curve fitting},
	volume = {60},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Denison, D. G. T. and Mallick, B. K. and Smith, A. F. M.},
	year = {1998},
	note = {ISBN: 1369-7412
Publisher: Wiley Online Library},
	pages = {333--350},
}

@article{metodiev_easily_2024,
	title = {Easily {Computed} {Marginal} {Likelihoods} from {Posterior} {Simulation} {Using} the {THAMES} {Estimator}},
	volume = {-1},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/advance-publication/Easily-Computed-Marginal-Likelihoods-from-Posterior-Simulation-Using-the-THAMES/10.1214/24-BA1422.full},
	doi = {10.1214/24-BA1422},
	abstract = {We propose an easily computed estimator of the marginal likelihood from posterior simulation output, via reciprocal importance sampling, combining earlier proposals of DiCiccio et al (1997) and Robert and Wraith (2009). This involves only the unnormalized posterior densities from the sampled parameter values, and does not involve additional simulations beyond the main posterior simulation, or additional complicated calculations, provided that the parameter space is unconstrained. Even if this is not the case, the estimator is easily adjusted by a simple Monte Carlo approximation. It is unbiased for the reciprocal of the marginal likelihood, consistent, has finite variance, and is asymptotically normal. It involves one user-specified control parameter, and we derive an optimal way of specifying this. We illustrate it with several numerical examples.},
	number = {-1},
	urldate = {2025-05-16},
	journal = {Bayesian Analysis},
	author = {Metodiev, Martin and Perrot-Dockès, Marie and Ouadah, Sarah and Irons, Nicholas J. and Latouche, Pierre and Raftery, Adrian E.},
	month = jan,
	year = {2024},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {62-04, 62F12, 62F15, marginal likelihood estimation, reciprocal importance sampling},
	pages = {1--28},
	file = {Full Text PDF:C\:\\Users\\Jack Swanson\\Zotero\\storage\\ZWDQPAMY\\Metodiev et al. - 2024 - Easily Computed Marginal Likelihoods from Posterio.pdf:application/pdf},
}

@article{goepp_spline_2025,
	title = {Spline regression with automatic knot selection},
	volume = {202},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947324001270},
	doi = {10.1016/j.csda.2024.108043},
	abstract = {Spline regression has proven to be a useful tool for nonparametric regression. The flexibility of this function family is based on basepoints defining shifts in the behavior of the function – called knots. The question of setting the adequate number of knots and their placement is usually overcome by penalizing over the spline's overall smoothness (e.g. P-splines). However, there are areas of application where finding the best knot placement is of interest. A new method is introduced for automatically selecting knots in spline regression. The approach consists in setting many initial knots and fitting the spline regression through a penalized likelihood procedure called adaptive ridge, which discards the least relevant knots. The method – called A-splines, for adaptive splines – compares favorably with other knot selection methods: it runs way faster (∼10 to ∼400 faster) than comparable methods and has close to equal predictive performance. A-splines are applied to both simulated and real datasets.},
	journal = {Computational Statistics \& Data Analysis},
	author = {Goepp, Vivien and Bouaziz, Olivier and Nuel, Grégory},
	month = feb,
	year = {2025},
	keywords = {B-splines, Adaptive ridge, Changepoint detection, Penalized likelihood, Spline regression},
	pages = {108043},
}

@misc{souza_switching_2013,
	title = {Switching {Nonparametric} {Regression} {Models} and the {Motorcycle} {Data} revisited},
	url = {http://arxiv.org/abs/1305.2227},
	doi = {10.48550/arXiv.1305.2227},
	abstract = {We propose a methodology to analyze data arising from a curve that, over its domain, switches among J states. We consider a sequence of response variables, where each response y depends on a covariate x according to an unobserved state z. The states form a stochastic process and their possible values are j=1,...,J. If z equals j the expected response of y is one of J unknown smooth functions evaluated at x. We call this model a switching nonparametric regression model. We develop an EM algorithm to estimate the parameters of the latent state process and the functions corresponding to the J states. We also obtain standard errors for the parameter estimates of the state process. We conduct simulation studies to analyze the frequentist properties of our estimates. We also apply the proposed methodology to the well-known motorcycle data set treating the data as coming from more than one simulated accident run with unobserved run labels.},
	urldate = {2025-05-20},
	publisher = {arXiv},
	author = {Souza, Camila P. E. de and Heckman, Nancy E.},
	month = may,
	year = {2013},
	note = {arXiv:1305.2227 [stat]},
	keywords = {Statistics - Methodology},
	annote = {Comment: The article has one supplementary pdf file (DeSouzaHeckman-supplementA.pdf)},
	file = {Preprint PDF:C\:\\Users\\Jack Swanson\\Zotero\\storage\\ZFPRNJS5\\Souza and Heckman - 2013 - Switching Nonparametric Regression Models and the .pdf:application/pdf;Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\TTEEZGY5\\1305.html:text/html},
}

@article{silverman_aspects_1985,
	title = {Some {Aspects} of the {Spline} {Smoothing} {Approach} to {Non}-{Parametric} {Regression} {Curve} {Fitting}},
	volume = {47},
	copyright = {© 1985 Royal Statistical Society},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1985.tb01327.x},
	doi = {10.1111/j.2517-6161.1985.tb01327.x},
	abstract = {Non-parametric regression using cubic splines is an attractive, flexible and widely-applicable approach to curve estimation. Although the basic idea was formulated many years ago, the method is not as widely known or adopted as perhaps it should be. The topics and examples discussed in this paper are intended to promote the understanding and extend the practicability of the spline smoothing methodology. Particular subjects covered include the basic principles of the method; the relation with moving average and other smoothing methods; the automatic choice of the amount of smoothing; and the use of residuals for diagnostic checking and model adaptation. The question of providing inference regions for curves – and for relevant properties of curves – is approached via a finite-dimensional Bayesian formulation.},
	language = {en},
	number = {1},
	urldate = {2025-05-20},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Silverman, B. W.},
	year = {1985},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1985.tb01327.x},
	keywords = {automatic smoothing, b-splines, bayesian inference, change point, cross-validation, empirical bayes, functionals of curves, generalized smoothing, growth curves, local reweighting, model choice, regression diagnostics, residuals, robust smoothing, roughness penalty, smoothing, surface estimation, variable kernel, weight function},
	pages = {1--21},
	file = {Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\UYGI4SST\\j.2517-6161.1985.tb01327.html:text/html},
}

@article{gelfand_bayesian_1994,
	title = {Bayesian {Model} {Choice}: {Asymptotics} and {Exact} {Calculations}},
	volume = {56},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2346123},
	abstract = {[Model determination is a fundamental data analytic task. Here we consider the problem of choosing among a finite (without loss of generality we assume two) set of models. After briefly reviewing classical and Bayesian model choice strategies we present a general predictive density which includes all proposed Bayesian approaches that we are aware of. Using Laplace approximations we can conveniently assess and compare the asymptotic behaviour of these approaches. Concern regarding the accuracy of these approximations for small to moderate sample sizes encourages the use of Monte Carlo techniques to carry out exact calculations. A data set fitted with nested non-linear models enables comparisons between proposals and between exact and asymptotic values.]},
	number = {3},
	urldate = {2025-05-20},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Gelfand, A. E. and Dey, D. K.},
	year = {1994},
	note = {Publisher: [Royal Statistical Society, Oxford University Press]},
	pages = {501--514},
}

@article{henderson_building_1981,
	title = {Building {Multiple} {Regression} {Models} {Interactively}},
	volume = {37},
	issn = {0006341X, 15410420},
	url = {http://www.jstor.org/stable/2530428},
	doi = {10.2307/2530428},
	abstract = {[Automated multiple regression model-building techniques often hide important aspects of data from the data analyst. Such features as nonlinearity, collinearity, outliers, and points with high leverage can profoundly affect automated analyses, yet remain undetected. An alternative technique uses interactive computing and exploratory methods to discover unexpected features of the data. One important advantage of this approach is that the data analyst can use knowledge of the subject matter in the resolution of difficulties. The methods are illustrated with reanalyses of the two data sets used by Hocking (1976, Biometrics 32, 1-44) to illustrate the use of automated regression methods.]},
	number = {2},
	urldate = {2025-05-20},
	journal = {Biometrics},
	author = {Henderson, Harold V. and Velleman, Paul F.},
	year = {1981},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {391--411},
}

@misc{garton_knot_2020,
	title = {Knot {Selection} in {Sparse} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2002.09538},
	doi = {10.48550/arXiv.2002.09538},
	abstract = {Knot-based, sparse Gaussian processes have enjoyed considerable success as scalable approximations to full Gaussian processes. Problems can occur, however, when knot selection is done by optimizing the marginal likelihood. For example, the marginal likelihood surface is highly multimodal, which can cause suboptimal knot placement where some knots serve practically no function. This is especially a problem when many more knots are used than are necessary, resulting in extra computational cost for little to no gains in accuracy. We propose a one-at-a-time knot selection algorithm to select both the number and placement of knots. Our algorithm uses Bayesian optimization to efficiently propose knots that are likely to be good and largely avoids the pathologies encountered when using the marginal likelihood as the objective function. We provide empirical results showing improved accuracy and speed over the current standard approaches.},
	urldate = {2025-05-21},
	publisher = {arXiv},
	author = {Garton, Nathaniel and Niemi, Jarad and Carriquiry, Alicia},
	month = feb,
	year = {2020},
	note = {arXiv:2002.09538 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Jack Swanson\\Zotero\\storage\\ZNIPVZYX\\Garton et al. - 2020 - Knot Selection in Sparse Gaussian Processes.pdf:application/pdf;Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\F8PI9VHJ\\2002.html:text/html},
}

@article{raftery_bayesian_1995,
	title = {Bayesian {Model} {Selection} in {Social} {Research}},
	volume = {25},
	issn = {00811750, 14679531},
	url = {http://www.jstor.org/stable/271063},
	doi = {10.2307/271063},
	abstract = {[It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection, and accounting for model uncertainty is presented. Implementing this is straightforward through the use of the simple and accurate BIC approximation, and it can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P-values and standard model selection procedures based on them. It also allows easy comparison of nonnested models, and permits the quantification of the evidence for a null hypothesis of interest, such as a convergence theory or a hypothesis about societal norms.]},
	urldate = {2025-05-29},
	journal = {Sociological Methodology},
	author = {Raftery, Adrian E.},
	year = {1995},
	note = {Publisher: [American Sociological Association, Wiley, Sage Publications, Inc.]},
	pages = {111--163},
}

@article{llorente_marginal_2023,
	title = {Marginal {Likelihood} {Computation} for {Model} {Selection} and {Hypothesis} {Testing}: {An} {Extensive} {Review}},
	volume = {65},
	issn = {0036-1445},
	shorttitle = {Marginal {Likelihood} {Computation} for {Model} {Selection} and {Hypothesis} {Testing}},
	url = {https://epubs.siam.org/doi/10.1137/20M1310849},
	doi = {10.1137/20M1310849},
	abstract = {Prior distributions for Bayesian inference that rely on the \$l\_1\$-norm of the parameters are of considerable interest, in part because they promote parameter fields with less regularity than Gaussian priors (e.g., discontinuities and blockiness). These \$l\_1\$-type priors include the total variation (TV) prior and the Besov space \$B\_\{1,1\}{\textasciicircum}s\$ prior, and in general yield non-Gaussian posterior distributions. Sampling from these posteriors is challenging, particularly in the inverse problem setting where the parameter space is high-dimensional and the forward problem may be nonlinear. This paper extends the randomize-then-optimize (RTO) method, an optimization-based sampling algorithm developed for Bayesian inverse problems with Gaussian priors, to inverse problems with \$l\_1\$-type priors. We use a variable transformation to convert an \$l\_1\$-type prior to a standard Gaussian prior, such that the posterior distribution of the transformed parameters is amenable to Metropolized sampling via RTO. We demonstrate this approach on several deconvolution problems and an elliptic PDE inverse problem, using TV or Besov space \$B\_\{1,1\}{\textasciicircum}s\$ priors. Our results show that the transformed RTO algorithm characterizes the correct posterior distribution and can be more efficient than other sampling algorithms. The variable transformation can also be extended to other non-Gaussian priors. (An erratum is attached.)},
	number = {1},
	urldate = {2025-05-29},
	journal = {SIAM Review},
	author = {Llorente, F. and Martino, L. and Delgado, D. and López-Santiago, J.},
	month = feb,
	year = {2023},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {3--58},
	file = {Submitted Version:C\:\\Users\\Jack Swanson\\Zotero\\storage\\4MNTF2HC\\Llorente et al. - 2023 - Marginal Likelihood Computation for Model Selectio.pdf:application/pdf},
}

@article{murphy_conjugate_2007,
	title = {Conjugate {Bayesian} analysis of the {Gaussian} distribution},
	author = {Murphy, Kevin},
	month = nov,
	year = {2007},
}

@book{taylor_classical_2005,
	series = {G - {Reference},{Information} and {Interdisciplinary} {Subjects} {Series}},
	title = {Classical {Mechanics}},
	isbn = {978-1-891389-22-1},
	url = {https://books.google.com/books?id=P1kCtNr-pJsC},
	publisher = {University Science Books},
	author = {Taylor, J.R.},
	year = {2005},
	lccn = {2004054971},
}

@article{laird_random-effects_1982,
	title = {Random-{Effects} {Models} for {Longitudinal} {Data}},
	volume = {38},
	issn = {0006341X, 15410420},
	url = {http://www.jstor.org/stable/2529876},
	doi = {10.2307/2529876},
	abstract = {[Models for the analysis of longitudinal data must recognize the relationship between serial observations on the same unit. Multivariate models with general covariance structure are often difficult to apply to highly unbalanced data, whereas two-stage random-effects models can be used easily. In two-stage models, the probability distributions for the response vectors of different individuals belong to a single family, but some random-effects parameters vary across individuals, with a distribution specified at the second stage. A general family of models is discussed, which includes both growth models and repeated-measures models as special cases. A unified approach to fitting these models, based on a combination of empirical Bayes and maximum likelihood estimation of model parameters and using the EM algorithm, is discussed. Two examples are taken from a current epidemiological study of the health effects of air pollution.]},
	number = {4},
	urldate = {2025-09-08},
	journal = {Biometrics},
	author = {Laird, Nan M. and Ware, James H.},
	year = {1982},
	note = {Publisher: [Wiley, International Biometric Society]},
	pages = {963--974},
}

@article{hart_kernel_1986,
	title = {Kernel {Regression} {Estimation} {Using} {Repeated} {Measurements} {Data}},
	volume = {81},
	issn = {01621459, 1537274X},
	url = {http://www.jstor.org/stable/2289087},
	doi = {10.2307/2289087},
	abstract = {[The estimation of growth curves has been studied extensively in parametric situations. Here we consider the nonparametric estimation of an average growth curve. Suppose that there are observations from several experimental units, each following the regression model y(x$_{\textrm{j}}$) = f(x$_{\textrm{j}}$) + ?$_{\textrm{j}}$ (j = 1, ..., n), where ?$_{\textrm{1}}$, ..., ?$_{\textrm{n}}$ are correlated zero mean errors and \$0 {\textbackslash}leq x\_1 {\textless} {\textbackslash}cdots {\textless} x\_n {\textbackslash}leq 1\$ are fixed constants. We study some of the properties of a kernel estimator of f(x). Asymptotic and finite-sample results concerning the mean squared error of the estimator are obtained. In particular, the influence of correlation on the bandwidth minimizing mean squared error is discussed. A data-based method for selecting the band-width is illustrated in a data analysis. Most previous research on kernel regression estimators has involved uncorrelated errors. We investigate how dependence of the errors changes the behavior of a kernel estimator. Our theorems concerning the asymptotic mean squared error show that the estimator cannot be consistent unless the number of experimental units tends to infinity. This contrasts with the results for uncorrelated errors, where the estimator is consistent when the number of distinct x values tends to infinity. Finite-sample results indicate that the optimum bandwidth when the errors are correlated can be either larger or smaller than the optimum bandwidth with uncorrelated errors. If the number of x values is not large and/or the errors are highly positively correlated, the optimum bandwidth tends to be smaller than when the errors are uncorrelated. This is contrary to existing examples wherein serially correlated errors require larger than usual bandwidths. In our data analysis, we choose the bandwidth that minimizes an estimate of the mean average squared error while taking into account the presence of correlated errors. Using the same data we show that ignoring correlation leads to an oversmoothed kernel estimate. An analytic result illustrates that this phenomenon is not necessarily an anomaly of the data.]},
	number = {396},
	urldate = {2025-09-08},
	journal = {Journal of the American Statistical Association},
	author = {Hart, Jeffrey D. and Wehrly, Thomas E.},
	year = {1986},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {1080--1088},
}

@article{barry_bayesian_1995,
	title = {A {Bayesian} {Model} for {Growth} {Curve} {Analysis}},
	volume = {51},
	issn = {0006341X, 15410420},
	url = {http://www.jstor.org/stable/2532951},
	doi = {10.2307/2532951},
	abstract = {[An experiment involves K subjects where for subject i, n$_{\textrm{i}}$ values y$_{\textrm{i1}}$, y$_{\textrm{i2}}$,..., y$_{\textrm{in{\textless}sub{\textgreater}i}}${\textless}/sub{\textgreater} of a random variable Y are observed at times t$_{\textrm{i1}}$, t$_{\textrm{i2}}$,..., t$_{\textrm{in{\textless}sub{\textgreater}i}}${\textless}/sub{\textgreater}. Assume that y$_{\textrm{ij}}$ = F(i, t$_{\textrm{ij}}$) + e$_{\textrm{ij}}$ where e$_{\textrm{ij}}$ are independently and identically distributed (i.i.d.) N(0, ?$^{\textrm{2}}$). We consider the estimation of the function F and the testing of the homogeneity hypothesis that, for i ? j, F(i, t) - F(j, t) does not depend on t. The function F(i, t) is modelled as a Gaussian process which seeks to quantify the notions that for each i, F(i, t) is a slowly changing function of t and that for i ? j, F(i, t), and F(j, t) are in some sense similar. We propose to estimate F(i, t) by its posterior mean given all of the data. This Bayes estimate is shown to be equivalent to a particular form of penalised likelihood estimation. We consider data-based methods for setting the parameters of the Gaussian process prior, develop a test of the homogeneity hypothesis, report the results of a Monte Carlo study illustrating the effectiveness of the proposed methodology, and apply the methods to a study of variations in temperature and blood pressure over the course of the menstrual cycle.]},
	number = {2},
	urldate = {2025-09-08},
	journal = {Biometrics},
	author = {Barry, Daniel},
	year = {1995},
	note = {Publisher: International Biometric Society},
	pages = {639--655},
}

@article{shi_analysis_1996,
	title = {An {Analysis} of {Paediatric} {CD4} {Counts} for {Acquired} {Immune} {Deficiency} {Syndrome} {Using} {Flexible} {Random} {Curves}},
	volume = {45},
	issn = {0035-9254},
	url = {https://doi.org/10.2307/2986151},
	doi = {10.2307/2986151},
	abstract = {In this paper we analyse CD4 counts from infants born to mothers who are infected with the human immunodeficiency virus. A random effects model with linear or low order polynomials in time is unsatisfactory for these longitudinal data. We develop an alternative approach based on a flexible family of models for which both the fixed and the random effects are linear combinations of B-splines. The fixed and random parts are smooth functions of time and the covariance structure is parsimonious. The procedure allows estimates of each individual's smooth trajectory over time to be exhibited. Model selection, estimation and computation are discussed. Centile curves are presented that take into account the longitudinal nature of the data. We emphasize a graphical approach to the presentation of results.},
	number = {2},
	urldate = {2025-09-08},
	journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
	author = {Shi, Minggao and Weiss, Robert E. and Taylor, Jeremy M. G.},
	month = jun,
	year = {1996},
	pages = {151--163},
}

@article{wu_local_2002,
	title = {Local {Polynomial} {Mixed}-{Effects} {Models} for {Longitudinal} {Data}},
	volume = {97},
	issn = {01621459},
	url = {http://www.jstor.org/stable/3085729},
	abstract = {[We consider a nonparametric mixed-effects model y$_{\textrm{i}}$(t$_{\textrm{ij}}$) = ? (t$_{\textrm{ij}}$) + v$_{\textrm{i}}$(t$_{\textrm{ij}}$) + ?$_{\textrm{i}}$(t$_{\textrm{ij}}$), j = 1,2,..., n$_{\textrm{i}}$; i = 1, 2,..., n for longitudinal data. We propose combining local polynomial kernel regression and linear mixed-effects (LME) model techniques to estimate both fixed-effects (population) curve ?(t) and random-effects curves v$_{\textrm{i}}$(t). The resulting estimator, called the local polynomial LME (LLME) estimator, takes the local correlation structure of the longitudinal data into account naturally. We also propose new bandwidth selection strategies for estimating ?(t) and v$_{\textrm{i}}$(t). Simulation studies show that our estimator for ?(t) is superior to the existing estimators in the sense of mean squared errors. The asymptotic bias, variance, mean squared errors, and asymptotic normality are established for the LLME estimators of ?(t). When n$_{\textrm{i}}$ is bounded and n tends to infinity, our LLME estimator converges in a standard nonparametric rate, and the asymptotic bias and variance are essentially the same as those of the kernel generalized estimating equation estimator proposed by Lin and Carroll. But when both n$_{\textrm{i}}$ and n tend to infinity, the LLME estimator is consistent with a slower rate of n$^{\textrm{1/2}}$ compared to the standard nonparametric rate, due to the existence of within-subject correlations of longitudinal data. We illustrate our methods with an application to a longitudinal dataset.]},
	number = {459},
	urldate = {2025-09-08},
	journal = {Journal of the American Statistical Association},
	author = {Wu, Hulin and Zhang, Jin-Ting},
	year = {2002},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {883--897},
}

@article{thompson_bayesian_2008,
	title = {A {Bayesian} model for sparse functional data.},
	volume = {64},
	issn = {0006-341X 1541-0420},
	doi = {10.1111/j.1541-0420.2007.00829.x},
	abstract = {We propose a method for analyzing data which consist of curves on multiple individuals, i.e., longitudinal or functional data. We use a Bayesian model where  curves are expressed as linear combinations of B-splines with random  coefficients. The curves are estimated as posterior means obtained via Markov  chain Monte Carlo (MCMC) methods, which automatically select the local level of  smoothing. The method is applicable to situations where curves are sampled  sparsely and/or at irregular time points. We construct posterior credible  intervals for the mean curve and for the individual curves. This methodology  provides unified, efficient, and flexible means for smoothing functional data.},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Thompson, Wesley K. and Rosen, Ori},
	month = mar,
	year = {2008},
	pmid = {17573864},
	pmcid = {PMC5598470},
	note = {Place: England},
	keywords = {Biometry/*methods, Computer Simulation, *Models, Statistical, *Algorithms, *Data Interpretation, Statistical, *Bayes Theorem, *Cohort Studies, *Longitudinal Studies},
	pages = {54--63},
}

@article{ogden_wavelet_2010,
	title = {Wavelet modeling of functional random effects with application to human vision data},
	volume = {140},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375810002247},
	doi = {10.1016/j.jspi.2010.04.044},
	abstract = {In modern statistical practice, it is increasingly common to observe a set of curves or images, often measured with noise, and to use these as the basis of analysis (functional data analysis). We consider a functional data model consisting of measurement error and functional random effects motivated by data from a study of human vision. By transforming the data into the wavelet domain we are able to exploit the expected sparse representation of the underlying function and the mechanism generating the random effects. We propose simple fitting procedures and illustrate the methods on the vision data.},
	number = {12},
	journal = {Special Issue in Honor of Emanuel Parzen on the Occasion of his 80th Birthday and Retirement from the Department of Statistics, Texas A\&M University},
	author = {Ogden, R. Todd and Greene, Ernest},
	month = dec,
	year = {2010},
	keywords = {False discovery rate, Functional data analysis},
	pages = {3797--3808},
}

@article{morris_wavelet-based_2003,
	title = {Wavelet-{Based} {Nonparametric} {Modeling} of {Hierarchical} {Functions} in {Colon} {Carcinogenesis}},
	volume = {98},
	issn = {01621459},
	url = {http://www.jstor.org/stable/30045283},
	abstract = {[In this article we develop new methods for analyzing the data from an experiment using rodent models to investigate the effect of type of dietary fat on \$O{\textasciicircum}6\$-methylguanine-DNA-methyltransferase (MGMT), an important biomarker in early colon carcinogenesis. The data consist of observed profiles over a spatial variable contained within a two-stage hierarchy, a structure that we dub hierarchical functional data. We present a new method providing a unified framework for modeling these data, simultaneously yielding estimates and posterior samples for mean, individual, and subsample-level profiles, as well as covariance parameters at the various hierarchical levels. Our method is nonparametric in that it does not require the prespecification of parametric forms for the functions and involves modeling in the wavelet space, which is especially effective for spatially heterogeneous functions as encountered in the MGMT data. Our approach is Bayesian; the only informative hyperparameters in our model are effectively smoothing parameters. Analysis of this dataset yields interesting new insights into how MGMT operates in early colon carcinogenesis, and how this may depend on diet. Our method is general, so it can be applied to other settings where hierarchical functional data are encountered.]},
	number = {463},
	urldate = {2025-09-10},
	journal = {Journal of the American Statistical Association},
	author = {Morris, Jeffrey S. and Vannucci, Marina and Brown, Philip J. and Carroll, Raymond J.},
	year = {2003},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {573--583},
}

@article{morris_wavelet-based_2006,
	title = {Wavelet-{Based} {Functional} {Mixed} {Models}},
	volume = {68},
	issn = {13697412, 14679868},
	url = {http://www.jstor.org/stable/3647565},
	abstract = {[Increasingly, scientific studies yield functional data, in which the ideal units of observation are curves and the observed data consist of sets of curves that are sampled on a fine grid. We present new methodology that generalizes the linear mixed model to the functional mixed model framework, with model fitting done by using a Bayesian wavelet-based approach. This method is flexible, allowing functions of arbitrary form and the full range of fixed effects structures and between-curve covariance structures that are available in the mixed model framework. It yields nonparametric estimates of the fixed and random-effects functions as well as the various between-curve and within-curve covariance matrices. The functional fixed effects are adaptively regularized as a result of the non-linear shrinkage prior that is imposed on the fixed effects' wavelet coefficients, and the random-effect functions experience a form of adaptive regularization because of the separately estimated variance components for each wavelet coefficient. Because we have posterior samples for all model quantities, we can perform pointwise or joint Bayesian inference or prediction on the quantities of the model. The adaptiveness of the method makes it especially appropriate for modelling irregular functional data that are characterized by numerous local features like peaks.]},
	number = {2},
	urldate = {2025-09-10},
	journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Morris, Jeffrey S. and Carroll, Raymond J.},
	year = {2006},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {179--199},
}

@article{staicu_modeling_2012,
	title = {Modeling {Functional} {Data} with {Spatially} {Heterogeneous} {Shape} {Characteristics}},
	volume = {68},
	issn = {0006341X, 15410420},
	url = {http://www.jstor.org/stable/23270434},
	abstract = {[We propose a novel class of models for functional data exhibiting skewness or other shape characteristics that vary with spatial or temporal location. We use copulas so that the marginal distributions and the dependence structure can be modeled independently. Dependence is modeled with a Gaussian or t-copula, so that there is an underlying latent Gaussian process. We model the marginal distributions using the skew t family. The mean, variance, and shape parameters are modeled nonparametrically as functions of location. A computationally tractable inferential framework for estimating heterogeneous asymmetric or heavy-tailed marginal distributions is introduced. This framework provides a new set of tools for increasingly complex data collected in medical and public health studies. Our methods were motivated by and are illustrated with a state-of-the-art study of neuronal tracts in multiple sclerosis patients and healthy controls. Using the tools we have developed, we were able to find those locations along the tract most affected by the disease. However, our methods are general and highly relevant to many functional data sets. In addition to the application to one-dimensional tract profiles illustrated here, higher-dimensional extensions of the methodology could have direct applications to other biological data including functional and structural magnetic resonance imaging (MRI).]},
	number = {2},
	urldate = {2025-09-10},
	journal = {Biometrics},
	author = {Staicu, Ana-Maria and Crainiceanu, Ciprian M. and Reich, Daniel S. and Ruppert, David},
	year = {2012},
	note = {Publisher: International Biometric Society},
	pages = {331--343},
}

@article{graps_introduction_1995,
	title = {An introduction to wavelets},
	volume = {2},
	issn = {1070-9924},
	number = {2},
	journal = {IEEE computational science and engineering},
	author = {Graps, Amara},
	year = {1995},
	note = {Publisher: IEEE},
	pages = {50--61},
}

@article{baladandayuthapani_bayesian_2008,
	title = {Bayesian hierarchical spatially correlated functional data analysis with application to colon carcinogenesis.},
	volume = {64},
	issn = {0006-341X 1541-0420},
	doi = {10.1111/j.1541-0420.2007.00846.x},
	abstract = {In this article, we present new methods to analyze data from an experiment using rodent models to investigate the role of p27, an important cell-cycle mediator,  in early colon carcinogenesis. The responses modeled here are essentially  functions nested within a two-stage hierarchy. Standard functional data analysis  literature focuses on a single stage of hierarchy and conditionally independent  functions with near white noise. However, in our experiment, there is substantial  biological motivation for the existence of spatial correlation among the  functions, which arise from the locations of biological structures called colonic  crypts: this possible functional correlation is a phenomenon we term crypt  signaling. Thus, as a point of general methodology, we require an analysis that  allows for functions to be correlated at the deepest level of the hierarchy. Our  approach is fully Bayesian and uses Markov chain Monte Carlo methods for  inference and estimation. Analysis of this data set gives new insights into the  structure of p27 expression in early colon carcinogenesis and suggests the  existence of significant crypt signaling. Our methodology uses regression  splines, and because of the hierarchical nature of the data, dimension reduction  of the covariance matrix of the spline coefficients is important: we suggest  simple methods for overcoming this problem.},
	language = {eng},
	number = {1},
	journal = {Biometrics},
	author = {Baladandayuthapani, Veerabhadran and Mallick, Bani K. and Young Hong, Mee and Lupton, Joanne R. and Turner, Nancy D. and Carroll, Raymond J.},
	month = mar,
	year = {2008},
	pmid = {17608780},
	pmcid = {PMC2740995},
	note = {Place: England},
	keywords = {*Models, Biological, Biometry/*methods, Computer Simulation, *Models, Statistical, *Algorithms, *Data Interpretation, Statistical, *Bayes Theorem, Animals, Colonic Neoplasms/*physiopathology, Rats, Statistics as Topic},
	pages = {64--73},
}

@article{de_cock_ames_2011,
	title = {Ames, {Iowa}: {Alternative} to the {Boston} {Housing} {Data} as an {End} of {Semester} {Regression} {Project}},
	volume = {19},
	issn = {null},
	url = {https://doi.org/10.1080/10691898.2011.11889627},
	doi = {10.1080/10691898.2011.11889627},
	number = {3},
	journal = {Journal of Statistics Education},
	author = {De Cock, Dean},
	month = nov,
	year = {2011},
	note = {Publisher: Taylor \& Francis},
	pages = {null--null},
	annote = {doi: 10.1080/10691898.2011.11889627},
}

@article{di_multilevel_2009,
	title = {{MULTILEVEL} {FUNCTIONAL} {PRINCIPAL} {COMPONENT} {ANALYSIS}.},
	volume = {3},
	issn = {1932-6157 1941-7330},
	doi = {10.1214/08-AOAS206SUPP},
	abstract = {The Sleep Heart Health Study (SHHS) is a comprehensive landmark study of sleep and its impacts on health outcomes. A primary metric of the SHHS is the in-home  polysomnogram, which includes two electroencephalographic (EEG) channels for each  subject, at two visits. The volume and importance of this data presents enormous  challenges for analysis. To address these challenges, we introduce multilevel  functional principal component analysis (MFPCA), a novel statistical methodology  designed to extract core intra- and inter-subject geometric components of  multilevel functional data. Though motivated by the SHHS, the proposed  methodology is generally applicable, with potential relevance to many modern  scientific studies of hierarchical or longitudinal functional outcomes. Notably,  using MFPCA, we identify and quantify associations between EEG activity during  sleep and adverse cardiovascular outcomes.},
	language = {eng},
	number = {1},
	journal = {The annals of applied statistics},
	author = {Di, Chong-Zhi and Crainiceanu, Ciprian M. and Caffo, Brian S. and Punjabi, Naresh M.},
	month = mar,
	year = {2009},
	pmid = {20221415},
	pmcid = {PMC2835171},
	note = {Place: United States},
	pages = {458--488},
}

@article{liu_functional_2012,
	title = {Functional mixed effects models},
	volume = {4},
	issn = {1939-5108},
	url = {https://doi.org/10.1002/wics.1226},
	doi = {10.1002/wics.1226},
	abstract = {Abstract Functional mixed effects model (FMM) is a mixed effects modeling framework that both the fixed effects and the random effects are modeled by nonparametric curves. The combination of mixed effects model and nonparametric smoothing enables FMMs to handle outcomes with complex profiles and at the same time to incorporate complex experimental designs and include covariates. Estimation and inference can be performed either using techniques from linear mixed effects models or using fully Bayesian approaches. As in functional data analysis, inference in FMMs is preliminary and needs to be further investigated. Several software packages have been developed to implement FMMs, although computational challenges do exist no matter which smoothing method is used. WIREs Comput Stat 2012, 4:527?534. doi: 10.1002/wics.1226 This article is categorized under: Statistical Models {\textgreater} Classification Models},
	number = {6},
	urldate = {2025-10-20},
	journal = {WIREs Computational Statistics},
	author = {Liu, Ziyue and Guo, Wensheng},
	month = nov,
	year = {2012},
	note = {Publisher: John Wiley \& Sons, Ltd},
	keywords = {functional data analysis, mixed effects, nonparametric smoothing},
	pages = {527--534},
}

@article{fine_examination_2019,
	title = {An {Examination} of a {Functional} {Mixed}-{Effects} {Modeling} {Approach} to the {Analysis} of {Longitudinal} {Data}},
	volume = {54},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2018.1520626},
	doi = {10.1080/00273171.2018.1520626},
	number = {4},
	journal = {Multivariate Behavioral Research},
	author = {Fine, Kimberly L. and Suk, Hye Won and Grimm, Kevin J.},
	month = jul,
	year = {2019},
	note = {Publisher: Routledge},
	pages = {475--491},
	annote = {doi: 10.1080/00273171.2018.1520626},
}

@misc{ghosal_variable_2025,
	title = {Variable {Selection} for {Fixed} and {Random} {Effects} in {Multilevel} {Functional} {Mixed} {Effects} {Models}},
	url = {http://arxiv.org/abs/2505.05416},
	doi = {10.48550/arXiv.2505.05416},
	abstract = {We develop a new method for simultaneously selecting fixed and random effects in a multilevel functional regression model. The proposed method is motivated by accelerometer-derived physical activity data from the 2011-12 cohort of the National Health and Nutrition Examination Survey (NHANES), where we are interested in identifying age and race-specific heterogeneity in covariate effects on the diurnal patterns of physical activity across the lifespan. Existing methods for variable selection in function-on-scalar regression have primarily been designed for fixed effect selection and for single-level functional data. In high-dimensional multilevel functional regression, the presence of cluster-specific heterogeneity in covariate effects could be detected through sparsity in fixed and random effects, and for this purpose, we propose a multilevel functional mixed effects selection (MuFuMES) method. The fixed and random functional effects are modelled using splines, with spike-and-slab group lasso (SSGL) priors on the unknown parameters of interest and a computationally efficient MAP estimation approach is employed for mixed effect selection through an Expectation Conditional Maximization (ECM) algorithm. Numerical analysis using simulation study illustrates the satisfactory selection accuracy of the variable selection method in having a negligible false-positive and false-negative rate. The proposed method is applied to the accelerometer data from the NHANES 2011-12 cohort, where it effectively identifies age and race-specific heterogeneity in covariate effects on the diurnal patterns of physical activity, recovering biologically meaningful insights.},
	urldate = {2025-10-21},
	publisher = {arXiv},
	author = {Ghosal, Rahul and Matabuena, Marcos and Saha, Enakshi},
	month = may,
	year = {2025},
	note = {arXiv:2505.05416 [stat]},
	keywords = {Statistics - Methodology},
	file = {Preprint PDF:C\:\\Users\\Jack Swanson\\Zotero\\storage\\EURCA8WE\\Ghosal et al. - 2025 - Variable Selection for Fixed and Random Effects in.pdf:application/pdf;Snapshot:C\:\\Users\\Jack Swanson\\Zotero\\storage\\3MIW5MSD\\2505.html:text/html},
}
